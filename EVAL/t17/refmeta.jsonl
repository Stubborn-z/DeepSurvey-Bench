{"paper_id": 199657967, "title": "6G Wireless Networks: Vision, Requirements, Architecture, and Key Technologies", "author_names": ["Zhengquan Zhang", "Yue Xiao", "Zheng Ma", "Ming Xiao", "Z. Ding", "Xianfu Lei", "G. Karagiannidis", "P. Fan"], "venue": "IEEE Vehicular Technology Magazine", "abstract": "A key enabler for the intelligent information society of 2030, 6G networks are expected to provide performance superior to 5G and satisfy emerging services and applications. In this article, we present our vision of what 6G will be and describe usage scenarios and requirements for multi-terabyte per second (Tb/s) and intelligent 6G networks. We present a large-dimensional and autonomous network architecture that integrates space, air, ground, and underwater networks to provide ubiquitous and unlimited wireless connectivity. We also discuss artificial intelligence (AI) and machine learning [1], [2] for autonomous networks and innovative air-interface design. Finally, we identify several promising technologies for the 6G ecosystem, including terahertz (THz) communications, very-large-scale antenna arrays [i.e., supermassive (SM) multiple-input, multiple-output (MIMO)], large intelligent surfaces (LISs) and holographic beamforming (HBF), orbital angular momentum (OAM) multiplexing, laser and visible-light communications (VLC), blockchain-based spectrum sharing, quantum communications and computing, molecular communications, and the Internet of Nano-Things.", "year": 2019, "publicationdate": "2019-07-18", "externalids": {"DOI": "10.1109/MVT.2019.2921208"}, "doi_lower": "10.1109/mvt.2019.2921208"}
{"paper_id": 268987880, "title": "Next generation multiple access for IMT towards 2030 and beyond", "author_names": ["Zhiguo Ding", "Robert Schober", "Pingzhi Fan", "H. V. Poor"], "venue": "Science China Information Sciences", "abstract": "NOMA assisted NGMA has been envisioned in the recently published IMT-2030 Framework. This perspective has outlined three important features of NOMA assisted NGMA, namely multi-domain utilization, multi-mode compatibility, and multi-dimensional optimality, where important directions for future research into the design of NOMA assisted NGMA have also been discussed.", "year": 2024, "publicationdate": "2024-04-05", "externalids": {"DOI": "10.1007/s11432-024-4014-x"}, "doi_lower": "10.1007/s11432-024-4014-x"}
{"paper_id": 254854065, "title": "Knowledge Transfer and Reuse: A Case Study of Ai-Enabled Resource Management in RAN Slicing", "author_names": ["Hao Zhou", "M. Erol-Kantarci", "Vincent Poor"], "venue": "IEEE wireless communications", "abstract": "Efficient resource management scheme is critical to enable network slicing in 5G networks, in envisioned 6G networks, and artificial intelligence (AI) techniques offer promising solutions. Considering rapidly emerging new machine learning (ML) techniques, such as graph learning, federated learning (FL), and transfer learning, a timely survey is needed to provide an overview of resource management and network slicing techniques for AI-enabled wireless networks. This article provides such a survey along with an application of knowledge transfer in radio access network (RAN) slicing. In particular, we first provide some background on resource management and network slicing, and review relevant state-of-the-art AI and ML techniques and their applications. Then, we introduce our AI-enabled knowledge transfer and reusebased resource management (AKRM) scheme, where we apply transfer learning to improve system performance. Compared with most existing works, which focus on the training of standalone agents from scratch, the main difference of AKRM lies in its knowledge transfer, and reuse capability between different tasks. Our article provides a roadmap for researchers for applying knowledge transfer schemes in AI-enabled wireless networks, as well as a case study of resource allocation problem in RAN slicing.", "year": 2022, "publicationdate": "2022-12-18", "externalids": {"DOI": "10.1109/MWC.004.2200025"}, "doi_lower": "10.1109/mwc.004.2200025"}
{"paper_id": 235727530, "title": "RAN Resource Slicing in 5G Using Multi-Agent Correlated Q-Learning", "author_names": ["Hao Zhou", "Medhat H. M. Elsayed", "M. Erol-Kantarci"], "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications", "abstract": "5G is regarded as a revolutionary mobile network, which is expected to satisfy a vast number of novel services, ranging from remote health care to smart cities. However, heterogeneous Quality of Service (QoS) requirements of different services and limited spectrum make the radio resource allocation a challenging problem in 5G. In this paper, we propose a multi-agent reinforcement learning (MARL) method for radio resource slicing in 5G. We model each slice as an intelligent agent that competes for limited radio resources, and the correlated Q-learning is applied for inter-slice resource block (RB) allocation. The proposed correlated Q-learning based inter-slice RB allocation (COQRA) scheme is compared with Nash Q-learning (NQL), Latency-Reliability-Throughput Q-learning (LRTQ) methods, and the priority proportional fairness (PPF) algorithm. Our simulation results show that the proposed CO-QRA achieves 32.4% lower latency and 6.3% higher throughput when compared with LRTQ, and 5.8% lower latency and 5.9% higher throughput than NQL. Significantly higher throughput and lower packet drop rate (PDR) is observed in comparison to PPF.", "year": 2021, "publicationdate": "2021-06-24", "externalids": {"DOI": "10.1109/pimrc50174.2021.9569358"}, "doi_lower": "10.1109/pimrc50174.2021.9569358"}
{"paper_id": 125827950, "title": "Channel State Information Prediction for 5G Wireless Communications: A Deep Learning Approach", "author_names": ["Changqing Luo", "Jinlong Ji", "Qianlong Wang", "Xuhui Chen", "Pan Li"], "venue": "IEEE Transactions on Network Science and Engineering", "abstract": "Channel state information (CSI) estimation is one of the most fundamental problems in wireless communication systems. Various methods, so far, have been developed to conduct CSI estimation. However, they usually require high computational complexity, which makes them unsuitable for 5G wireless communications due to employing many new techniques (e.g., massive MIMO, OFDM, and millimeter-Wave (mmWave)). In this paper, we propose an efficient online CSI prediction scheme, called OCEAN, for predicting CSI from historical data in 5G wireless communication systems. Specifically, we first identify several important features affecting the CSI of a radio link and a data sample consists of the information of the features and the CSI. We then design a learning framework that is an integration of a CNN (convolutional neural network) and a long short term with memory (LSTM) network. We also further develop an offline-online two-step training mechanism, enabling the prediction results to be more stable when applying it to practical 5G wireless communication systems. To validate OCEAN's efficacy, we consider four typical case studies, and conduct extensive experiments in the four scenarios, i.e., two outdoor and two indoor scenarios. The experiment results show that OCEAN not only obtains the predicted CSI values very quickly but also achieves highly accurate CSI prediction with up to 2.650-3.457 percent average difference ratio (ADR) between the predicted and measured CSI.", "year": 2020, "publicationdate": "2020-01-01", "externalids": {"DOI": "10.1109/TNSE.2018.2848960"}, "doi_lower": "10.1109/tnse.2018.2848960"}
{"paper_id": 251279932, "title": "Federated Deep Reinforcement Learning for Resource Allocation in O-RAN Slicing", "author_names": ["Han Zhang", "Hao Zhou", "M. Erol-Kantarci"], "venue": "Global Communications Conference", "abstract": "Recently, open radio access network (O-RAN) has become a promising technology to provide an open environment for network vendors and operators. Coordinating the x-applications (xAPPs) is critical to increase flexibility and guarantee high overall network performance in O-RAN. Meanwhile, federated reinforcement learning has been proposed as a promising technique to enhance the collaboration among distributed reinforcement learning agents and improve learning efficiency. In this paper, we propose a federated deep reinforcement learning algorithm to coordinate multiple independent xAPPs in O-RAN for network slicing. We design two xAPPs, namely a power control xAPP and a slice-based resource allocation xAPP, and we use a federated learning model to coordinate two xAPP agents to enhance learning efficiency and improve network performance. Compared with conventional deep reinforcement learning, our proposed algorithm can achieve 11% higher throughput for enhanced mobile broadband (eMBB) slices and 33% lower delay for ultra-reliable low-latency communication (URLLC) slices.", "year": 2022, "publicationdate": "2022-08-02", "externalids": {"DOI": "10.1109/GLOBECOM48099.2022.10001658"}, "doi_lower": "10.1109/globecom48099.2022.10001658"}
{"paper_id": 257766667, "title": "A Survey on Model-Based, Heuristic, and Machine Learning Optimization Approaches in RIS-Aided Wireless Networks", "author_names": ["Hao Zhou", "Senior Member Ieee Melike Erol-Kantarci", "Senior Member Ieee Yuanwei Liu", "L. F. I. H. Vincent Poor"], "venue": "IEEE Communications Surveys and Tutorials", "abstract": "Reconfigurable intelligent surfaces (RISs) have received considerable attention as a key enabler for envisioned 6G networks, for the purpose of improving the network capacity, coverage, efficiency, and security with low energy consumption and low hardware cost. However, integrating RISs into the existing infrastructure greatly increases the network management complexity, especially for controlling a significant number of RIS elements. To realize the full potential of RISs, efficient optimization approaches are of great importance. This work provides a comprehensive survey of optimization techniques for RIS-aided wireless communications, including model-based, heuristic, and machine learning (ML) algorithms. In particular, we first summarize the problem formulations in the literature with diverse objectives and constraints, e.g., sumrate maximization, power minimization, and imperfect channel state information constraints. Then, we introduce model-based algorithms that have been used in the literature, such as alternating optimization, the majorization-minimization method, and successive convex approximation. Next, heuristic optimization is discussed, which applies heuristic rules for obtaining lowcomplexity solutions. Moreover, we present state-of-the-art ML algorithms and applications towards RISs, i.e., supervised and unsupervised learning, reinforcement learning, federated learning, graph learning, transfer learning, and hierarchical learning-based approaches. Model-based, heuristic, and ML approaches are compared in terms of stability, robustness, optimality and so on, providing a systematic understanding of these techniques. Finally, we highlight RIS-aided applications towards 6G networks and identify future challenges.", "year": 2023, "publicationdate": "2023-03-25", "externalids": {"DOI": "10.1109/COMST.2023.3340099"}, "doi_lower": "10.1109/comst.2023.3340099"}
{"paper_id": 258715226, "title": "Towards Expert-Level Medical Question Answering with Large Language Models", "author_names": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "Mike Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Tomašev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Yossi Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "venue": "arXiv.org", "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09617"}, "doi_lower": "10.48550/arxiv.2305.09617"}
{"paper_id": 268253108, "title": "SaulLM-7B: A pioneering Large Language Model for Law", "author_names": ["P. Colombo", "T. Pires", "Malik Boudiaf", "Dominic Culver", "Rui Melo", "Caio Corro", "André Martins", "Fabrizio Esposito", "Vera L'ucia Raposo", "Sofia Morgado", "Michael Desa"], "venue": "arXiv.org", "abstract": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.", "year": 2024, "publicationdate": "2024-03-06", "externalids": {"DOI": "10.48550/arXiv.2403.03883"}, "doi_lower": "10.48550/arxiv.2403.03883"}
{"paper_id": 257833842, "title": "BloombergGPT: A Large Language Model for Finance", "author_names": ["Shijie Wu", "Ozan Irsoy", "Steven Lu", "Vadim Dabravolski", "Mark Dredze", "Sebastian Gehrmann", "P. Kambadur", "D. Rosenberg", "Gideon Mann"], "venue": "arXiv.org", "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 258686712, "title": "Symbol tuning improves in-context learning in language models", "author_names": ["Jerry W. Wei", "Le Hou", "Andrew Kyle Lampinen", "Xiangning Chen", "Da Huang", "Yi Tay", "Xinyun Chen", "Yifeng Lu", "Denny Zhou", "Tengyu Ma", "Quoc V. Le"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g.,\"positive/negative sentiment\") are replaced with arbitrary symbols (e.g.,\"foo/bar\"). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings. We experiment with symbol tuning across Flan-PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior semantic knowledge.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08298"}, "doi_lower": "10.48550/arxiv.2305.08298"}
{"paper_id": 259145106, "title": "Understanding Telecom Language Through Large Language Models", "author_names": ["Lina Bariah", "Han Zou", "Qiyang Zhao", "B. Mouhouche", "F. Bader", "M. Debbah"], "venue": "Global Communications Conference", "abstract": "The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.1109/GLOBECOM54140.2023.10437725"}, "doi_lower": "10.1109/globecom54140.2023.10437725"}
{"paper_id": 259924486, "title": "The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms", "author_names": ["Yuyang Du", "S. Liew", "Kexin Chen", "Yulin Shao"], "venue": "", "abstract": "Large language models (LLMs) have garnered significant attention across various research disciplines, including the wireless communication community. There have been several heated discussions on the intersection of LLMs and wireless technologies. While recent studies have demonstrated the ability of LLMs to generate hardware description language (HDL) code for simple computation tasks, developing wireless prototypes and products via HDL poses far greater challenges because of the more complex computation tasks involved. In this paper, we aim to address this challenge by investigating the role of LLMs in FPGA-based hardware development for advanced wireless signal processing. We begin by exploring LLM-assisted code refactoring, reuse, and validation, using an open-source software-defined radio (SDR) project as a case study. Through the case study, we find that an LLM assistant can potentially yield substantial productivity gains for researchers and developers. We then examine the feasibility of using LLMs to generate HDL code for advanced wireless signal processing, using the Fast Fourier Transform (FFT) algorithm as an example. This task presents two unique challenges: the scheduling of subtasks within the overall task and the multi-step thinking required to solve certain arithmetic problem within the task. To address these challenges, we employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting techniques, culminating in the successful generation of a 64-point Verilog FFT module. Our results demonstrate the potential of LLMs for generalization and imitation, affirming their usefulness in writing HDL code for wireless communication systems. Overall, this work contributes to understanding the role of LLMs in wireless communication and motivates further exploration of their capabilities.", "year": 2023, "publicationdate": "2023-07-14", "externalids": {}, "doi_lower": null}
{"paper_id": 259360434, "title": "Large Language Models Empowered Autonomous Edge AI for Connected Intelligence", "author_names": ["Yifei Shen", "Jiawei Shao", "Xinjie Zhang", "Zehong Lin", "Hao Pan", "Dongsheng Li", "Jun Zhang", "K. Letaief"], "venue": "IEEE Communications Magazine", "abstract": "The evolution of wireless networks gravitates toward connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge artificial intelligence (Edge AI) is a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. This article presents a vision of autonomous edge AI systems that automatically organize, adapt, and optimize themselves to meet users' diverse requirements, leveraging the power of large language models (LLMs), that is, generative pretrained transformer (GPT). By exploiting the powerful abilities of GPT in language understanding, planning, and code generation, as well as incorporating classic wisdom such as task-oriented communication and edge federated learning, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automatically generating code to train new models in a privacy-preserving manner. Experimental results demonstrate the system's remarkable ability to accurately comprehend user demands, efficiently execute AI models with minimal cost, and effectively create high-performance AI models at edge servers.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1109/MCOM.001.2300550"}, "doi_lower": "10.1109/mcom.001.2300550"}
{"paper_id": 263310790, "title": "Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities", "author_names": ["Zhengyi Lin", "Guanqiao Qu", "Qiyuan Chen", "Xianhao Chen", "Zhe Chen", "Kaibin Huang"], "venue": "IEEE Communications Magazine", "abstract": "Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing artificial intelligence (AI) and shaping our society. However, the status quo cloud-based LLM deployment faces critical challenges such as long response time and privacy concerns, whereas on-device LLM deployment is hindered by the limited capabilities of end devices. To address the dilemma, this article explores the transformative potential of deploying LLMs at the 6G edge. We first introduce killer applications to exemplify the urgent need for edge LLM deployment and then, we identify the inherent limitations of on-device LLM deployment. We therefore argue that end-edge cooperation at the 6G edge is a promising solution for the dilemma. Towards this end, we elaborate on the 6G MEC architecture tailored for LLMs. Furthermore, we delve into edge training and edge inference for LLMs, with a focus on end-edge cooperation. In both aspects, we discuss a spectrum of cutting-edge techniques, including split learning/inference, parameter-efficient fine-tuning, parameter-sharing inference, and small-large language model cooperation. Finally, we investigate open problems in green and privacy-preserving edge LLM deployment. This work provides a comprehensive and forward-looking perspective and pathways for enabling LLM deployment at the network edge.", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.1109/MCOM.001.2400764"}, "doi_lower": "10.1109/mcom.001.2400764"}
{"paper_id": 266999418, "title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment", "author_names": ["Minrui Xu", "D. Niyato", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han", "Dong In Kim", "K. B. Letaief"], "venue": "IEEE wireless communications", "abstract": "AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.", "year": 2024, "publicationdate": "2024-01-15", "externalids": {"DOI": "10.1109/MWC.005.2400019"}, "doi_lower": "10.1109/mwc.005.2400019"}
{"paper_id": 261681766, "title": "Toward Reproducing Network Research Results Using Large Language Models", "author_names": ["Qiao Xiang", "Yuling Lin", "Mingjun Fang", "Bang Huang", "Siyong Huang", "Ridi Wen", "Franck Le", "L. Kong", "Jiwu Shu"], "venue": "ACM Workshop on Hot Topics in Networks", "abstract": "Reproducing research results is important for the networking community. The current best practice typically resorts to: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; or (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private ones are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). We first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report our observations and lessons and discuss future open research questions of this proposal.", "year": 2023, "publicationdate": "2023-09-09", "externalids": {"DOI": "10.1145/3626111.3628189"}, "doi_lower": "10.1145/3626111.3628189"}
{"paper_id": 264350121, "title": "Prompt Distillation for Efficient LLM-based Recommendation", "author_names": ["Lei Li", "Yongfeng Zhang", "Li Chen"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.", "year": 2023, "publicationdate": "2023-10-21", "externalids": {"DOI": "10.1145/3583780.3615017"}, "doi_lower": "10.1145/3583780.3615017"}
{"paper_id": 275901979, "title": "The rise and potential of large language model based agents: a survey", "author_names": ["Zhiheng Xi", "Wenxiang Chen", "Xin Guo", "Wei He", "Yiwen Ding", "Boyang Hong", "Ming Zhang", "Junzhe Wang", "Senjie Jin", "Enyu Zhou", "Rui Zheng", "Xiaoran Fan", "Xiao Wang", "Limao Xiong", "Yuhao Zhou", "Weiran Wang", "Changhao Jiang", "Yicheng Zou", "Xiangyang Liu", "Zhangyue Yin", "Shihan Dou", "Rongxiang Weng", "Wenjuan Qin", "Yongyan Zheng", "Xipeng Qiu", "Xuanjing Huang", "Qi Zhang", "Tao Gui"], "venue": "Science China Information Sciences", "abstract": null, "year": 2025, "publicationdate": "2025-01-17", "externalids": {"DOI": "10.1007/s11432-024-4222-0"}, "doi_lower": "10.1007/s11432-024-4222-0"}
{"paper_id": 261582296, "title": "Large Language Models as Optimizers", "author_names": ["Chengrun Yang", "Xuezhi Wang", "Yifeng Lu", "Hanxiao Liu", "Quoc V. Le", "Denny Zhou", "Xinyun Chen"], "venue": "International Conference on Learning Representations", "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.", "year": 2023, "publicationdate": "2023-09-07", "externalids": {"DOI": "10.48550/arXiv.2309.03409"}, "doi_lower": "10.48550/arxiv.2309.03409"}
{"paper_id": 282357014, "title": "EdgeNetLLM: Cloud-Edge Collaborative Adaptation of Large Language Models for Mobile Networking", "author_names": ["Xixi Zheng", "You Li", "Baokun Zheng", "Chuan Zhang", "Liehuang Zhu"], "venue": "IEEE Transactions on Network Science and Engineering", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.1109/tnse.2025.3624100"}, "doi_lower": "10.1109/tnse.2025.3624100"}
{"paper_id": 256320327, "title": "Fine-Tuning BERT-based Language Models for Duplicate Trouble Report Retrieval", "author_names": ["Nathan Bosch", "Serveh Shalmashi", "Forough Yaghoubi", "Henrik Holm", "Fitsum Gaim", "A. H. Payberah"], "venue": "2022 IEEE International Conference on Big Data (Big Data)", "abstract": "In large software-intensive organizations, trouble reports (TRs) are heavily involved in reporting, analyzing, and resolving faults. Due to the scale of modern organizations and products, multiple people independently often identify faults, leading to duplicate TRs. To mitigate the additional manual effort to identify and resolve these duplicate TRs, prior work at Ericsson focused on developing a 2-stage BERT-based retrieval system for identifying similar TRs when provided a new fault observation. This approach, although powerful, struggled to generalize to out-of-domain TRs. In this paper, we evaluate several fine-tuning strategies to integrate domain knowledge further, notably telecommunications knowledge, into the BERT-based TR retrieval models to (i) attain better performance on duplicate TR retrieval/identification and (ii) improve model generalizability to out-of-domain TR data. We find that adding domain-specific data into the fine-tuning models led to improved results on both overall model performance and model generalizability.", "year": 2022, "publicationdate": "2022-12-17", "externalids": {"DOI": "10.1109/BigData55660.2022.10020825"}, "doi_lower": "10.1109/bigdata55660.2022.10020825"}
{"paper_id": 265502669, "title": "CloudEval-YAML: A Practical Benchmark for Cloud Configuration Generation", "author_names": ["Yifei Xu", "Yuning Chen", "Xumiao Zhang", "Xianshang Lin", "Pan Hu", "Yunfei Ma", "Songwu Lu", "Wan Du", "Z. Mao", "Ennan Zhai", "Dennis Cai"], "venue": "Conference on Machine Learning and Systems", "abstract": "Among the thriving ecosystem of cloud computing and the proliferation of Large Language Model (LLM)-based code generation tools, there is a lack of benchmarking for code generation in cloud-native applications. In response to this need, we present CloudEval-YAML, a practical benchmark for cloud configuration generation. CloudEval-YAML tackles the diversity challenge by focusing on YAML, the de facto standard of numerous cloud-native tools. We develop the CloudEval-YAML benchmark with practicality in mind: the dataset consists of hand-written problems with unit tests targeting practical scenarios. We further enhanced the dataset to meet practical needs by rephrasing questions in a concise, abbreviated, and bilingual manner. The dataset consists of 1011 problems that take more than 1200 human hours to complete. To improve practicality during evaluation, we build a scalable evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a single machine. To the best of our knowledge, the CloudEval-YAML dataset is the first hand-written dataset targeting cloud-native applications. We present an in-depth evaluation of 12 LLMs, leading to a deeper understanding of the problems and LLMs, as well as effective methods to improve task performance and reduce cost.", "year": 2023, "publicationdate": "2023-11-10", "externalids": {"DOI": "10.48550/arXiv.2401.06786"}, "doi_lower": "10.48550/arxiv.2401.06786"}
{"paper_id": 231951803, "title": "Vision-Aided 6G Wireless Communications: Blockage Prediction and Proactive Handoff", "author_names": ["Gouranga Charan", "Muhammad Alrabeiah", "A. Alkhateeb"], "venue": "IEEE Transactions on Vehicular Technology", "abstract": "The sensitivity to blockages is a key challenge for millimeter wave and terahertz networks in 5G and beyond. Since these networks mainly rely on line-of-sight (LOS) links, sudden link blockages highly threaten the reliability of the networks. Further, when the LOS link is blocked, the network typically needs to hand off the user to another LOS basestation, which may incur critical time latency, especially if a search over a large codebook of narrow beams is needed. A promising way to tackle the reliability and latency challenges lies in enabling proaction in wireless networks. Proaction allows the network to anticipate future blockages, especially dynamic blockages, and initiate user hand-off beforehand. This article presents a complete machine learning framework for enabling proaction in wireless networks relying on visual data captured, for example, by red-green-blue (RGB) cameras deployed at the base stations. In particular, the article proposes a vision-aided wireless communication solution that utilizes bimodal machine learning to perform proactive blockage prediction and user hand-off. This is mainly achieved via a deep learning algorithm that learns from visual and wireless data how to predict incoming blockages. The predictions of this algorithm are used by the wireless network to proactively initiate hand-off decisions and avoid any unnecessary latency. The algorithm is developed on a vision-wireless dataset generated using the ViWi data-generation framework. Experimental results on two basestations with different cameras indicate that the algorithm is capable of accurately detecting incoming blockages more than ${\\sim} 90\\%$ of the time. Such blockage prediction ability is directly reflected in the accuracy of proactive hand-off, which also approaches 87%. This highlights a promising direction for enabling high reliability and low latency in future wireless networks.", "year": 2021, "publicationdate": "2021-02-18", "externalids": {"DOI": "10.1109/TVT.2021.3104219"}, "doi_lower": "10.1109/tvt.2021.3104219"}
{"paper_id": 260641252, "title": "Visual-LLM Zero-Shot Classification", "author_names": ["Misaki Matsuura", "Young Kyun", "AI JungMeta", "Ser Nam", "Lim"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 259103159, "title": "The Perils of Trial-and-Error Reward Design: Misdesign through Overfitting and Invalid Task Specifications", "author_names": ["S. Booth", "W. B. Knox", "J. Shah", "S. Niekum", "Peter Stone", "A. Allievi", "Bosch"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "In reinforcement learning (RL), a reward function that aligns exactly with a task's true performance metric is often necessarily sparse. For example, a true task metric might encode a reward of 1 upon success and 0 otherwise. The sparsity of these true task metrics can make them hard to learn from, so in practice they are often replaced with alternative dense reward functions. These dense reward functions are typically designed by experts through an ad hoc process of trial and error. In this process, experts manually search for a reward function that improves performance with respect to the task metric while also enabling an RL algorithm to learn faster. This process raises the question of whether the same reward function is optimal for all algorithms, i.e., whether the reward function can be overfit to a particular algorithm. In this paper, we study the consequences of this wide yet unexamined practice of trial-and-error reward design. We first conduct computational experiments that confirm that reward functions can be overfit to learning algorithms and their hyperparameters. We then conduct a controlled observation study which emulates expert practitioners' typical experiences of reward design, in which we similarly find evidence of reward function overfitting. We also find that experts' typical approach to reward design---of adopting a myopic strategy and weighing the relative goodness of each state-action pair---leads to misdesign through invalid task specifications, since RL algorithms use cumulative reward rather than rewards for individual state-action pairs as an optimization target.\n\nCode, data: github.com/serenabooth/reward-design-perils", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.1609/aaai.v37i5.25733"}, "doi_lower": "10.1609/aaai.v37i5.25733"}
{"paper_id": 265128566, "title": "AI-native Interconnect Framework for Integration of Large Language Model Technologies in 6G Systems", "author_names": ["Sasu Tarkoma", "Roberto Morabito", "Jaakko Sauvola"], "venue": "arXiv.org", "abstract": "The evolution towards 6G architecture promises a transformative shift in communication networks, with artificial intelligence (AI) playing a pivotal role. This paper delves deep into the seamless integration of Large Language Models (LLMs) and Generalized Pretrained Transformers (GPT) within 6G systems. Their ability to grasp intent, strategize, and execute intricate commands will be pivotal in redefining network functionalities and interactions. Central to this is the AI Interconnect framework, intricately woven to facilitate AI-centric operations within the network. Building on the continuously evolving current state-of-the-art, we present a new architectural perspective for the upcoming generation of mobile networks. Here, LLMs and GPTs will collaboratively take center stage alongside traditional pre-generative AI and machine learning (ML) algorithms. This union promises a novel confluence of the old and new, melding tried-and-tested methods with transformative AI technologies. Along with providing a conceptual overview of this evolution, we delve into the nuances of practical applications arising from such an integration. Through this paper, we envisage a symbiotic integration where AI becomes the cornerstone of the next-generation communication paradigm, offering insights into the structural and functional facets of an AI-native 6G network.", "year": 2023, "publicationdate": "2023-11-10", "externalids": {"DOI": "10.48550/arXiv.2311.05842"}, "doi_lower": "10.48550/arxiv.2311.05842"}
{"paper_id": 260865814, "title": "Big AI Models for 6G Wireless Networks: Opportunities, Challenges, and Research Directions", "author_names": ["Zi-Yuan Chen", "Zhaoyang Zhang", "Zhaohui Yang"], "venue": "IEEE wireless communications", "abstract": "Recently, big artificial intelligence models (BAIMs) represented by chatGPT have brought an incredible revolution. With the pre-trained BAIMs in certain fields, numerous downstream tasks can be accomplished with only few-shot, or even zero-shot, learning, and exhibit state-of-the-art performances. As widely envisioned, the big AI models can rapidly penetrate into major intelligent services and applications, and are able to run at low unit cost with high flexibility. In 6G wireless networks, to fully enable intelligent communication, sensing, and computing, apart from providing other intelligent wireless services and applications, it is of vital importance to design and deploy certain wireless BAIMs (wBAIMs). However, investigation into architecture design and system evaluation for wBAIM is still lacking. In this article, we provide a comprehensive discussion as well as some in-depth prospects on the demand, design, and deployment aspects of the wBAIM. We opine that wBAIM will be a recipe for the 6G wireless networks to build high-efficient, sustainable, versatile, and extensible wireless intelligence for numerous promising visions. Then, we provide the core characteristics, principles, and pilot studies to guide the design of wBAIMs, and discuss the key aspects of developing wBAIMs through identifying the differences between the existing BAIMs and the emerging wBAIMs. Finally, related research directions and potential solutions are outlined.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.1109/MWC.015.2300404"}, "doi_lower": "10.1109/mwc.015.2300404"}
{"paper_id": 267938558, "title": "Large Language Models for Telecom: The Next Big Thing?", "author_names": ["Lina Bariah", "Qiyang Zhao", "Han Zou", "Yu Tian", "Faouzi Bader", "M. Debbah"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.10249"}, "doi_lower": "10.48550/arxiv.2306.10249"}
{"paper_id": 267413048, "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems", "author_names": ["Shengzhe Xu", "Christo Kurisummoottil Thomas", "Omar Hashash", "Nikhil Muralidhar", "Walid Saad", "Naren Ramakrishnan"], "venue": "IEEE Network", "abstract": "Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.", "year": 2024, "publicationdate": "2024-01-30", "externalids": {"DOI": "10.1109/MNET.2024.3427313"}, "doi_lower": "10.1109/mnet.2024.3427313"}
{"paper_id": 271038693, "title": "Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions", "author_names": ["Shumaila Javaid", "R. A. Khalil", "Nasir Saeed", "Bin He", "M. Alouini"], "venue": "IEEE Open Journal of the Communications Society", "abstract": "Integrated satellite, aerial, and terrestrial networks (ISATNs) represent a sophisticated convergence of diverse communication technologies to ensure seamless connectivity across different altitudes and platforms. This paper explores the transformative potential of integrating Large Language Models (LLMs) into ISATNs, leveraging advanced Artificial Intelligence (AI) and Machine Learning (ML) capabilities to enhance these networks. We outline the current architecture of ISATNs and highlight the significant role LLMs can play in optimizing data flow, signal processing, and network management to advance 5G/6G communication technologies through advanced predictive algorithms and real-time decision-making. A comprehensive analysis of ISATN components is conducted, assessing how LLMs can effectively address traditional data transmission and processing bottlenecks. The paper delves into the network management challenges within ISATNs, emphasizing the necessity for sophisticated resource allocation strategies, traffic routing, and security management to ensure seamless connectivity and optimal performance under varying conditions. Furthermore, we examine the technical challenges and limitations associated with integrating LLMs into ISATNs, such as data integration for LLM processing, scalability issues, latency in decision-making processes, and the design of robust, fault-tolerant systems. The study also identifies critical future research directions for fully harnessing LLM capabilities in ISATNs, which is important for enhancing network reliability, optimizing performance, and achieving a truly interconnected and intelligent global network system.", "year": 2024, "publicationdate": "2024-07-05", "externalids": {"DOI": "10.1109/OJCOMS.2024.3522103"}, "doi_lower": "10.1109/ojcoms.2024.3522103"}
{"paper_id": 260865838, "title": "Large Language Models for Telecom: Forthcoming Impact on the Industry", "author_names": ["Ali Maatouk", "Nicola Piovesan", "Fadhel Ayed", "A. Domenico", "M. Debbah"], "venue": "IEEE Communications Magazine", "abstract": "Large language models (LLMs) – AI-driven models that can achieve general-purpose language understanding and generation – have emerged as a transformative force, revolutionizing fields well beyond natural language processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its impact on the landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining tasks, such as anomaly resolution and technical specification comprehension, which currently hinder operational efficiency and demand significant manpower and expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing them represents a significant stride toward fully harnessing the potential of LLMs, and unlocking their capabilities to the fullest extent within the telecom domain.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.1109/MCOM.001.2300473"}, "doi_lower": "10.1109/mcom.001.2300473"}
{"paper_id": 270062447, "title": "WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence", "author_names": ["Jiawei Shao", "Jingwen Tong", "Qiong Wu", "Wei Guo", "Zijian Li", "Zehong Lin", "Jun Zhang"], "venue": "J. Commun. Inf. Networks", "abstract": "The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.", "year": 2024, "publicationdate": "2024-05-27", "externalids": {"DOI": "10.48550/arXiv.2405.17053"}, "doi_lower": "10.48550/arxiv.2405.17053"}
{"paper_id": 265498773, "title": "Large Language Models for Networking: Applications, Enabling Techniques, and Challenges", "author_names": ["Yudong Huang", "Hongyang Du", "Xinyuan Zhang", "Dusist Niyato", "Jiawen Kang", "Zehui Xiong", "Shuo Wang", "Tao Huang"], "venue": "IEEE Network", "abstract": "The rapid evolution of network technologies and the growing complexity of network tasks necessitate a paradigm shift in how networks are designed, configured, and managed. With a wealth of knowledge and expertise, large language models (LLMs) are one of the most promising candidates. This paper aims to pave the way for constructing domain-adapted LLMs for networking. Firstly, we present potential LLM applications for vertical network fields and showcase the mapping from natural language to network language. Then, several enabling technologies are investigated, including parameter-efficient finetuning and prompt engineering. The insight is that language understanding and tool usage are both required for network LLMs. Driven by the idea of embodied intelligence, we propose the ChatNet, a domain-adapted network LLM framework with access to various external network tools. ChatNet can reduce the time required for burdensome network planning tasks significantly, leading to a substantial improvement in processing efficiency. Finally, key challenges and future research directions are highlighted.", "year": 2023, "publicationdate": "2023-11-29", "externalids": {"DOI": "10.1109/MNET.2024.3435752"}, "doi_lower": "10.1109/mnet.2024.3435752"}
{"paper_id": 220889238, "title": "Cellular Traffic Load Prediction with LSTM and Gaussian Process Regression", "author_names": ["Wei Wang", "Conghao Zhou", "Hongli He", "Wen Wu", "W. Zhuang", "X. Shen"], "venue": "ICC 2020 - 2020 IEEE International Conference on Communications (ICC)", "abstract": "Accurate cellular traffic load prediction is a pre-requisite for efficient and automatic network planning and management. Considering diverse users’ activities at different locations and times, it is technically challenging to characterize the network resource demands at different time scales via traditional prediction methods. In this paper, we propose to combine the long short-term memory (LSTM) and Gaussian process regression (GPR) to achieve accurate single-cell level cellular traffic prediction, using the open Milan cellular traffic dataset provided by Telecom Italia. Firstly, the dominant periodic components of the cellular data are extracted, and then the small components are fed to the LSTM network. To further improve the prediction accuracy, GPR is used to recover the residual components. Extensive experiments are conducted based on the dataset, and it is shown that the proposed LSTM-GPR scheme outperforms the benchmark schemes, especially for a relatively long time and burst traffic prediction.", "year": 2020, "publicationdate": "2020-06-01", "externalids": {"DOI": "10.1109/ICC40277.2020.9148738"}, "doi_lower": "10.1109/icc40277.2020.9148738"}
{"paper_id": 267770002, "title": "Generative AI for Secure Physical Layer Communications: A Survey", "author_names": ["Changyuan Zhao", "Hongyang Du", "D. Niyato", "Jiawen Kang", "Zehui Xiong", "Dong In Kim", "Xuemin Shen", "K. B. Letaief"], "venue": "IEEE Transactions on Cognitive Communications and Networking", "abstract": "Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.", "year": 2024, "publicationdate": "2024-02-21", "externalids": {"DOI": "10.1109/TCCN.2024.3438379"}, "doi_lower": "10.1109/tccn.2024.3438379"}
{"paper_id": 266693775, "title": "Generative AI-Driven Semantic Communication Networks: Architecture, Technologies, and Applications", "author_names": ["Chengsi Liang", "Hongyang Du", "Yao Sun", "Dusist Niyato", "Jiawen Kang", "Dezong Zhao", "M. Imran"], "venue": "IEEE Transactions on Cognitive Communications and Networking", "abstract": "Generative artificial intelligence (GAI) has emerged as a rapidly burgeoning field demonstrating significant potential in creating diverse content intelligently and automatically. To support such artificial intelligence-generated content (AIGC) services, future communication systems must fulfill stringent requirements, including high data rates, throughput, and low latency, while efficiently utilizing limited spectrum resources. Semantic communication (SemCom) has been deemed as a revolutionary communication scheme to tackle this challenge by conveying the meaning of messages instead of bit reproduction. GAI algorithms serve as the foundation for enabling intelligent and efficient SemCom systems in terms of model pre-training and fine-tuning, knowledge base construction, and resource allocation. Conversely, SemCom can provide AIGC services with low latency and high reliability due to its ability to perform semantic-aware encoding and compression of data, as well as knowledge- and context-based reasoning. In this survey, we break new ground by investigating the architecture, wireless communication schemes, and network management of GAI-driven SemCom networks. We first introduce a novel architecture for GAI-driven SemCom networks, comprising the data plane, physical infrastructure, and network control plane. In turn, we provide an in-depth analysis of the transceiver design and semantic effectiveness calculation of end-to-end GAI-driven SemCom systems. Subsequently, we present innovative generation level and knowledge management strategies in the proposed networks, including knowledge construction, update, and sharing, ensuring accurate and timely knowledge-based reasoning. Finally, we explore several promising use cases, i.e., autonomous driving, smart cities, and the Metaverse, to provide a comprehensive understanding and future direction of GAI-driven SemCom networks.", "year": 2023, "publicationdate": "2023-12-30", "externalids": {"DOI": "10.1109/TCCN.2024.3435524"}, "doi_lower": "10.1109/tccn.2024.3435524"}
{"paper_id": 258291863, "title": "Generative AI-Enabled Vehicular Networks: Fundamentals, Framework, and Case Study", "author_names": ["Ruichen Zhang", "Ke Xiong", "Hongyang Du", "Fellow Ieee Dusit Niyato", "Jiawen Kang", "Fellow Ieee Xuemin Shen", "L. F. I. H. Vincent Poor"], "venue": "IEEE Network", "abstract": "Recognizing the tremendous improvements that the integration of generative artificial intelligence (AI) can bring to intelligent transportation systems, this article explores the integration of generative AI technologies in vehicular networks, focusing on their potential applications and challenges. Generative AI, with its capabilities of generating realistic data and facilitating advanced decision-making processes, enhances various applications when combined with vehicular networks, such as navigation optimization, traffic prediction, data generation, and evaluation. Despite these promising applications, the integration of generative AI with vehicular networks faces several challenges, such as real-time data processing and decision-making, adapting to dynamic and unpredictable environments, as well as privacy and security concerns. To address these challenges, we propose a multi-modality semantic-aware framework to enhance the service quality of generative AI. By leveraging multi-modal and semantic communication technologies, the framework enables the use of text and image data for creating multi-modal content, providing more reliable guidance to receiving vehicles and ultimately improving system usability and efficiency. To further improve the reliability and efficiency of information transmission and reconstruction within the framework, taking generative AI-enabled vehicle-to-vehicle (V2V) as a case study, a deep reinforcement learning (DRL)-based approach is proposed for resource allocation. Finally, we discuss potential research directions and anticipated advancements in the field of generative AI-enabled vehicular networks.", "year": 2023, "publicationdate": "2023-04-21", "externalids": {"DOI": "10.1109/MNET.2024.3391767"}, "doi_lower": "10.1109/mnet.2024.3391767"}
{"paper_id": 264935643, "title": "The Age of Generative AI and AI-Generated Everything", "author_names": ["Hongyang Du", "Dusist Niyato", "Jiawen Kang", "Zehui Xiong", "Ping Zhang", "Shuguang Cui", "Xuemin Shen", "Shiwen Mao", "Zhu Han", "Abbas Jamalipour", "H. V. Poor", "Dong In Kim"], "venue": "IEEE Network", "abstract": "Generative AI (GAI) has emerged as a significant advancement in artificial intelligence, renowned for its language and image generation capabilities. This paper presents “AI-Generated Everything” (AIGX), a concept that extends GAI beyond mere content creation to real-time adaptation and control across diverse technological domains. In networking, AIGX collaborates closely with physical, data link, network, and application layers to enhance real-time network management that responds to various system and service settings as well as application and user requirements. Networks, in return, serve as crucial components in further AIGX capability optimization through the AIGX lifecycle, i.e., data collection, distributed pre-training, and rapid decision-making, thereby establishing a mutually enhancing interplay. Moreover, we offer an in-depth case study focused on power allocation to illustrate the interdependence between AIGX and networking systems. Through this exploration, the article analyzes the significant role of GAI for networking, clarifies the ways networks augment AIGX functionalities, and underscores the virtuous interactive cycle they form. It is hoped that this article will pave the way for subsequent future research aimed at fully unlocking the potential of GAI and networks.", "year": 2023, "publicationdate": "2023-11-02", "externalids": {"DOI": "10.1109/MNET.2024.3422241"}, "doi_lower": "10.1109/mnet.2024.3422241"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 261705660, "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics", "author_names": ["Jiayang Song", "Zhehua Zhou", "Jiawei Liu", "Chunrong Fang", "Zhan Shu", "Lei Ma"], "venue": "arXiv.org", "abstract": "Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach.", "year": 2023, "publicationdate": "2023-09-13", "externalids": {"DOI": "10.48550/arXiv.2309.06687"}, "doi_lower": "10.48550/arxiv.2309.06687"}
{"paper_id": 257255456, "title": "Reward Design with Language Models", "author_names": ["Minae Kwon", "Sang Michael Xie", "Kalesha Bullard", "Dorsa Sadigh"], "venue": "International Conference on Learning Representations", "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.48550/arXiv.2303.00001"}, "doi_lower": "10.48550/arxiv.2303.00001"}
{"paper_id": 264306288, "title": "Eureka: Human-Level Reward Design via Coding Large Language Models", "author_names": ["Yecheng Jason Ma", "William Liang", "Guanzhi Wang", "De-An Huang", "Osbert Bastani", "Dinesh Jayaraman", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "venue": "International Conference on Learning Representations", "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.", "year": 2023, "publicationdate": "2023-10-19", "externalids": {"DOI": "10.48550/arXiv.2310.12931"}, "doi_lower": "10.48550/arxiv.2310.12931"}
{"paper_id": 263908782, "title": "Large Language Models Are Zero-Shot Time Series Forecasters", "author_names": ["Nate Gruver", "Marc Finzi", "Shikai Qiu", "Andrew Gordon Wilson"], "venue": "Neural Information Processing Systems", "abstract": "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07820"}, "doi_lower": "10.48550/arxiv.2310.07820"}
{"paper_id": 259252045, "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge", "author_names": ["Yunxiang Li", "Zihan Li", "Kai Zhang", "Ruilong Dan", "Steven Jiang", "You Zhang"], "venue": "Cureus", "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.", "year": 2023, "publicationdate": "2023-03-24", "externalids": {"DOI": "10.7759/cureus.40895"}, "doi_lower": "10.7759/cureus.40895"}
{"paper_id": 263609325, "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models", "author_names": ["Ming Jin", "Shiyu Wang", "Lintao Ma", "Zhixuan Chu", "James Y. Zhang", "X. Shi", "Pin-Yu Chen", "Yuxuan Liang", "Yuan-Fang Li", "Shirui Pan", "Qingsong Wen"], "venue": "International Conference on Learning Representations", "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.01728"}, "doi_lower": "10.48550/arxiv.2310.01728"}
{"paper_id": 263605524, "title": "DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model", "author_names": ["Zhenhua Xu", "Yujia Zhang", "Enze Xie", "Zhen Zhao", "Yong Guo", "K. K. Wong", "Zhenguo Li", "Hengshuang Zhao"], "venue": "IEEE Robotics and Automation Letters", "abstract": "Multimodallarge language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.1109/LRA.2024.3440097"}, "doi_lower": "10.1109/lra.2024.3440097"}
{"paper_id": 264128019, "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems", "author_names": ["Junjie Zhang", "Yupeng Hou", "Ruobing Xie", "Wenqi Sun", "Julian McAuley", "Wayne Xin Zhao", "Leyu Lin", "Ji-rong Wen"], "venue": "The Web Conference", "abstract": "Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents' decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.", "year": 2023, "publicationdate": "2023-10-13", "externalids": {"DOI": "10.1145/3589334.3645537"}, "doi_lower": "10.1145/3589334.3645537"}
{"paper_id": 233210249, "title": "Not All Attention Is All You Need", "author_names": ["Hongqiu Wu", "Hai Zhao", "Min Zhang"], "venue": "arXiv.org", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.", "year": 2021, "publicationdate": "2021-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 207880429, "title": "Fast Transformer Decoding: One Write-Head is All You Need", "author_names": ["Noam Shazeer"], "venue": "arXiv.org", "abstract": "Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention \"heads\", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.", "year": 2019, "publicationdate": "2019-11-06", "externalids": {}, "doi_lower": null}
{"paper_id": 258833177, "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints", "author_names": ["J. Ainslie", "J. Lee-Thorp", "Michiel de Jong", "Yury Zemlyanskiy", "Federico Lebr'on", "Sumit K. Sanghai"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.13245"}, "doi_lower": "10.48550/arxiv.2305.13245"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 198953378, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "author_names": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "year": 2019, "publicationdate": "2019-07-26", "externalids": {}, "doi_lower": null}
{"paper_id": 202888986, "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "author_names": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "venue": "International Conference on Learning Representations", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.", "year": 2019, "publicationdate": "2019-09-26", "externalids": {}, "doi_lower": null}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "author_names": ["M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdel-rahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "publicationdate": "2019-10-29", "externalids": {"DOI": "10.18653/v1/2020.acl-main.703"}, "doi_lower": "10.18653/v1/2020.acl-main.703"}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 252715691, "title": "GLM-130B: An Open Bilingual Pre-trained Model", "author_names": ["Aohan Zeng", "Xiao Liu", "Zhengxiao Du", "Zihan Wang", "Hanyu Lai", "Ming Ding", "Zhuoyi Yang", "Yifan Xu", "Wendi Zheng", "Xiao Xia", "W. Tam", "Zixuan Ma", "Yufei Xue", "Jidong Zhai", "Wenguang Chen", "P. Zhang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.", "year": 2022, "publicationdate": "2022-10-05", "externalids": {"DOI": "10.48550/arXiv.2210.02414"}, "doi_lower": "10.48550/arxiv.2210.02414"}
{"paper_id": 214733361, "title": "Wiki-40B: Multilingual Language Model Dataset", "author_names": ["Mandy Guo", "Zihang Dai", "Denny Vrandečić", "Rami Al-Rfou"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2020, "publicationdate": "2020-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 202558505, "title": "SciBERT: A Pretrained Language Model for Scientific Text", "author_names": ["Iz Beltagy", "Kyle Lo", "Arman Cohan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.", "year": 2019, "publicationdate": "2019-03-01", "externalids": {"DOI": "10.18653/v1/D19-1371"}, "doi_lower": "10.18653/v1/d19-1371"}
{"paper_id": 247158549, "title": "A systematic evaluation of large language models of code", "author_names": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "Vincent J. Hellendoorn"], "venue": "MAPS@PLDI", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.", "year": 2022, "publicationdate": "2022-02-26", "externalids": {"DOI": "10.1145/3520312.3534862"}, "doi_lower": "10.1145/3520312.3534862"}
{"paper_id": 257378329, "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "author_names": ["Hugo Laurenccon", "Lucile Saulnier", "Thomas Wang", "Christopher Akiki", "Albert Villanova del Moral", "Teven Le Scao", "L. V. Werra", "Chenghao Mou", "E. G. Ponferrada", "Huu Nguyen", "Jorg Frohberg", "Mario vSavsko", "Quentin Lhoest", "Angelina McMillan-Major", "Gérard Dupont", "Stella Biderman", "Anna Rogers", "Loubna Ben Allal", "F. Toni", "Giada Pistilli", "Olivier Nguyen", "Somaieh Nikpoor", "Maraim Masoud", "Pierre Colombo", "Javier de la Rosa", "Paulo Villegas", "Tristan Thrush", "S. Longpre", "Sebastian Nagel", "Leon Weber", "M. Muñoz", "Jian Zhu", "Daniel Alexander van Strien", "Zaid Alyafeai", "Khalid Almubarak", "Minh Chien Vu", "Itziar Gonzalez-Dios", "Aitor Soroa Etxabe", "Kyle Lo", "Manan Dey", "Pedro Ortiz Suarez", "Aaron Gokaslan", "Shamik Bose", "David Ifeoluwa Adelani", "Long Phan", "H. Tran", "I. Yu", "S. Pai", "Jenny Chim", "Violette Lepercq", "Suzana Ilic", "Margaret Mitchell", "Sasha Luccioni", "Yacine Jernite"], "venue": "Neural Information Processing Systems", "abstract": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03915"}, "doi_lower": "10.48550/arxiv.2303.03915"}
{"paper_id": 13753208, "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates", "author_names": ["Taku Kudo"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.", "year": 2018, "publicationdate": "2018-04-29", "externalids": {"DOI": "10.18653/v1/P18-1007"}, "doi_lower": "10.18653/v1/p18-1007"}
{"paper_id": 231934213, "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models", "author_names": ["Zhuohan Li", "Siyuan Zhuang", "Shiyuan Guo", "Danyang Zhuo", "Hao Zhang", "D. Song", "Ion Stoica"], "venue": "International Conference on Machine Learning", "abstract": "Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe", "year": 2021, "publicationdate": "2021-02-16", "externalids": {}, "doi_lower": null}
{"paper_id": 53670168, "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism", "author_names": ["Yanping Huang", "Yonglong Cheng", "Dehao Chen", "HyoukJoong Lee", "Jiquan Ngiam", "Quoc V. Le", "Z. Chen"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2018, "publicationdate": "2018-11-16", "externalids": {}, "doi_lower": null}
{"paper_id": 269617042, "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models", "author_names": ["Samyam Rajbhandari", "Jeff Rasley", "Olatunji Ruwase", "Yuxiong He"], "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "abstract": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today’s hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8. 3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world’s largest language model at the time (17B parameters) with record breaking accuracy.", "year": 2019, "publicationdate": "2019-10-04", "externalids": {"DOI": "10.1109/SC41405.2020.00024"}, "doi_lower": "10.1109/sc41405.2020.00024"}
{"paper_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "author_names": ["Jason Wei", "Maarten Bosma", "Vincent Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "venue": "International Conference on Learning Representations", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 263218031, "title": "GPT-4V(ision) System Card", "author_names": [], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 202660943, "title": "Fine-Tuning Language Models from Human Preferences", "author_names": ["Daniel M. Ziegler", "Nisan Stiennon", "Jeff Wu", "Tom B. Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "G. Irving"], "venue": "arXiv.org", "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "year": 2019, "publicationdate": "2019-09-18", "externalids": {}, "doi_lower": null}
{"paper_id": 4787508, "title": "Deep Reinforcement Learning from Human Preferences", "author_names": ["P. Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "S. Legg", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "year": 2017, "publicationdate": "2017-06-12", "externalids": {}, "doi_lower": null}
{"paper_id": 258822910, "title": "LIMA: Less Is More for Alignment", "author_names": ["Chunting Zhou", "Pengfei Liu", "Puxin Xu", "Srini Iyer", "Jiao Sun", "Yuning Mao", "Xuezhe Ma", "Avia Efrat", "Ping Yu", "L. Yu", "Susan Zhang", "Gargi Ghosh", "M. Lewis", "Luke Zettlemoyer", "Omer Levy"], "venue": "Neural Information Processing Systems", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {}, "doi_lower": null}
{"paper_id": 254823489, "title": "Constitutional AI: Harmlessness from AI Feedback", "author_names": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "John Kernion", "Andy Jones", "A. Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Carol Chen", "Catherine Olsson", "Chris Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "E. Perez", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Kamilė Lukošiūtė", "Liane Lovitt", "M. Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noem'i Mercado", "Nova Dassarma", "R. Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "T. Henighan", "Tristan Hume", "Sam Bowman", "Zac Hatfield-Dodds", "Benjamin Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom B. Brown", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.08073"}, "doi_lower": "10.48550/arxiv.2212.08073"}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 233296494, "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "author_names": ["Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2022.acl-long.556"}, "doi_lower": "10.18653/v1/2022.acl-long.556"}
{"paper_id": 257505009, "title": "The Learnability of In-Context Learning", "author_names": ["Noam Wies", "Yoav Levine", "A. Shashua"], "venue": "Neural Information Processing Systems", "abstract": "In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to prove that, under mild assumptions, when the pretraining distribution is a mixture of latent tasks (a model often considered for natural language pretraining), these tasks can be efficiently learned via in-context learning, even though the model's weights are unchanged and the input significantly diverges from the pretraining distribution. Our theoretical analysis reveals that in this setting, in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings. We hope that the in-context learnability framework presented in this paper will facilitate future progress towards a deeper understanding of this important new learning paradigm.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.48550/arXiv.2303.07895"}, "doi_lower": "10.48550/arxiv.2303.07895"}
{"paper_id": 257378479, "title": "Larger language models do in-context learning differently", "author_names": ["Jerry W. Wei", "Jason Wei", "Yi Tay", "Dustin Tran", "Albert Webson", "Yifeng Lu", "Xinyun Chen", "Hanxiao Liu", "Da Huang", "Denny Zhou", "Tengyu Ma"], "venue": "arXiv.org", "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03846"}, "doi_lower": "10.48550/arxiv.2303.03846"}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 248986239, "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "author_names": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "Ed H. Chi"], "venue": "International Conference on Learning Representations", "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "year": 2022, "publicationdate": "2022-05-21", "externalids": {"DOI": "10.48550/arXiv.2205.10625"}, "doi_lower": "10.48550/arxiv.2205.10625"}
{"paper_id": 258558102, "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models", "author_names": ["Lei Wang", "Wanyu Xu", "Yihuai Lan", "Zhiqiang Hu", "Yunshi Lan", "Roy Ka-Wei Lee", "Ee-Peng Lim"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.", "year": 2023, "publicationdate": "2023-05-06", "externalids": {"DOI": "10.48550/arXiv.2305.04091"}, "doi_lower": "10.48550/arxiv.2305.04091"}
{"paper_id": 277621538, "title": "TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation", "author_names": ["Tianyu Cui", "Xinjie Lin", "Sijia Li", "Miao Chen", "Qilei Yin", "Qi Li", "Ke Xu"], "venue": "arXiv.org", "abstract": "Machine learning (ML) powered network traffic analysis has been widely used for the purpose of threat detection. Unfortunately, their generalization across different tasks and unseen data is very limited. Large language models (LLMs), known for their strong generalization capabilities, have shown promising performance in various domains. However, their application to the traffic analysis domain is limited due to significantly different characteristics of network traffic. To address the issue, in this paper, we propose TrafficLLM, which introduces a dual-stage fine-tuning framework to learn generic traffic representation from heterogeneous raw traffic data. The framework uses traffic-domain tokenization, dual-stage tuning pipeline, and extensible adaptation to help LLM release generalization ability on dynamic traffic analysis tasks, such that it enables traffic detection and traffic generation across a wide range of downstream tasks. We evaluate TrafficLLM across 10 distinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of 0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than existing detection and generation methods. It also shows strong generalization on unseen traffic with an 18.6% performance improvement. We further evaluate TrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy to scale and achieves accurate detection performance on enterprise traffic.", "year": 2025, "publicationdate": "2025-04-05", "externalids": {"DOI": "10.48550/arXiv.2504.04222"}, "doi_lower": "10.48550/arxiv.2504.04222"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 248942462, "title": "DialSummEval: Revisiting Summarization Evaluation for Dialogues", "author_names": ["Mingqi Gao", "Xiaojun Wan"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multi-faceted dataset of human judgments containing the outputs of 14 models on SAMSum.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.naacl-main.418"}, "doi_lower": "10.18653/v1/2022.naacl-main.418"}
{"paper_id": 263620702, "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference", "author_names": ["S. Samsi", "Dan Zhao", "Joseph McDonald", "Baolin Li", "Adam Michaleas", "Michael Jones", "William Bergeron", "J. Kepner", "Devesh Tiwari", "V. Gadepally"], "venue": "IEEE Conference on High Performance Extreme Computing", "abstract": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs-despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA-a recent state-of-the-art LLM-developed by Meta AI on two generations of popular GPUs (NVIDIA V100 & A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.", "year": 2023, "publicationdate": "2023-09-25", "externalids": {"DOI": "10.1109/HPEC58863.2023.10363447"}, "doi_lower": "10.1109/hpec58863.2023.10363447"}
{"paper_id": 262825233, "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models", "author_names": ["Ahmad Faiz", "S. Kaneda", "Ruhan Wang", "Rita Osi", "Parteek Sharma", "Fan Chen", "Lei Jiang"], "venue": "International Conference on Learning Representations", "abstract": "The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \\textit{\\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \\url{https://github.com/SotaroKaneda/MLCarbon}.", "year": 2023, "publicationdate": "2023-09-25", "externalids": {"DOI": "10.48550/arXiv.2309.14393"}, "doi_lower": "10.48550/arxiv.2309.14393"}
{"paper_id": 248887431, "title": "Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models", "author_names": ["Joseph McDonald", "Baolin Li", "Nathan C Frey", "Devesh Tiwari", "V. Gadepally", "S. Samsi"], "venue": "NAACL-HLT", "abstract": "The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15\\% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.", "year": 2022, "publicationdate": "2022-05-19", "externalids": {"DOI": "10.18653/v1/2022.findings-naacl.151"}, "doi_lower": "10.18653/v1/2022.findings-naacl.151"}
{"paper_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "author_names": ["Yu-Chu Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Kaijie Zhu", "Hao Chen", "Linyi Yang", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Weirong Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qian Yang", "Xingxu Xie"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1145/3641289"}, "doi_lower": "10.1145/3641289"}
{"paper_id": 258547324, "title": "Can Large Language Models Transform Computational Social Science?", "author_names": ["Caleb Ziems", "William B. Held", "Omar Shaikh", "Jiaao Chen", "Zhehao Zhang", "Diyi Yang"], "venue": "International Conference on Computational Logic", "abstract": "Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.", "year": 2023, "publicationdate": "2023-04-12", "externalids": {"DOI": "10.1162/coli_a_00502"}, "doi_lower": "10.1162/coli_a_00502"}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher Ré", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Yüksekgönül", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 257316425, "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models", "author_names": ["Ning Ding", "Yujia Qin", "Guang Yang", "Fu Wei", "Zonghan Yang", "Yusheng Su", "Shengding Hu", "Yulin Chen", "Chi-Min Chan", "Weize Chen", "Jing Yi", "Weilin Zhao", "Xiaozhi Wang", "Zhiyuan Liu", "Haitao Zheng", "Jianfei Chen", "Y. Liu", "Jie Tang", "Juanzi Li", "Maosong Sun"], "venue": "Nature Machine Intelligence", "abstract": "With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs. Training a deep neural network can be costly but training time is reduced when a pre-trained network can be adapted to different use cases. Ideally, only a small number of parameters needs to be changed in this process of fine-tuning, which can then be more easily distributed. In this Analysis, different methods of fine-tuning with only a small number of parameters are compared on a large set of natural language processing tasks.", "year": 2023, "publicationdate": "2023-03-01", "externalids": {"DOI": "10.1038/s42256-023-00626-4"}, "doi_lower": "10.1038/s42256-023-00626-4"}
{"paper_id": 257771867, "title": "Efficient Parallel Split Learning Over Resource-Constrained Wireless Edge Networks", "author_names": ["Zhengyi Lin", "Guangyu Zhu", "Yiqin Deng", "Xianhao Chen", "Yue Gao", "Kaibin Huang", "Yuguang Fang"], "venue": "IEEE Transactions on Mobile Computing", "abstract": "The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple edge devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and a large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of activations’ gradients for backpropagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabilities at edge devices, we jointly optimize subchannel allocation, power control, and cut layer selection to minimize the per-round latency. Simulation results show that the proposed EPSL framework significantly decreases the training latency needed to achieve a target accuracy compared with the state-of-the-art benchmarks, and the tailored resource management and layer split strategy can considerably reduce latency than the counterpart without optimization.", "year": 2023, "publicationdate": "2023-03-26", "externalids": {"DOI": "10.1109/TMC.2024.3359040"}, "doi_lower": "10.1109/tmc.2024.3359040"}
{"paper_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "author_names": ["Tim Dettmers", "Artidoro Pagnoni", "Ari Holtzman", "Luke Zettlemoyer"], "venue": "Neural Information Processing Systems", "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14314"}, "doi_lower": "10.48550/arxiv.2305.14314"}
{"paper_id": 266362016, "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory", "author_names": ["Keivan Alizadeh-Vahid", "Iman Mirzadeh", "Dmitry Belenko", "Karen Khatamifard", "Minsik Cho", "C. C. D. Mundo", "Mohammad Rastegari", "Mehrdad Farajtabar"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,\"windowing\"strategically reduces data transfer by reusing previously activated neurons, and second,\"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.", "year": 2023, "publicationdate": "2023-12-12", "externalids": {"DOI": "10.48550/arXiv.2312.11514"}, "doi_lower": "10.48550/arxiv.2312.11514"}
{"paper_id": 39871222, "title": "Qualcomm snapdragon \"bullet train\"", "author_names": ["Marilyn A. Pipes"], "venue": "SIGGRAPH Computer Animation Festival", "abstract": null, "year": 2015, "publicationdate": "2015-07-31", "externalids": {"DOI": "10.1145/2745234.2746856"}, "doi_lower": "10.1145/2745234.2746856"}
{"paper_id": 265294855, "title": "EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge", "author_names": ["Bufang Yang", "Lixing He", "Neiwen Ling", "Zhenyu Yan", "Guoliang Xing", "Xian Shuai", "Xiaozhe Ren", "Xin Jiang"], "venue": "ACM International Conference on Embedded Networked Sensor Systems", "abstract": "Deep Learning (DL) models have been widely deployed on IoT devices with the help of advancements in DL algorithms and chips. However, the limited resources of edge devices make these on-device DL models hard to be generalizable to diverse environments and tasks. Although the recently emerged foundation models (FMs) show impressive generalization power, how to effectively leverage the rich knowledge of FMs on resource-limited edge devices is still not explored. In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with open-set recognition capability. EdgeFM selectively uploads unlabeled data to query the FM on the cloud and customizes the specific knowledge and architectures for edge models. Meanwhile, EdgeFM conducts dynamic model switching at run-time taking into account both data uncertainty and dynamic network variations, which ensures the accuracy always close to the original FM. We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on three public datasets and two self-collected datasets. Results show that EdgeFM can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy increase compared with the baseline.", "year": 2023, "publicationdate": "2023-11-12", "externalids": {"DOI": "10.1145/3625687.3625793"}, "doi_lower": "10.1145/3625687.3625793"}
{"paper_id": 257985497, "title": "Instruction Tuning with GPT-4", "author_names": ["Baolin Peng", "Chunyuan Li", "Pengcheng He", "Michel Galley", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {}, "doi_lower": null}
{"paper_id": 261049152, "title": "Instruction Tuning for Large Language Models: A Survey", "author_names": ["Shengyu Zhang", "Linfeng Dong", "Xiaoya Li", "Sen Zhang", "Xiaofei Sun", "Shuhe Wang", "Jiwei Li", "Runyi Hu", "Tianwei Zhang", "Fei Wu", "Guoyin Wang"], "venue": "ACM Computing Surveys", "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis of aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.1145/3777411"}, "doi_lower": "10.1145/3777411"}
{"paper_id": 258508784, "title": "Tabi: An Efficient Multi-Level Inference System for Large Language Models", "author_names": ["Yiding Wang", "Kai Chen", "Haisheng Tan", "Kun Guo"], "venue": "European Conference on Computer Systems", "abstract": "Today's trend of building ever larger language models (LLMs), while pushing the performance of natural language processing, adds significant latency to the inference stage. We observe that due to the diminishing returns of adding parameters to LLMs, a smaller model could make the same prediction as a costly LLM for a majority of queries. Based on this observation, we design Tabi, an inference system with a multi-level inference engine that serves queries using small models and optional LLMs for demanding applications. Tabi is optimized for discriminative models (i.e., not generative LLMs) in a serving framework. Tabi uses the calibrated confidence score to decide whether to return the accurate results of small models extremely fast or re-route them to LLMs. For re-routed queries, it uses attention-based word pruning and weighted ensemble techniques to offset the system overhead and accuracy loss. We implement and evaluate Tabi with multiple tasks and models. Our result shows that Tabi achieves 21%-40% average latency reduction (with comparable tail latency) over the state-of-the-art while meeting LLM-grade high accuracy targets.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.1145/3552326.3587438"}, "doi_lower": "10.1145/3552326.3587438"}
{"paper_id": 261660737, "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference", "author_names": ["Daliang Xu", "Wangsong Yin", "Xin Jin", "Y. Zhang", "Shiyun Wei", "Mengwei Xu", "Xuanzhe Liu"], "venue": "arXiv.org", "abstract": "Generative tasks, such as text generation and question answering, hold a crucial position in the realm of mobile applications. Due to their sensitivity to privacy concerns, there is a growing demand for their execution directly on mobile devices. Currently, the execution of these generative tasks heavily depends on Large Language Models (LLMs). Nevertheless, the limited memory capacity of these devices presents a formidable challenge to the scalability of such models. In our research, we introduce LLMCad, an innovative on-device inference engine specifically designed for efficient generative Natural Language Processing (NLP) tasks. The core idea behind LLMCad revolves around model collaboration: a compact LLM, residing in memory, takes charge of generating the most straightforward tokens, while a high-precision LLM steps in to validate these tokens and rectify any identified errors. LLMCad incorporates three novel techniques: (1) Instead of generating candidate tokens in a sequential manner, LLMCad employs the smaller LLM to construct a token tree, encompassing a wider range of plausible token pathways. Subsequently, the larger LLM can efficiently validate all of these pathways simultaneously. (2) It employs a self-adjusting fallback strategy, swiftly initiating the verification process whenever the smaller LLM generates an erroneous token. (3) To ensure a continuous flow of token generation, LLMCad speculatively generates tokens during the verification process by implementing a compute-IO pipeline. Through an extensive series of experiments, LLMCad showcases an impressive token generation speed, achieving rates up to 9.3x faster than existing inference engines.", "year": 2023, "publicationdate": "2023-09-08", "externalids": {"DOI": "10.48550/arXiv.2309.04255"}, "doi_lower": "10.48550/arxiv.2309.04255"}
{"paper_id": 259836926, "title": "NetGPT: An AI-Native Network Architecture for Provisioning Beyond Personalized Generative Services", "author_names": ["Yuxuan Chen", "Rongpeng Li", "Zhifeng Zhao", "Chenghui Peng", "Jianjun Wu", "E. Hossain", "Honggang Zhang"], "venue": "IEEE Network", "abstract": "Large language models (LLMs) have triggered tremendous success to empower our daily life by generative information. The personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology is promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, we put forward NetGPT to capably synergize appropriate LLMs at the edge and the cloud based on their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with the cloud LLM. In particular, we present the feasibility of NetGPT by leveraging low-rank adaptation-based fine-tuning of open-source LLMs (i.e., GPT-2-base model and LLaMA model), and conduct comprehensive numerical comparisons with alternative cloudedge collaboration or cloud-only techniques, so as to demonstrate the superiority of NetGPT. Subsequently, we highlight the essential changes required for an artificial intelligence (AI)-native network architecture towards NetGPT, with emphasis on deeper integration of communications and computing resources and careful calibration of logical AI workflows. Furthermore, we demonstrate several benefits of NetGPT, which come as by-products, as the edge LLMs’ capability to predict trends and infer intents promises a unified solution for intelligent network management & orchestration. We argue that NetGPT is a promising Al-native network architecture for provisioning beyond personalized generative services.", "year": 2024, "publicationdate": "2024-11-01", "externalids": {"DOI": "10.1109/MNET.2024.3376419"}, "doi_lower": "10.1109/mnet.2024.3376419"}
{"paper_id": 258332142, "title": "Cooperative Hierarchical Deep Reinforcement Learning based Joint Sleep, Power, and RIS Control for Energy-Efficient HetNet", "author_names": ["Hao Zhou", "Medhat H. M. Elsayed", "Majid Bavand", "Raimundas Gaigalas", "Steve Furr", "M. Erol-Kantarci"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2304.13226"}, "doi_lower": "10.48550/arxiv.2304.13226"}
{"paper_id": 275174947, "title": "Hardware Accelerator for Bidirectional Encoder Representations from Transformers (BERT)", "author_names": ["Yiming Wang"], "venue": "International Congress of Mathematicans", "abstract": "Recently Bidirectional Encoder Representations from Transformers (BERT) model has gained lots of attention because of its state-of-the-art performance in multiple natu-ral language processing (NLP) tasks. However, just like many other deep learning based tasks, large model size and intensive computation load of BERT make it difficult and expensive to run and implement on general purpose processors. The proposed hardware accelerator for BERT model realizes faster inference speed and higher energy efficiency. Design procedure is elaborated with two stages: model compression and hardware architecture. Quantization is chosen as the compression technique because of good speed-up performance as well as small model size and less complexity. In the hardware design, systolic tensor array (STA) is applied as processing elements (PE) array to achieve lower area and power consumption by reducing the ratio between registers and number of Floating-point operations per second (FLOPS). Dedicated hardware is designed for Softmax and layer normalization operations. Mathematical transformation is used to replace complicate nonlinear functions with simple operations to reduce the required hardware resources. Performance is evaluated based on transformer-base model. The maximum speed of overall hardware design is 125 MHz and the total latency is 165.9 us. Compared to the same task run on GPU, 22.4x and 7.5x speed up are achieved in multi-head-attention (MHA) and feed-forward networks(FFN) separately. The peak performance of this design is 4.1 TOPs/s and the maximum required memory bandwidth is 80 GB/s.", "year": 2024, "publicationdate": "2024-12-14", "externalids": {"DOI": "10.1109/ICM63406.2024.10815822"}, "doi_lower": "10.1109/icm63406.2024.10815822"}
{"paper_id": 283319582, "title": "Natural Language Processing for Security Policy and Log Analysis", "author_names": [], "venue": "International Journal of Research in all Subjects in Multi Languages", "abstract": null, "year": 2022, "publicationdate": "2022-04-01", "externalids": {"DOI": "10.63345/ijrsml.v10.i4.1"}, "doi_lower": "10.63345/ijrsml.v10.i4.1"}
{"paper_id": 258833510, "title": "Observations on LLMs for Telecom Domain: Capabilities and Limitations", "author_names": ["Sumit Soman", "G. RanjaniH"], "venue": "International Conference on AI-ML-Systems", "abstract": "The landscape for building conversational interfaces (chatbots) has witnessed a paradigm shift with recent developments in generative Artificial Intelligence (AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and GPT4), Google’s Bard, Large Language Model Meta AI (LLaMA), among others. In this paper, we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services. Using Cradlepoint’s publicly available data for our experiments, we present a comparative analysis of the responses from such models for multiple use-cases including domain adaptation for terminology and product taxonomy, context continuity, robustness to input perturbations and errors. We believe this evaluation would provide useful insights to data scientists engaged in building customized conversational interfaces for domain-specific requirements.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.1145/3639856.3639892"}, "doi_lower": "10.1145/3639856.3639892"}
{"paper_id": 258967345, "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models", "author_names": ["Bailin Wang", "Zi Wang", "Xuezhi Wang", "Yuan Cao", "R. Saurous", "Yoon Kim"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.48550/arXiv.2305.19234"}, "doi_lower": "10.48550/arxiv.2305.19234"}
{"paper_id": 260865807, "title": "Enhancing Network Management Using Code Generated by Large Language Models", "author_names": ["Sathiya Kumaran Mani", "Yajie Zhou", "Kevin Hsieh", "Santiago Segarra", "Trevor Eberl", "Eliran Azulai", "Ido Frizler", "Ranveer Chandra", "Srikanth Kandula"], "venue": "ACM Workshop on Hot Topics in Networks", "abstract": "Analyzing network topologies and communication graphs is essential in modern network management. However, the lack of a cohesive approach results in a steep learning curve, increased errors, and inefficiencies. In this paper, we present a novel approach that enables natural-language-based network management experiences, leveraging large language models (LLMs) to generate task-specific code from natural language queries. This method addresses the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, removing the need to share network data with LLMs, and focusing on application-specific requests combined with program synthesis techniques. We develop and evaluate a prototype system using benchmark applications, demonstrating high accuracy, cost-effectiveness, and potential for further improvements using complementary program synthesis techniques.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.1145/3626111.3628183"}, "doi_lower": "10.1145/3626111.3628183"}
{"paper_id": 252595945, "title": "Repairing Bugs in Python Assignments Using Large Language Models", "author_names": ["Jialu Zhang", "J. Cambronero", "Sumit Gulwani", "Vu Le", "R. Piskac", "Gustavo Soares", "Gust Verbruggen"], "venue": "arXiv.org", "abstract": "Students often make mistakes on their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate MMAPR on 286 real student programs and compare to a baseline built by combining a state-of-the-art Python syntax repair engine, BIFI, and state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that MMAPR can fix more programs and produce smaller patches on average.", "year": 2022, "publicationdate": "2022-09-29", "externalids": {"DOI": "10.48550/arXiv.2209.14876"}, "doi_lower": "10.48550/arxiv.2209.14876"}
{"paper_id": 254926675, "title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation", "author_names": ["Shailja Thakur", "Baleegh Ahmad", "Zhenxing Fan", "H. Pearce", "Benjamin Tan", "R. Karri", "Brendan Dolan-Gavitt", "S. Garg"], "venue": "Design, Automation and Test in Europe", "abstract": "Automating hardware design could obviate a signif-icant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). We release our training/evaluation scripts and LLM checkpoints as open source contributions.", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.23919/DATE56975.2023.10137086"}, "doi_lower": "10.23919/date56975.2023.10137086"}
{"paper_id": 265500844, "title": "LLM-Based Policy Generation for Intent-Based Management of Applications", "author_names": ["Kristina Dzeparoska", "Jieyu Lin", "A. Tizghadam", "Alberto Leon-Garcia"], "venue": "Conference on Network and Service Management", "abstract": "Automated management requires decomposing high-level user requests, such as intents, to an abstraction that the system can understand and execute. This is challenging because even a simple intent requires performing a number of ordered steps. And the task of identifying and adapting these steps (as conditions change) requires a decomposition approach that cannot be exactly pre-defined beforehand. To tackle these challenges and support automated intent decomposition and execution, we explore the few-shot capability of Large Language Models (LLMs). We propose a pipeline that progressively decomposes intents by generating the required actions using a policy-based abstraction. This allows us to automate the policy execution by creating a closed control loop for the intent deployment. To do so, we generate and map the policies to APIs and form application management loops that perform the necessary monitoring, analysis, planning and execution. We evaluate our proposal with a use-case to fulfill and assure an application service chain of virtual network functions. Using our approach, we can generalize and generate the necessary steps to realize intents, thereby enabling intent automation for application management.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.23919/CNSM59352.2023.10327837"}, "doi_lower": "10.23919/cnsm59352.2023.10327837"}
{"paper_id": 261696620, "title": "Making Network Configuration Human Friendly", "author_names": ["Changjie Wang", "Mariano Scazzariello", "Alireza Farshin", "Dejan Kostic", "Marco Chiesa"], "venue": "arXiv.org", "abstract": "This paper explores opportunities to utilize Large Language Models (LLMs) to make network configuration human-friendly, simplifying the configuration of network devices and minimizing errors. We examine the effectiveness of these models in translating high-level policies and requirements (i.e., specified in natural language) into low-level network APIs, which requires understanding the hardware and protocols. More specifically, we propose NETBUDDY for generating network configurations from scratch and modifying them at runtime. NETBUDDY splits the generation of network configurations into fine-grained steps and relies on self-healing code-generation approaches to better take advantage of the full potential of LLMs. We first thoroughly examine the challenges of using these models to produce a fully functional&correct configuration, and then evaluate the feasibility of realizing NETBUDDY by building a proof-of-concept solution using GPT-4 to translate a set of high-level requirements into P4 and BGP configurations and run them using the Kathar\\'a network emulator.", "year": 2023, "publicationdate": "2023-09-12", "externalids": {"DOI": "10.48550/arXiv.2309.06342"}, "doi_lower": "10.48550/arxiv.2309.06342"}
{"paper_id": 259766303, "title": "What do LLMs need to Synthesize Correct Router Configurations?", "author_names": ["Rajdeep Mondal", "Alan Tang", "Ryan Beckett", "T. Millstein", "G. Varghese"], "venue": "ACM Workshop on Hot Topics in Networks", "abstract": "We investigate whether Large Language Models (e.g., GPT-4) can synthesize correct router configurations with reduced manual effort. We find GPT-4 works very badly by itself, producing promising draft configurations but with egregious errors in topology, syntax, and semantics. Our strategy, that we call Verified Prompt Programming, is to combine GPT-4 with verifiers, and use localized feedback from the verifier to automatically correct errors. Verification requires a specification and actionable localized feedback to be effective. We show results for two use cases: translating from Cisco to Juniper configurations on a single router, and implementing a no-transit policy on multiple routers. While human input is still required, if we define the leverage as the number of automated prompts to the number of human prompts, our experiments show a leverage of 10X for Juniper translation, and 6X for implementing the no-transit policy, ending with verified configurations.", "year": 2023, "publicationdate": "2023-07-11", "externalids": {"DOI": "10.1145/3626111.3628194"}, "doi_lower": "10.1145/3626111.3628194"}
{"paper_id": 279116672, "title": "TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge", "author_names": ["Ali Maatouk", "Fadhel Ayed", "Nicola Piovesan", "Antonio De Domenico", "Mérouane Debbah", "Zhi-Quan Luo"], "venue": "IEEE Network", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.1109/mnet.2025.3576035"}, "doi_lower": "10.1109/mnet.2025.3576035"}
{"paper_id": 248870432, "title": "The Evolution of Telecom Business, Economy, Policies and Regulations", "author_names": ["Eva Ibarrola", "K. Jakobs", "M. Sherif", "D. Sparrell"], "venue": "IEEE Communications Magazine", "abstract": null, "year": 2022, "publicationdate": "2022-05-01", "externalids": {"DOI": "10.1109/mcom.2022.9777264"}, "doi_lower": "10.1109/mcom.2022.9777264"}
{"paper_id": 220919723, "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "author_names": ["Yu Gu", "Robert Tinn", "Hao Cheng", "Michael R. Lucas", "N. Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon"], "venue": "ACM Trans. Comput. Heal.", "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.", "year": 2020, "publicationdate": "2020-07-31", "externalids": {"DOI": "10.1145/3458754"}, "doi_lower": "10.1145/3458754"}
{"paper_id": 260460088, "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author_names": ["Payal Bajaj", "Daniel Fernando Campos", "Nick Craswell", "Li Deng", "Jianfeng Gao", "Xiaodong Liu", "Rangan Majumder", "Andrew McNamara", "Bhaskar Mitra", "Tri Minh Nguyen", "Mir Rosenberg", "Xia Song", "A. Stoica", "Saurabh Tiwary", "Tong Wang"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-11-28", "externalids": {}, "doi_lower": null}
{"paper_id": 252668917, "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "author_names": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Haiquan Wang", "Yingbo Zhou", "S. Savarese", "Caiming Xiong"], "venue": "International Conference on Learning Representations", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "year": 2022, "publicationdate": "2022-03-25", "externalids": {}, "doi_lower": null}
{"paper_id": 216080785, "title": "Code Smells and Refactoring: A Tertiary Systematic Review of Challenges and Observations", "author_names": ["Guilherme Lacerda", "Fábio Petrillo", "M. Pimenta", "Yann-Gaël Guéhéneuc"], "venue": "Journal of Systems and Software", "abstract": "Abstract Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring.", "year": 2020, "publicationdate": "2020-04-22", "externalids": {"DOI": "10.17632/72fvy4r3gj.1"}, "doi_lower": "10.17632/72fvy4r3gj.1"}
{"paper_id": 219811196, "title": "openwifi: a free and open-source IEEE802.11 SDR implementation on SoC", "author_names": ["Xianjun Jiao", "Wei Liu", "M. Mehari", "Muhammad Aslam", "I. Moerman"], "venue": "IEEE Vehicular Technology Conference", "abstract": "Open source Software Defined Radio (SDR) project, such as srsLTE and Open Air Interface (OAI), has been widely used for 4G/5G research. However the SDR implementation of the IEEE802.11 (Wi-Fi) is still difficult. The Wi-Fi Short InterFrame Space (SIFS) requires acknowledgement (ACK) packet being sent out in $10 \\mu \\mathrm {s}/ 16 \\mu \\mathrm {s}(2.4$ GHz/5GHz) after receiving a packet successfully, thus the Personal Computer (PC) based SDR architecture hardly can be used due to the latency $(\\ge 100 \\mu \\mathrm {s})$ between PC and Radio Frequency (RF) front-end. Researchers have to do simulation, hack a commercial chip or buy an expensive reference design to test their ideas. To change this situation, we have developed an open-source full-stack IEEE802.11a/g/n SDR implementation — openwifi. It is based on Xilinx Zynq Systemon-Chip (SoC) that includes Field Programmable Gate Array (FPGA) and ARM processor. With the low latency connection between FPGA and RF front-end, the most critical SIFS timing is achieved by implementing Physical layer (PHY) and low level Media Access Control (low MAC) in FPGA. The corresponding driver is implemented in the embedded Linux running on the ARM processor. The driver instantiates Application Programming Interfaces (APIs) defined by Linux mac80211 subsystem, which is widely used for most SoftMAC Wi-Fi chips. Researchers could study and modify openwifi easily thanks to the modular design. Compared to PC based SDR, the SoC is also a better choice for portable and embedded scenarios.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.1109/VTC2020-Spring48590.2020.9128614"}, "doi_lower": "10.1109/vtc2020-spring48590.2020.9128614"}
{"paper_id": 2623404, "title": "A Survey of Software-Defined Networking: Past, Present, and Future of Programmable Networks", "author_names": ["B. A. A. Nunes", "Marc Mendonca", "X. Nguyen", "K. Obraczka", "T. Turletti"], "venue": "IEEE Communications Surveys and Tutorials", "abstract": null, "year": 2014, "publicationdate": "2014-02-13", "externalids": {"DOI": "10.1109/SURV.2014.012214.00180"}, "doi_lower": "10.1109/surv.2014.012214.00180"}
{"paper_id": 4787777, "title": "NetComplete: Practical Network-Wide Configuration Synthesis with Autocompletion", "author_names": ["Ahmed El-Hassany", "Petar Tsankov", "Laurent Vanbever", "Martin T. Vechev"], "venue": "Symposium on Networked Systems Design and Implementation", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263174742, "title": "When Configuration Verification Meets Machine Learning: A DRL Approach for Finding Minimum k-Link Failures", "author_names": ["Hao Chen", "Yili Jin", "Weipeng Wang", "Wei Liu", "Lizhao You", "Liqun Fu", "Qiao Xiang"], "venue": "Asia-Pacific Network Operations and Management Symposium", "abstract": null, "year": 2023, "publicationdate": "2023-09-06", "externalids": {}, "doi_lower": null}
{"paper_id": 266036669, "title": "A General Approach to Network Configuration Analysis", "author_names": [], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 253080552, "title": "SecureBERT: A Domain-Specific Language Model for Cybersecurity", "author_names": ["Ehsan Aghaei", "Xi Niu", "W. Shadid", "E. Al-Shaer"], "venue": "Security and Privacy in Communication Networks", "abstract": "Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures. This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT\\footnote{\\url{https://github.com/ehsanaghaei/SecureBERT}} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.", "year": 2022, "publicationdate": "2022-04-06", "externalids": {"DOI": "10.1007/978-3-031-25538-0_3"}, "doi_lower": "10.1007/978-3-031-25538-0_3"}
{"paper_id": 243794734, "title": "CyBERT: Cybersecurity Claim Classification by Fine-Tuning the BERT Language Model", "author_names": ["Kimia Ameri", "M. Hempel", "H. Sharif", "Juan Lopez", "K. Perumalla"], "venue": "Journal of Cybersecurity and Privacy", "abstract": "We introduce CyBERT, a cybersecurity feature claims classifier based on bidirectional encoder representations from transformers and a key component in our semi-automated cybersecurity vetting for industrial control systems (ICS). To train CyBERT, we created a corpus of labeled sequences from ICS device documentation collected across a wide range of vendors and devices. This corpus provides the foundation for fine-tuning BERT’s language model, including a prediction-guided relabeling process. We propose an approach to obtain optimal hyperparameters, including the learning rate, the number of dense layers, and their configuration, to increase the accuracy of our classifier. Fine-tuning all hyperparameters of the resulting model led to an increase in classification accuracy from 76% obtained with BertForSequenceClassification’s original architecture to 94.4% obtained with CyBERT. Furthermore, we evaluated CyBERT for the impact of randomness in the initialization, training, and data-sampling phases. CyBERT demonstrated a standard deviation of ±0.6% during validation across 100 random seed values. Finally, we also compared the performance of CyBERT to other well-established language models including GPT2, ULMFiT, and ELMo, as well as neural network models such as CNN, LSTM, and BiLSTM. The results showed that CyBERT outperforms these models on the validation accuracy and the F1 score, validating CyBERT’s robustness and accuracy as a cybersecurity feature claims classifier.", "year": 2021, "publicationdate": "2021-11-04", "externalids": {"DOI": "10.3390/jcp1040031"}, "doi_lower": "10.3390/jcp1040031"}
{"paper_id": 225129357, "title": "Apply transfer learning to cybersecurity: Predicting exploitability of vulnerabilities by description", "author_names": ["Jiao Yin", "Mingjian Tang", "Jinli Cao", "Hua Wang"], "venue": "Knowledge-Based Systems", "abstract": null, "year": 2020, "publicationdate": "2020-12-01", "externalids": {"DOI": "10.1016/j.knosys.2020.106529"}, "doi_lower": "10.1016/j.knosys.2020.106529"}
{"paper_id": 267547416, "title": "Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices", "author_names": ["M. Ferrag", "Mthandazo Ndhlovu", "Norbert Tihanyi", "Lucas C. Cordeiro", "M. Debbah", "Thierry Lestable", "Narinderjit Singh Thandi"], "venue": "IEEE Access", "abstract": "The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.", "year": 2023, "publicationdate": "2023-06-25", "externalids": {"DOI": "10.1109/ACCESS.2024.3363469"}, "doi_lower": "10.1109/access.2024.3363469"}
{"paper_id": 249998594, "title": "An Attack Detection Framework Based on BERT and Deep Learning", "author_names": ["Yunus Emre Seyyar", "A. Yavuz", "H. Ünver"], "venue": "IEEE Access", "abstract": "Deep Learning (DL) and Natural Language Processing (NLP) techniques are improving and enriching with a rapid pace. Furthermore, we witness that the use of web applications is increasing in almost every direction in parallel with the related technologies. Web applications encompass a wide array of use cases utilizing personal, financial, defense, and political information (e.g., wikileaks incident). Indeed, to access and to manipulate such information are among the primary goals of attackers. Thus, vulnerability of the information targeted by adversaries is a vital problem and if such information is captured then the consequences can be devastating, which can, potentially, become national security risks in the extreme cases. In this study, as a remedy to this problem, we propose a novel model that is capable of distinguishing normal HTTP requests and anomalous HTTP requests. Our model employs NLP techniques, Bidirectional Encoder Representations from Transformers (BERT) model, and DL techniques. Our experimental results reveal that the proposed approach achieves a success rate over 99.98% and an F1 score over 98.70% in the classification of anomalous and normal requests. Furthermore, web attack detection time of our model is significantly lower (i.e., 0.4 ms) than the other approaches presented in the literature.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2022.3185748"}, "doi_lower": "10.1109/access.2022.3185748"}
{"paper_id": 255964486, "title": "Using the AraBERT Model for Customer Satisfaction Classification of Telecom Sectors in Saudi Arabia", "author_names": ["Sulaiman Aftan", "H. Shah"], "venue": "Brain Science", "abstract": "Customer satisfaction and loyalty are essential for every business. Feedback prediction and social media classification are crucial and play a key role in accurately identifying customer satisfaction. This paper presents sentiment analysis-based customer feedback prediction based on Twitter Arabic datasets of telecommunications companies in Saudi Arabia. The human brain, which contains billions of neurons, provides feedback based on the current and past experience provided by the services and other related stakeholders. Artificial Intelligent (AI) based methods, parallel to human brain processing methods such as Deep Learning (DL) algorithms, are famous for classifying and analyzing such datasets. Comparing the Arabic Dataset to English, it is pretty challenging for typical methods to outperform in the classification or prediction tasks. Therefore, the Arabic Bidirectional Encoder Representations from Transformers (AraBERT) model was used and analyzed with various parameters such as activation functions and topologies and simulated customer satisfaction prediction takes using Arabic Twitter datasets. The prediction results were compared with two famous DL algorithms: Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). Results show that these methods have been successfully applied and obtained highly accurate classification results. AraBERT achieved the best prediction accuracy among the three ML methods, especially with Mobily and STC datasets.", "year": 2023, "publicationdate": "2023-01-01", "externalids": {"DOI": "10.3390/brainsci13010147"}, "doi_lower": "10.3390/brainsci13010147"}
{"paper_id": 232259636, "title": "Q-Meter: Quality Monitoring System for Telecommunication Services Based on Sentiment Analysis Using Deep Learning", "author_names": ["Samuel Terra Vieira", "R. L. Rosa", "D. Z. Rodríguez", "M. A. Ramírez", "Muhammad Saadi", "L. Wuttisittikulkij"], "venue": "Italian National Conference on Sensors", "abstract": "A quality monitoring system for telecommunication services is relevant for network operators because it can help to improve users’ quality-of-experience (QoE). In this context, this article proposes a quality monitoring system, named Q-Meter, whose main objective is to improve subscriber complaint detection about telecommunication services using online-social-networks (OSNs). The complaint is detected by sentiment analysis performed by a deep learning algorithm, and the subscriber’s geographical location is extracted to evaluate the signal strength. The regions in which users posted a complaint in OSN are analyzed using a freeware application, which uses the radio base station (RBS) information provided by an open database. Experimental results demonstrated that sentiment analysis based on a convolutional neural network (CNN) and a bidirectional long short-term memory (BLSTM)-recurrent neural network (RNN) with the soft-root-sign (SRS) activation function presented a precision of 97% for weak signal topic classification. Additionally, the results showed that 78.3% of the total number of complaints are related to weak coverage, and 92% of these regions were proved that have coverage problems considering a specific cellular operator. Moreover, a Q-Meter is low cost and easy to integrate into current and next-generation cellular networks, and it will be useful in sensing and monitoring tasks.", "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.3390/s21051880"}, "doi_lower": "10.3390/s21051880"}
{"paper_id": 251280312, "title": "Joint Sensing and Communications for Deep Reinforcement Learning-based Beam Management in 6G", "author_names": ["Y. Yao", "Hao Zhou", "M. Erol-Kantarci"], "venue": "Global Communications Conference", "abstract": "User location is a piece of critical information for network management and control. However, location uncertainty is unavoidable in certain settings leading to localization errors. In this paper, we consider the user location uncertainty in the mmWave networks, and investigate joint vision-aided sensing and communications using deep reinforcement learning-based beam management for future 6G networks. In particular, we first extract pixel characteristic-based features from satellite images to improve localization accuracy. Then we propose a UK-medoids based method for user clustering with location uncertainty, and the clustering results are consequently used for the beam management. Finally, we apply the DRL algorithm for intra-beam radio resource allocation. The simulations first show that our proposed vision-aided method can substantially reduce the localization error. The proposed UK-medoids and DRL based scheme (UKM-DRL) is compared with two other schemes: K-means based clustering and DRL based resource allocation (K-DRL) and UK-means based clustering and DRL based resource allocation (UK-DRL). The proposed method has 17.2% higher throughput and 7.7% lower delay than UK-DRL, and more than doubled throughput and 55.8% lower delay than K-DRL.", "year": 2022, "publicationdate": "2022-08-03", "externalids": {"DOI": "10.1109/GLOBECOM48099.2022.10001317"}, "doi_lower": "10.1109/globecom48099.2022.10001317"}
{"paper_id": 252111028, "title": "What does a platypus look like? Generating customized prompts for zero-shot image classification", "author_names": ["Sarah Pratt", "Rosanne Liu", "Ali Farhadi"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called \"prompts\", typically consists of a set of hand-written templates (e.g., \"a photo of a {}\") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced \"couple\"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on these regions in the image when making predictions. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this simple baseline requires no additional training and remains completely zero-shot. Code available at https://github.com/sarahpratt/CuPL.", "year": 2022, "publicationdate": "2022-09-07", "externalids": {"DOI": "10.1109/ICCV51070.2023.01438"}, "doi_lower": "10.1109/iccv51070.2023.01438"}
{"paper_id": 256181978, "title": "BFCN: A Novel Classification Method of Encrypted Traffic Based on BERT and CNN", "author_names": ["Zhaolei Shi", "Nurbol Luktarhan", "Yangyang Song", "Gaoqi Tian"], "venue": "Electronics", "abstract": "With the speedy advancement of encryption technology and the exponential increase in applications, network traffic classification has become an increasingly important research topic. Existing methods for classifying encrypted traffic have certain limitations. For example, traditional approaches such as machine learning rely heavily on feature engineering, deep learning approaches are susceptible to the amount and distribution of labeled data, and pretrained models focus merely on the global traffic features while ignoring local features. To solve the above problem, we propose a BERT-based byte-level feature convolutional network (BFCN) model consisting of two novel modules. The first is a packet encoder module, in which we use the BERT pretrained encrypted traffic classification model to capture global traffic features through its attention mechanism; the second is a CNN module, which captures byte-level local features in the traffic through convolutional operations. The packet-level and byte-level features are concatenated as the traffic’s final representation, which can better represent encrypted traffic. Our approach achieves state-of-the-art performance on the publicly available ISCX-VPN dataset for the traffic service and application identification task, achieving F1 scores of 99.11% and 99.41%, respectively, on these two tasks. The experimental results demonstrate that our method further improves the performance of encrypted traffic classification.", "year": 2023, "publicationdate": "2023-01-19", "externalids": {"DOI": "10.3390/electronics12030516"}, "doi_lower": "10.3390/electronics12030516"}
{"paper_id": 246822885, "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification", "author_names": ["Xinjie Lin", "G. Xiong", "Gaopeng Gou", "Zhen Li", "Junzheng Shi", "J. Yu"], "venue": "The Web Conference", "abstract": "Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper, we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-VPN-Service to 98.9% (5.2%↑), Cross-Platform (Android) to 92.5% (5.4%↑), CSTNET-TLS 1.3 to 97.4% (10.0%↑). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.", "year": 2022, "publicationdate": "2022-02-13", "externalids": {"DOI": "10.1145/3485447.3512217"}, "doi_lower": "10.1145/3485447.3512217"}
{"paper_id": 211265114, "title": "FlowPrint: Semi-Supervised Mobile-App Fingerprinting on Encrypted Network Traffic", "author_names": ["T. V. Ede", "Riccardo Bortolameotti", "Andrea Continella", "Jingjing Ren", "Daniel J. Dubois", "Martina Lindorfer", "D. Choffnes", "M. Steen", "Andreas Peter"], "venue": "Network and Distributed System Security Symposium", "abstract": "Mobile-application fingerprinting of network traffic is valuable for many security solutions as it provides insights into the apps active on a network. Unfortunately, existing techniques require prior knowledge of apps to be able to recognize them. However, mobile environments are constantly evolving, i.e., apps are regularly installed, updated, and uninstalled. Therefore, it is infeasible for existing fingerprinting approaches to cover all apps that may appear on a network. Moreover, most mobile traffic is encrypted, shows similarities with other apps, e.g., due to common libraries or the use of content delivery networks, and depends on user input, further complicating the fingerprinting process. As a solution, we propose FlowPrint, a semi-supervised approach for fingerprinting mobile apps from (encrypted) network traffic. We automatically find temporal correlations among destination-related features of network traffic and use these correlations to generate app fingerprints. Our approach is able to fingerprint previously unseen apps, something that existing techniques fail to achieve. We evaluate our approach for both Android and iOS in the setting of app recognition, where we achieve an accuracy of 89.2%, significantly outperforming state-of-the-art solutions. In addition, we show that our approach can detect previously unseen apps with a precision of 93.5%, detecting 72.3% of apps within the first five minutes of communication.", "year": 2020, "publicationdate": "2020-02-24", "externalids": {"DOI": "10.14722/ndss.2020.24412"}, "doi_lower": "10.14722/ndss.2020.24412"}
{"paper_id": 21535780, "title": "Characterization of Encrypted and VPN Traffic using Time-related Features", "author_names": ["G. Draper-Gil", "Arash Habibi Lashkari", "M. Mamun", "A. Ghorbani"], "venue": "International Conference on Information Systems Security and Privacy", "abstract": "Traffic characterization is one of the major challenges in today’s security industry. The continuous evolution\nand generation of new applications and services, together with the expansion of encrypted communications\nmakes it a difficult task. Virtual Private Networks (VPNs) are an example of encrypted communication service\nthat is becoming popular, as method for bypassing censorship as well as accessing services that are geographically\nlocked. In this paper, we study the effectiveness of flow-based time-related features to detect VPN traffic\nand to characterize encrypted traffic into different categories, according to the type of traffic e.g., browsing,\nstreaming, etc. We use two different well-known machine learning techniques (C4.5 and KNN) to test the accuracy\nof our features. Our results show high accuracy and performance, confirming that time-related features\nare good classifiers for encrypted traffic characterization.", "year": 2016, "publicationdate": "2016-02-19", "externalids": {"DOI": "10.5220/0005740704070414"}, "doi_lower": "10.5220/0005740704070414"}
{"paper_id": 49313245, "title": "Improving Language Understanding by Generative Pre-Training", "author_names": ["Alec Radford", "Karthik Narasimhan"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 772792, "title": "Context-Aware QoE Modelling, Measurement, and Prediction in Mobile Computing Systems", "author_names": ["Karan Mitra", "A. Zaslavsky", "C. Åhlund"], "venue": "IEEE Transactions on Mobile Computing", "abstract": null, "year": 2015, "publicationdate": "2015-05-01", "externalids": {"DOI": "10.1109/TMC.2013.155"}, "doi_lower": "10.1109/tmc.2013.155"}
{"paper_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "author_names": ["Jean-Baptiste Alayrac", "Jeff Donahue", "Pauline Luc", "Antoine Miech", "Iain Barr", "Yana Hasson", "Karel Lenc", "A. Mensch", "Katie Millican", "Malcolm Reynolds", "Roman Ring", "Eliza Rutherford", "Serkan Cabi", "Tengda Han", "Zhitao Gong", "Sina Samangooei", "Marianne Monteiro", "Jacob Menick", "Sebastian Borgeaud", "Andy Brock", "Aida Nematzadeh", "Sahand Sharifzadeh", "Mikolaj Binkowski", "Ricardo Barreira", "O. Vinyals", "Andrew Zisserman", "K. Simonyan"], "venue": "Neural Information Processing Systems", "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.", "year": 2022, "publicationdate": "2022-04-29", "externalids": {}, "doi_lower": null}
{"paper_id": 244714558, "title": "ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic", "author_names": ["Yoad Tewel", "Yoav Shalev", "Idan Schwartz", "Lior Wolf"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent text-to-image matching models apply contrastive learning to large corpora of uncurated pairs of images and sentences. While such models can provide a powerful score for matching and subsequent zero-shot tasks, they are not capable of generating caption given an image. In this work, we repurpose such models to generate a descriptive text given an image at inference time, without any further training or tuning step. This is done by combining the visual-semantic model with a large language model, benefiting from the knowledge in both web-scale models. The resulting captions are much less restrictive than those obtained by supervised captioning methods. Moreover, as a zero-shot learning method, it is extremely flexible and we demonstrate its ability to perform image arithmetic in which the inputs can be either images or text and the output is a sentence. This enables novel high-level vision capabilities such as comparing two images or solving visual analogy tests. Our code is available at: https://github.com/YoadTew/zero-shot-image-to-text.", "year": 2021, "publicationdate": "2021-11-29", "externalids": {"DOI": "10.1109/CVPR52688.2022.01739"}, "doi_lower": "10.1109/cvpr52688.2022.01739"}
{"paper_id": 247778949, "title": "Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model", "author_names": ["Yu Du", "Fangyun Wei", "Zihe Zhang", "Miaojing Shi", "Yue Gao", "Guoqi Li"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recently, vision-language pre-training shows great potential in open-vocabulary object detection, where detectors trained on base classes are devised for detecting new classes. The class text embedding is firstly generated by feeding prompts to the text encoder of a pre-trained vision-language model. It is then used as the region classifier to supervise the training of a detector. The key element that leads to the success of this model is the proper prompt, which requires careful words tuning and ingenious design. To avoid laborious prompt engineering, there are some prompt representation learning methods being proposed for the image classification task, which however can only be sub-optimal solutions when applied to the detection task. In this paper, we introduce a novel method, detection prompt (DetPro), to learn continuous prompt representations for open-vocabulary object detection based on the pre-trained vision-language model. Different from the previous classification-oriented methods, DetPro has two highlights: 1) a background interpretation scheme to include the proposals in image background into the prompt training; 2) a context grading scheme to separate proposals in image foreground for tailored prompt training. We assemble DetPro with ViLD, a recent state-of-the-art openworld object detector, and conduct experiments on the LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365 datasets. Experimental results show that our DetPro outperforms the baseline ViLD [7] in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. Code and models are available at https://github.com/dyabel/detpro.", "year": 2022, "publicationdate": "2022-03-28", "externalids": {"DOI": "10.1109/CVPR52688.2022.01369"}, "doi_lower": "10.1109/cvpr52688.2022.01369"}
{"paper_id": 258872061, "title": "Text Summarizer Using SpaCy in NLP", "author_names": ["S. Lade"], "venue": "International Journal for Research in Applied Science and Engineering Technology", "abstract": "Abstract: Using machine learning and natural language processing techniques to summarize the huge volume of text data and give a short summary. To develop a system which identifies contexts of a document and give the best possible summarized text with the help of SpaCy in natural language processing.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.22214/ijraset.2023.52248"}, "doi_lower": "10.22214/ijraset.2023.52248"}
{"paper_id": 246367315, "title": "Edge-IIoTset: A New Comprehensive Realistic Cyber Security Dataset of IoT and IIoT Applications for Centralized and Federated Learning", "author_names": ["M. Ferrag", "Othmane Friha", "Djallel Hamouda", "Leandros A. Maglaras", "H. Janicke"], "venue": "IEEE Access", "abstract": "In this paper, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the dataset has been generated using a purpose-built IoT/IIoT testbed with a large representative set of devices, sensors, protocols and cloud/edge configurations. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, etc.). Furthermore, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. In addition, we extract features obtained from different sources, including alerts, system resources, logs, network traffic, and propose new 61 features with high correlations from 1176 found features. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches (i.e., traditional machine learning as well as deep learning) in both centralized and federated learning modes. The Edge-IIoTset dataset can be publicly accessed from [1].", "year": 2022, "publicationdate": "2022-01-27", "externalids": {"DOI": "10.1109/ACCESS.2022.3165809"}, "doi_lower": "10.1109/access.2022.3165809"}
{"paper_id": 282907399, "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "author_names": ["Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer", "Patrick von Platen", "Clara Ma", "Yacine Jernite", "J. Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush"], "venue": "arXiv.org", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.", "year": 2019, "publicationdate": "2019-10-09", "externalids": {}, "doi_lower": null}
{"paper_id": 195259279, "title": "ThreatZoom: neural network for automated vulnerability mitigation", "author_names": ["Ehsan Aghaei", "E. Al-Shaer"], "venue": "Symposium and Bootcamp on the Science of Security", "abstract": "Increasing the variety and quantity of cyber threats becoming the evident that traditional human-in-loop approaches are no longer sufficient to keep systems safe. To address this momentous moot point, forward-thinking pioneers propose new cyber security strategy using automation to build a more efficient and cheaper defense. Associating large number of unpatchable CVEs (vulnerability descriptions) generated everyday to appropriate CWE (weakness) and CAPEC (attack pattern) can be used to automatically infer the expected impact and corresponding mitigation course of actions for that new CVE. Routinely, adversary exploits a vulnerability to trigger a cyber attack where this vulnerability results from a product or system weakness. Hence, finding a common system weakness associated with a vulnerability within a particular product can help to identifying the software, system, or architecture flaw and the potential attack impacts. This identification leads to prevent, detect, and mitigate those flaws. On the other hand, after recognizing the cause and the effect of a vulnerability, discovering the procedural-oriented description of the attack to create behavioral observables for detection and mitigation is necessary that can be derived from CAPEC and ATTCK. Mapping the CWE to CAPEC and ATTCK which provides pre-TTP and post-TTP respectively where TTP stands for Tactics, Techniques, and Procedures. Having all CWE, CAPEC, and ATTCK in one hand enables us to find corresponding mitigation for each one. On the other hand, extracting threat actions provided by each of these concepts leads to find another type of mitigation coming from Critical Security Controls (CSC). In this proposal, the target is to do mapping all the way from CVE to CAPEC and ATTCk automatically using machine learning, deep learning, and natural language processing and find the appropriate mitigation for each one and then find a proper patch as course of action defense. So far, we have introduced a neural network model which successfully classifies CVE to CWE automatically and as working on a deep learning model to classify CWEs to CAPEC.", "year": 2019, "publicationdate": "2019-04-01", "externalids": {"DOI": "10.1145/3314058.3318167"}, "doi_lower": "10.1145/3314058.3318167"}
{"paper_id": 229182459, "title": "ThreatZoom: CVE2CWE using Hierarchical Neural Network", "author_names": ["Ehsan Aghaei", "W. Shadid", "E. Al-Shaer"], "venue": "Security and Privacy in Communication Networks", "abstract": "The Common Vulnerabilities and Exposures (CVE) represent standard means for sharing publicly known information security vulnerabilities. One or more CVEs are grouped into the Common Weakness Enumeration (CWE) classes for the purpose of understanding the software or configuration flaws and potential impacts enabled by these vulnerabilities and identifying means to detect or prevent exploitation. As the CVE-to-CWE classification is mostly performed manually by domain experts, thousands of critical and new CVEs remain unclassified, yet they are unpatchable. This significantly limits the utility of CVEs and slows down proactive threat mitigation. This paper presents the first automatic tool to classify CVEs to CWEs. ThreatZoom uses a novel learning algorithm that employs an adaptive hierarchical neural network which adjusts its weights based on text analytic scores and classification errors. It automatically estimates the CWE classes corresponding to a CVE instance using both statistical and semantic features extracted from the description of a CVE. This tool is rigorously tested by various datasets provided by MITRE and the National Vulnerability Database (NVD). The accuracy of classifying CVE instances to their correct CWE classes are 92% (fine-grain) and 94% (coarse-grain) for NVD dataset, and 75% (fine-grain) and 90% (coarse-grain) for MITRE dataset, despite the small corpus.", "year": 2020, "publicationdate": "2020-09-24", "externalids": {"DOI": "10.1007/978-3-030-63086-7_2"}, "doi_lower": "10.1007/978-3-030-63086-7_2"}
{"paper_id": 248096571, "title": "THE AUTOMATED MACHINE LEARNING CLASSIFICATION APPROACH ON TELCO TROUBLE TICKET DATASET", "author_names": ["Fauzy Che", "Yayah", "K. I. Ghauth", "Choo-Yee Ting"], "venue": "", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 262459084, "title": "Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies", "author_names": ["C. Shah", "Ryen W. White", "Reid Andersen", "Georg Buscher", "Scott Counts", "Sarkar Snigdha Sarathi Das", "Ali Montazer", "Sathish Manivannan", "Jennifer Neville", "Xiaochuan Ni", "N.Kasturi Rangan", "Tara Safavi", "Siddharth Suri", "Mengting Wan", "Leijie Wang", "Longfei Yang"], "venue": "ACM Transactions on the Web", "abstract": "Understanding user intents in information access scenarios can help us provide more relevant and personalized search results and recommendations. However, analyzing user intents is not easy, especially for emerging forms of Web search such as Artificial Intelligence (AI)-driven chat. To understand user intents from retrospective log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or Machine-Learned (ML) labeling, which is either expensive or inflexible for large and dynamic datasets. Large Language Models (LLMs) could generate rich and relevant concepts, descriptions, and examples for user intents using log data of user interactions. However, using LLMs to generate a user intent taxonomy and applying it for a given Information Retrieval (IR) application can be problematic for two main reasons: (1) such a taxonomy is not externally validated; and (2) there may be an undesirable feedback loop if an LLM does both these tasks without external validation. To address this, we propose a new methodology with human experts and assessors to verify the quality of the LLM-generated taxonomy. We also present an end-to-end pipeline that uses an LLM with Human-in-the-Loop (HITL) to produce, refine, and apply labels for user intent analysis in log data. We demonstrate its effectiveness by uncovering new insights into user intents from search and chat logs from the Microsoft Bing Web search engine. The novelty in this research stems from the method for generating purpose-driven user intent taxonomies with strong validation. Our approach not only helps remove methodological and practical bottlenecks from intent-focused research, but also provides a new framework for generating, validating, and applying other kinds of taxonomies in a scalable and adaptable way, with reasonable human effort.", "year": 2023, "publicationdate": "2023-09-14", "externalids": {"DOI": "10.1145/3732294"}, "doi_lower": "10.1145/3732294"}
{"paper_id": 252090373, "title": "Toward Intelligent Millimeter and Terahertz Communication for 6G: Computer Vision-Aided Beamforming", "author_names": ["Yongjun Ahn", "Jinhong Kim", "Seungnyun Kim", "Kyuhong Shim", "Jiyoung Kim", "Sangtae Kim", "B. Shim"], "venue": "IEEE wireless communications", "abstract": "Beamforming technique realized by the multipleinput-multiple-output (MIMO) antenna arrays has been widely used to compensate for the severe path loss in the millimeter wave (mmWave) bands. In 5G NR system, the beam sweeping and beam refinement are employed to find out the best beam codeword aligned to the mobile. Due to the complicated handshaking and finite resolution of the codebook, today's 5G-based beam management strategy is ineffective in various scenarios in terms of the data rate, energy consumption, and also processing latency. An aim of this article is to introduce a new type of beam management framework based on the computer vision (CV) technique. In this framework referred to as computer vision-aided beam management (CVBM), a camera attached to the BS captures the image and the deep learning-based object detector identifies the 3D location of the mobile. Since the base station can directly set the beam direction without the codebook quantization and feedback delay, CVBM achieves the significant beamforming gain and latency reduction. Using the specially designed dataset called Vision Objects for Beam Management (VOBEM), we demonstrate that CVBM achieves more than 40 percent improvement in the beamforming gain and 40 percent reduction in the beam training overhead over the 5G NR beam management.", "year": 2022, "publicationdate": "2022-09-06", "externalids": {"DOI": "10.1109/MWC.007.2200155"}, "doi_lower": "10.1109/mwc.007.2200155"}
{"paper_id": 26250486, "title": "Automated Moving Object Classification in Wireless Multimedia Sensor Networks", "author_names": ["Muhsin Civelek", "A. Yazıcı"], "venue": "IEEE Sensors Journal", "abstract": null, "year": 2017, "publicationdate": "2017-02-15", "externalids": {"DOI": "10.1109/JSEN.2016.2638853"}, "doi_lower": "10.1109/jsen.2016.2638853"}
{"paper_id": 221218889, "title": "Edge-Network-Assisted Real-Time Object Detection Framework for Autonomous Driving", "author_names": ["Seung Wook Kim", "Keunsoo Ko", "Haneul Ko", "V. Leung"], "venue": "IEEE Network", "abstract": "Computer vision tasks such as object detection are crucial for the operations of autonomous vehicles (AVs). Results of many tasks, even those requiring high computational power, can be obtained within a short delay by offloading them to edge clouds. However, although edge clouds are exploited, real-time object detection cannot always be guaranteed due to dynamic channel quality. To mitigate this problem, we propose an edge-network-assisted real-time object detection framework (EODF). In an EODF, AVs extract the region of interest (Rols) of the captured image when the channel quality is not sufficiently good for supporting real-time object detection. Then AVs compress the image data on the basis of the Rols and transmit the compressed one to the edge cloud. In so doing, real-time object detection can be achieved due to the reduced transmission latency. To verify the feasibility of our framework, we evaluate the probability that the results of object detection are not received within the inter-frame duration (i.e., outage probability) and their accuracy. From the evaluation, we demonstrate that the proposed EODF provides the results to AVs in real time and achieves satisfactory accuracy.", "year": 2020, "publicationdate": "2020-08-17", "externalids": {"DOI": "10.1109/MNET.011.2000248"}, "doi_lower": "10.1109/mnet.011.2000248"}
{"paper_id": 263310951, "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)", "author_names": ["Zhengyuan Yang", "Linjie Li", "Kevin Lin", "Jianfeng Wang", "Chung-Ching Lin", "Zicheng Liu", "Lijuan Wang"], "venue": "arXiv.org", "abstract": "Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf", "year": 2023, "publicationdate": "2023-09-30", "externalids": {"DOI": "10.48550/arXiv.2309.17421"}, "doi_lower": "10.48550/arxiv.2309.17421"}
{"paper_id": 114678741, "title": "Independent comparison of popular DPI tools for traffic classification", "author_names": ["Albert Saiz"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-10-01", "externalids": {}, "doi_lower": null}
{"paper_id": 233354830, "title": "TSCRNN: A novel classification scheme of encrypted traffic based on flow spatiotemporal features for efficient management of IIoT", "author_names": ["Kunda Lin", "Xiaolong Xu", "Honghao Gao"], "venue": "Comput. Networks", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1016/j.comnet.2021.107974"}, "doi_lower": "10.1016/j.comnet.2021.107974"}
{"paper_id": 44736663, "title": "Deep Fingerprinting: Undermining Website Fingerprinting Defenses with Deep Learning", "author_names": ["Payap Sirinam", "M. Imani", "Marc Juárez", "M. Wright"], "venue": "Conference on Computer and Communications Security", "abstract": "Website fingerprinting enables a local eavesdropper to determine which websites a user is visiting over an encrypted connection. State-of-the-art website fingerprinting attacks have been shown to be effective even against Tor. Recently, lightweight website fingerprinting defenses for Tor have been proposed that substantially degrade existing attacks: WTF-PAD and Walkie-Talkie. In this work, we present Deep Fingerprinting (DF), a new website fingerprinting attack against Tor that leverages a type of deep learning called Convolutional Neural Networks (CNN) with a sophisticated architecture design, and we evaluate this attack against WTF-PAD and Walkie-Talkie. The DF attack attains over 98% accuracy on Tor traffic without defenses, better than all prior attacks, and it is also the only attack that is effective against WTF-PAD with over 90% accuracy. Walkie-Talkie remains effective, holding the attack to just 49.7% accuracy. In the more realistic open-world setting, our attack remains effective, with 0.99 precision and 0.94 recall on undefended traffic. Against traffic defended with WTF-PAD in this setting, the attack still can get 0.96 precision and 0.68 recall. These findings highlight the need for effective defenses that protect against this new attack and that could be deployed in Tor.", "year": 2018, "publicationdate": "2018-01-07", "externalids": {"DOI": "10.1145/3243734.3243768"}, "doi_lower": "10.1145/3243734.3243768"}
{"paper_id": 3614172, "title": "Fractional Programming for Communication Systems—Part I: Power Control and Beamforming", "author_names": ["Kaiming Shen", "Wei Yu"], "venue": "IEEE Transactions on Signal Processing", "abstract": "Fractional programming (FP) refers to a family of optimization problems that involve ratio term(s). This two-part paper explores the use of FP in the design and optimization of communication systems. Part I of this paper focuses on FP theory and on solving continuous problems. The main theoretical contribution is a novel quadratic transform technique for tackling the multiple-ratio concave–convex FP problem—in contrast to conventional FP techniques that mostly can only deal with the single-ratio or the max-min-ratio case. Multiple-ratio FP problems are important for the optimization of communication networks, because system-level design often involves multiple signal-to-interference-plus-noise ratio terms. This paper considers the applications of FP to solving continuous problems in communication system design, particularly for power control, beamforming, and energy efficiency maximization. These application cases illustrate that the proposed quadratic transform can greatly facilitate the optimization involving ratios by recasting the original nonconvex problem as a sequence of convex problems. This FP-based problem reformulation gives rise to an efficient iterative optimization algorithm with provable convergence to a stationary point. The paper further demonstrates close connections between the proposed FP approach and other well-known algorithms in the literature, such as the fixed-point iteration and the weighted minimum mean-square-error beamforming. The optimization of discrete problems is discussed in Part II of this paper.", "year": 2018, "publicationdate": "2018-02-27", "externalids": {"DOI": "10.1109/TSP.2018.2812733"}, "doi_lower": "10.1109/tsp.2018.2812733"}
{"paper_id": 38116122, "title": "Metaheuristics and Applications to Optimization Problems in Telecommunications", "author_names": ["S. Martins", "C. Ribeiro"], "venue": "Handbook of Optimization in Telecommunications", "abstract": null, "year": 2006, "publicationdate": null, "externalids": {"DOI": "10.1007/978-0-387-30165-5_4"}, "doi_lower": "10.1007/978-0-387-30165-5_4"}
{"paper_id": 229018512, "title": "Two decades of blackbox optimization applications", "author_names": ["S. Alarie", "Charles Audet", "A. Gheribi", "M. Kokkolaras", "Sébastien Le Digabel"], "venue": "EURO Journal on Computational Optimization", "abstract": null, "year": 2020, "publicationdate": "2020-10-01", "externalids": {"DOI": "10.1016/j.ejco.2021.100011"}, "doi_lower": "10.1016/j.ejco.2021.100011"}
{"paper_id": 240291976, "title": "Explicable Reward Design for Reinforcement Learning Agents", "author_names": ["Rati Devidze", "Goran Radanovic", "Parameswaran Kamalaruban", "A. Singla"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 67027138, "title": "A comparative analysis of optimization solvers", "author_names": ["R. Anand", "Divyam Aggarwal", "Vijay Kumar"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-07-04", "externalids": {"DOI": "10.1080/09720510.2017.1395182"}, "doi_lower": "10.1080/09720510.2017.1395182"}
{"paper_id": 257663729, "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "author_names": ["Sébastien Bubeck", "Varun Chandrasekaran", "Ronen Eldan", "J. Gehrke", "Eric Horvitz", "Ece Kamar", "Peter Lee", "Y. Lee", "Yuan-Fang Li", "Scott M. Lundberg", "Harsha Nori", "Hamid Palangi", "Marco Tulio Ribeiro", "Yi Zhang"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {}, "doi_lower": null}
{"paper_id": 258833055, "title": "Reflexion: language agents with verbal reinforcement learning", "author_names": ["Noah Shinn", "Federico Cassano", "Beck Labash", "A. Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {}, "doi_lower": null}
{"paper_id": 239998651, "title": "Training Verifiers to Solve Math Word Problems", "author_names": ["K. Cobbe", "Vineet Kosaraju", "Mo Bavarian", "Mark Chen", "Heewoo Jun", "Lukasz Kaiser", "Matthias Plappert", "Jerry Tworek", "Jacob Hilton", "Reiichiro Nakano", "Christopher Hesse", "John Schulman"], "venue": "arXiv.org", "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.", "year": 2021, "publicationdate": "2021-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 252917648, "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "author_names": ["Mirac Suzgun", "Nathan Scales", "Nathanael Scharli", "Sebastian Gehrmann", "Yi Tay", "Hyung Won Chung", "A. Chowdhery", "Quoc V. Le", "Ed H. Chi", "Denny Zhou", "Jason Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.", "year": 2022, "publicationdate": "2022-10-17", "externalids": {"DOI": "10.48550/arXiv.2210.09261"}, "doi_lower": "10.48550/arxiv.2210.09261"}
{"paper_id": 263829184, "title": "Towards Optimizing with Large Language Models", "author_names": ["Pei-Fu Guo", "Ying-Hsuan Chen", "Yun-Da Tsai", "Shou-De Lin"], "venue": "KiL@KDD", "abstract": "In this work, we conduct an assessment of the optimization capabilities of LLMs across various tasks and data sizes. Each of these tasks corresponds to unique optimization domains, and LLMs are required to execute these tasks with interactive prompting. That is, in each optimization step, the LLM generates new solutions from the past generated solutions with their values, and then the new solutions are evaluated and considered in the next optimization step. Additionally, we introduce three distinct metrics for a comprehensive assessment of task performance from various perspectives. These metrics offer the advantage of being applicable for evaluating LLM performance across a broad spectrum of optimization tasks and are less sensitive to variations in test samples. By applying these metrics, we observe that LLMs exhibit strong optimization capabilities when dealing with small-sized samples. However, their performance is significantly influenced by factors like data size and values, underscoring the importance of further research in the domain of optimization tasks for LLMs.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.48550/arXiv.2310.05204"}, "doi_lower": "10.48550/arxiv.2310.05204"}
{"paper_id": 261101004, "title": "Diagnosing infeasible optimization problems using large language models", "author_names": ["Hao Chen", "Gonzalo E. Constante-Flores", "Canzhou Li"], "venue": "INFOR. Information systems and operational research", "abstract": "Abstract Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields, such as economics, engineering, transportation, and health care. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat’s reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.", "year": 2023, "publicationdate": "2023-08-23", "externalids": {"DOI": "10.1080/03155986.2024.2385189"}, "doi_lower": "10.1080/03155986.2024.2385189"}
{"paper_id": 263831215, "title": "OptiMUS: Optimization Modeling Using MIP Solvers and large language models", "author_names": ["Ali AhmadiTeshnizi", "Wenzhi Gao", "Madeleine Udell"], "venue": "arXiv.org", "abstract": "Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}", "year": 2023, "publicationdate": "2023-10-09", "externalids": {"DOI": "10.48550/arXiv.2310.06116"}, "doi_lower": "10.48550/arxiv.2310.06116"}
{"paper_id": 260119332, "title": "Leveraging Large Language Models for the Generation of Novel Metaheuristic Optimization Algorithms", "author_names": ["Michal Pluháček", "Anezka Kazikova", "T. Kadavy", "Adam Viktorin", "R. Šenkeřík"], "venue": "GECCO Companion", "abstract": "In this paper, we investigate the potential of using Large Language Models (LLMs) such as GPT-4 to generate novel hybrid swarm intelligence optimization algorithms. We use the LLM to identify and decompose six well-performing swarm algorithms for continuous optimization: Particle Swarm Optimization (PSO), Cuckoo Search (CS), Artificial Bee Colony (ABC), Grey Wolf Optimizer (GWO), Self-Organizing Migrating Algorithm (SOMA), and Whale Optimization Algorithm (WOA). We leverage GPT-4 to propose a hybrid algorithm that combines the strengths of these techniques for two distinct use-case scenarios. Our focus is on the process itself and various challenges that emerge during the use of GPT-4 to fulfill a series of set tasks. Furthermore, we discuss the potential impact of LLM-generated algorithms in the metaheuristics domain and explore future research directions.", "year": 2023, "publicationdate": "2023-07-15", "externalids": {"DOI": "10.1145/3583133.3596401"}, "doi_lower": "10.1145/3583133.3596401"}
{"paper_id": 264305875, "title": "Large Language Model for Multi-objective Evolutionary Optimization", "author_names": ["Fei Liu", "Xi Lin", "Zhenkun Wang", "Shunyu Yao", "Xialiang Tong", "Mingxuan Yuan", "Qingfu Zhang"], "venue": "International Conference on Evolutionary Multi-Criterion Optimization", "abstract": "Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the search operators need a carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well on new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose a new version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on different test benchmarks show that our proposed method can achieve competitive performance with widely used MOEAs. It is also promising to see the operator only learned from a few instances can have robust generalization performance on unseen problems with quite different patterns and settings. The results reveal the potential benefits of using pre-trained LLMs in the design of MOEAs.To foster reproducibility and accessibility, the source code is https://github.com/FeiLiu36/LLM4MOEA.", "year": 2023, "publicationdate": "2023-10-19", "externalids": {"DOI": "10.48550/arXiv.2310.12541"}, "doi_lower": "10.48550/arxiv.2310.12541"}
{"paper_id": 4852047, "title": "Zero-Shot Learning—A Comprehensive Evaluation of the Good, the Bad and the Ugly", "author_names": ["Yongqin Xian", "Christoph H. Lampert", "B. Schiele", "Zeynep Akata"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Due to the importance of zero-shot learning, i.e., classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g., pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.", "year": 2017, "publicationdate": "2017-07-03", "externalids": {"DOI": "10.1109/TPAMI.2018.2857768"}, "doi_lower": "10.1109/tpami.2018.2857768"}
{"paper_id": 248227809, "title": "Learning From Peers: Deep Transfer Reinforcement Learning for Joint Radio and Cache Resource Allocation in 5G RAN Slicing", "author_names": ["Hao Zhou", "M. Erol-Kantarci", "Vincent Poor"], "venue": "IEEE Transactions on Cognitive Communications and Networking", "abstract": "Network slicing is a critical technique for 5G communications that covers radio access network (RAN), edge, transport and core slicing. The evolving network architecture requires the orchestration of multiple network resources such as radio and cache resources. In recent years, machine learning (ML) techniques have been widely applied for network management. However, most existing works do not take advantage of the knowledge transfer capability in ML. In this paper, we propose a deep transfer reinforcement learning (DTRL) scheme for joint radio and cache resource allocation to serve 5G RAN slicing. We first define a hierarchical architecture for joint resource allocation. Then we propose two DTRL algorithms: Q-value-based deep transfer reinforcement learning (QDTRL) and action selection-based deep transfer reinforcement learning (ADTRL). In the proposed schemes, learner agents utilize expert agents’ knowledge to improve their performance on current tasks. The proposed algorithms are compared with both the model-free exploration bonus deep Q-learning (EB-DQN) and the model-based priority proportional fairness and time-to-live (PPF-TTL) algorithms. Compared with EB-DQN, our proposed DTRL-based method presents 21.4% lower delay for Ultra Reliable Low Latency Communications (URLLC) slice and 22.4% higher throughput for enhanced Mobile Broad Band (eMBB) slice, while achieving significantly faster convergence than EB-DQN. Moreover, 40.8% lower URLLC delay and 59.8% higher eMBB throughput are observed with respect to PPF-TTL.", "year": 2021, "publicationdate": "2021-09-16", "externalids": {"DOI": "10.1109/TCCN.2022.3204572"}, "doi_lower": "10.1109/tccn.2022.3204572"}
{"paper_id": 3805733, "title": "Inverse Reward Design", "author_names": ["Dylan Hadfield-Menell", "S. Milli", "P. Abbeel", "Stuart J. Russell", "A. Dragan"], "venue": "Neural Information Processing Systems", "abstract": "Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.", "year": 2017, "publicationdate": "2017-11-08", "externalids": {}, "doi_lower": null}
{"paper_id": 283428236, "title": "Reinforcement Learning: An Introduction", "author_names": ["R. S. Sutton"], "venue": "IEEE Transactions on Neural Networks", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.1109/tnn.2004.842673"}, "doi_lower": "10.1109/tnn.2004.842673"}
{"paper_id": 252762395, "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "author_names": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "venue": "International Conference on Learning Representations", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "year": 2022, "publicationdate": "2022-10-06", "externalids": {}, "doi_lower": null}
{"paper_id": 19971112, "title": "Google Vizier: A Service for Black-Box Optimization", "author_names": ["D. Golovin", "Benjamin Solnik", "Subhodeep Moitra", "G. Kochanski", "John E. Karro", "D. Sculley"], "venue": "Knowledge Discovery and Data Mining", "abstract": null, "year": 2017, "publicationdate": "2017-08-13", "externalids": {"DOI": "10.1145/3097983.3098043"}, "doi_lower": "10.1145/3097983.3098043"}
{"paper_id": 4070290, "title": "Distributed Base Station Association and Power Control for Heterogeneous Cellular Networks", "author_names": ["H. Vu", "L. Le"], "venue": "IEEE Transactions on Vehicular Technology", "abstract": null, "year": 2014, "publicationdate": null, "externalids": {"DOI": "10.1109/TVT.2013.2273503"}, "doi_lower": "10.1109/tvt.2013.2273503"}
{"paper_id": 56517354, "title": "Toward Intelligent Network Optimization in Wireless Networking: An Auto-Learning Framework", "author_names": ["Wenyu Zhang", "Zhenjiang Zhang", "H. Chao", "M. Guizani"], "venue": "IEEE wireless communications", "abstract": "In wireless communication systems (WCSs), the network optimization problems (NOPs) play an important role in maximizing system performance by setting appropriate network configurations. When dealing with NOPs by using conventional optimization methodologies, there exist the following three problems: human intervention, model invalidity, and high computation complexity. As such, in this article we propose an auto-learning framework to achieve intelligent and automatic network optimization by using machine learning (ML) techniques. We review the basic concepts of ML, and propose their rudimentary employment models in WCSs, including automatic model construction, experience replay, efficient trial and error, RL-driven gaming, complexity reduction, and solution recommendation. We hope these proposals can provide new insights and motivation in future research for dealing with NOPs in WCSs by using ML techniques.", "year": 2018, "publicationdate": "2018-12-19", "externalids": {"DOI": "10.1109/MWC.2019.1800350"}, "doi_lower": "10.1109/mwc.2019.1800350"}
{"paper_id": 225066951, "title": "Optimizing Coverage and Capacity in Cellular Networks using Machine Learning", "author_names": ["Ryan M. Dreifuerst", "Sam Daulton", "Yuchen Qian", "Paul Varkey", "Maximilian Balandat", "S. Kasturia", "Anoop Tomar", "Ali Yazdan", "V. Ponnampalam", "R. Heath"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "abstract": "Wireless cellular networks have many parameters that are normally tuned upon deployment and re-tuned as the network changes. Many operational parameters affect reference signal received power (RSRP), reference signal received quality (RSRQ), signal-to-interference-plus-noise-ratio (SINR), and, ultimately, throughput. In this paper, we develop and compare two approaches for maximizing coverage and minimizing interference by jointly optimizing the transmit power and downtilt (elevation tilt) settings across sectors. To evaluate different parameter configurations offline, we construct a realistic simulation model that captures geographic correlations. Using this model, we evaluate two optimization methods: deep deterministic policy gradient (DDPG), a reinforcement learning (RL) algorithm, and multi-objective Bayesian optimization (BO). Our simulations show that both approaches significantly outperform random search and converge to comparable Pareto frontiers, but that BO converges with two orders of magnitude fewer evaluations than DDPG. Our results suggest that data-driven techniques can effectively self-optimize coverage and capacity in cellular networks.", "year": 2020, "publicationdate": "2020-10-22", "externalids": {"DOI": "10.1109/ICASSP39728.2021.9414155"}, "doi_lower": "10.1109/icassp39728.2021.9414155"}
{"paper_id": 6298008, "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization", "author_names": ["Steven Diamond", "Stephen P. Boyd"], "venue": "Journal of machine learning research", "abstract": "CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at http://www.cvxpy.org/ under the GPL license, along with documentation and examples.", "year": 2016, "publicationdate": "2016-03-03", "externalids": {}, "doi_lower": null}
{"paper_id": 259341705, "title": "Heuristic Algorithms for RIS-Assisted Wireless Networks: Exploring Heuristic-Aided Machine Learning", "author_names": ["Hao Zhou", "Senior Member Ieee Melike Erol-Kantarci", "Senior Member Ieee Yuanwei Liu", "L. F. I. H. Vincent Poor"], "venue": "IEEE wireless communications", "abstract": "Reconfigurable intelligent surfaces (RISs) are a promising technology to enable smart radio environments. However, integrating RISs into wireless networks also leads to substantial complexity for network management. This work investigates heuristic algorithms and applications to optimize RIS-aided wireless networks, including greedy algorithms, meta-heuristic algorithms, and matching theory. Moreover, we combine heuristic algorithms with machine learning (ML), and propose three heuristic-aided ML algorithms: heuristic deep reinforcement learning (DRL), heuristic-aided supervised learning, and heuristic hierarchical learning. Finally, a case study shows that heuristic DRL can achieve higher data rates and faster convergence than conventional deep Q-networks (DQNs). This work provides a new perspective for optimizing RIS-aided wireless networks by taking advantage of heuristic algorithms and ML.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.1109/MWC.010.2300321"}, "doi_lower": "10.1109/mwc.010.2300321"}
{"paper_id": 232240448, "title": "Reconfigurable Intelligent Surface Aided Massive MIMO Systems With Low-Resolution DACs", "author_names": ["J. Dai", "Yuanyuan Wang", "Cunhua Pan", "Kangda Zhi", "Hong Ren", "Kezhi Wang"], "venue": "IEEE Communications Letters", "abstract": "We investigate a reconfigurable intelligent surface (RIS)-aided multi-user massive multiple-input multi-output (MIMO) system where low-resolution digital-analog converters (DACs) are configured at the base station (BS) in order to reduce the cost and power consumption. An approximate analytical expression for the downlink achievable rate is derived based on maximum ratio transmission (MRT) and additive quantization noise model (AQNM), and the rate maximization problem is solved by particle swarm optimization (PSO) method under both continuous phase shifts (CPSs) and discrete phase shifts (DPSs) at the RIS. Simulation results show that the downlink sum achievable rate tends to a constant with the increase of the number of quantization bits of DACs, and four quantization bits are enough to capture a large portion of the performance of the ideal perfect DACs case.", "year": 2021, "publicationdate": "2021-03-16", "externalids": {"DOI": "10.1109/lcomm.2021.3097208"}, "doi_lower": "10.1109/lcomm.2021.3097208"}
{"paper_id": 225066972, "title": "Power Scaling Law Analysis and Phase Shift Optimization of RIS-Aided Massive MIMO Systems With Statistical CSI", "author_names": ["Kangda Zhi", "Cunhua Pan", "Hong Ren", "Kezhi Wang"], "venue": "IEEE Transactions on Communications", "abstract": "This paper considers an uplink reconfigurable intelligent surface (RIS)-aided massive multiple-input multiple-output (MIMO) system, where the phase shifts of the RIS are designed relying on statistical channel state information (CSI). Considering the complex environment, the general Rician channel model is adopted for both the users-RIS links and RIS-BS links. We first derive the closed-form approximate expressions for the achievable rate which holds for arbitrary numbers of base station (BS) antennas and RIS elements. Then, we utilize the derived expressions to provide some insights, including the asymptotic rate performance, the power scaling laws, and the impacts of various system parameters on the achievable rate. We also tackle the sum-rate maximization and the minimum user rate maximization problems by optimizing the phase shifts at the RIS based on genetic algorithm (GA). Finally, extensive simulations are provided to validate the benefits by integrating RIS into conventional massive MIMO systems. Our simulations also demonstrate the feasibility of deploying large-size but low-resolution RIS in massive MIMO systems.", "year": 2020, "publicationdate": "2020-10-26", "externalids": {"DOI": "10.1109/TCOMM.2022.3162580"}, "doi_lower": "10.1109/tcomm.2022.3162580"}
{"paper_id": 249017743, "title": "Large Language Models are Zero-Shot Reasoners", "author_names": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "venue": "Neural Information Processing Systems", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 258714700, "title": "Smart Home Energy Management: VAE-GAN Synthetic Dataset Generator and Q-Learning", "author_names": ["Mina Razghandi", "Hao Zhou", "M. Erol-Kantarci", "D. Turgut"], "venue": "IEEE Transactions on Smart Grid", "abstract": "In recent years, there has been a growing interest in academia and industry in the analysis of electrical consumption in residential buildings and the implementation of smart home energy management systems (HEMS) to reduce household energy usage and costs. HEMS have been designed to emulate the statistical and functional characteristics of real smart grids. However, a major challenge in this research area is the limited availability of publicly accessible datasets. To address this challenge and further leverage the potential of artificial HEMS applications, it is crucial to develop time series that accurately represent diverse operating conditions of synthetic systems. This paper introduces a novel approach based on the combination of variational auto-encoder-generative adversarial network (VAE-GAN) techniques to generate time-series data of energy consumption in smart homes. Additionally, we investigate the performance of the generative model when integrated with a Q-learning based HEMS. The effectiveness of the Q-learning based HEMS is assessed through online experiments using real-world smart home data. To evaluate the quality of the generated dataset, we employ various metrics including Kullback–Leibler (KL) divergence, maximum mean discrepancy (MMD), and the Wasserstein distance, which quantify the disparities between probability distributions of the real and synthetic data. Our experimental results demonstrate that the synthetic data generated by VAE-GAN closely aligns with the distribution of real data. Furthermore, we demonstrate that the utilization of the generated data facilitates the training of a more efficient Q-learning based HEMS, surpassing the performance achieved with datasets generated using baseline approaches.", "year": 2023, "publicationdate": "2023-05-14", "externalids": {"DOI": "10.1109/TSG.2023.3288824"}, "doi_lower": "10.1109/tsg.2023.3288824"}
{"paper_id": 225039882, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "author_names": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "M. Minderer", "G. Heigold", "S. Gelly", "Jakob Uszkoreit", "N. Houlsby"], "venue": "International Conference on Learning Representations", "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.", "year": 2020, "publicationdate": "2020-10-22", "externalids": {}, "doi_lower": null}
{"paper_id": 232134936, "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth", "author_names": ["Yihe Dong", "Jean-Baptiste Cordonnier", "Andreas Loukas"], "venue": "International Conference on Machine Learning", "abstract": "Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards\"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.", "year": 2021, "publicationdate": "2021-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 264172792, "title": "A decoder-only foundation model for time-series forecasting", "author_names": ["Abhimanyu Das", "Weihao Kong", "Rajat Sen", "Yichen Zhou"], "venue": "International Conference on Machine Learning", "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.", "year": 2023, "publicationdate": "2023-10-14", "externalids": {"DOI": "10.48550/arXiv.2310.10688"}, "doi_lower": "10.48550/arxiv.2310.10688"}
{"paper_id": 261550915, "title": "A Large-Scale Dataset of 4G, NB-IoT, and 5G Non-Standalone Network Measurements", "author_names": ["Konstantinos Kousias", "Mohammad Rajiullah", "G. Caso", "Usman Ali", "O. Alay", "A. Brunstrom", "L. D. Nardis", "Marco Neri", "Maria-Gabriella Di Benedetto"], "venue": "IEEE Communications Magazine", "abstract": "Mobile networks are highly complex systems. Therefore, it is crucial to examine them from an empirical perspective to better understand how network features affect performance, and suggest additional improvements. This article presents a large-scale dataset of measurements collected over fourth generation (4G) and fifth generation (5G) operational networks, providing Long Term Evolution (LTE), Narrowband Internet of Things (NB-IoT), and 5G New Radio (NR) connectivity. We collected our dataset during seven weeks in Rome, Italy, by performing several tests on the infrastructures of two major mobile network operators (MNOs). The open-sourced dataset has enabled multi-faceted analyses of network deployment, coverage, and end-user performance, and can be further used for designing and testing artificial intelligence (AI) and machine learning (ML) solutions for network optimization.", "year": 2024, "publicationdate": "2024-05-01", "externalids": {"DOI": "10.1109/MCOM.011.2200707"}, "doi_lower": "10.1109/mcom.011.2200707"}
{"paper_id": 263896742, "title": "Beyond throughput, the next generation: a 5G dataset with channel and context metrics", "author_names": ["Darijo Raca", "Dylan Leahy", "C. Sreenan", "Jason J. Quinlan"], "venue": "ACM SIGMM Conference on Multimedia Systems", "abstract": "In this paper, we present a 5G trace dataset collected from a major Irish mobile operator. The dataset is generated from two mobility patterns (static and car), and across two application patterns (video streaming and file download). The dataset is composed of client-side cellular key performance indicators (KPIs) comprised of channel-related metrics, context-related metrics, cell-related metrics and throughput information. These metrics are generated from a well-known non-rooted Android network monitoring application, G-NetTrack Pro. To the best of our knowledge, this is the first publicly available dataset that contains throughput, channel and context information for 5G networks. To supplement our real-time 5G production network dataset, we also provide a 5G large scale multi-cell ns-3 simulation framework. The availability of the 5G/mmwave module for the ns-3 mmwave network simulator provides an opportunity to improve our understanding of the dynamic reasoning for adaptive clients in 5G multi-cell wireless scenarios. The purpose of our framework is to provide additional information (such as competing metrics for users connected to the same cell), thus providing otherwise unavailable information about the base station (eNodeB or eNB) environment and scheduling principle, to end user. Our framework permits other researchers to investigate this interaction through the generation of their own synthetic datasets.", "year": 2020, "publicationdate": "2020-05-27", "externalids": {"DOI": "10.1145/3339825.3394938"}, "doi_lower": "10.1145/3339825.3394938"}
{"paper_id": 253254774, "title": "PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting", "author_names": ["Hao Xue", "Flora D.Salim"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.", "year": 2022, "publicationdate": "2022-09-20", "externalids": {"DOI": "10.1109/TKDE.2023.3342137"}, "doi_lower": "10.1109/tkde.2023.3342137"}
{"paper_id": 233296808, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "author_names": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.243"}, "doi_lower": "10.18653/v1/2021.emnlp-main.243"}
{"paper_id": 260926247, "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters", "author_names": ["Ching Chang", "Wen-Chih Peng", "Tien-Fu Chen"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across seven time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TS’s effectiveness with representation learning in forecasting tasks. Ablation studies further validate each component’s contribution to LLM4TS and underscore the essential role of utilizing LLM’s pre-trained weights for optimal performance. The code is available at https://github.com/blacksnail789521/LLM4TS.", "year": 2023, "publicationdate": "2023-08-16", "externalids": {"DOI": "10.1145/3719207"}, "doi_lower": "10.1145/3719207"}
{"paper_id": 258741419, "title": "One Fits All: Power General Time Series Analysis by Pretrained LM", "author_names": ["Tian Zhou", "Peisong Niu", "Xue Wang", "Liang Sun", "Rong Jin"], "venue": "Neural Information Processing Systems", "abstract": "Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer.The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.", "year": 2023, "publicationdate": "2023-02-23", "externalids": {}, "doi_lower": null}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 232168936, "title": "Pretrained Transformers as Universal Computation Engines", "author_names": ["Kevin Lu", "Aditya Grover", "P. Abbeel", "Igor Mordatch"], "venue": "arXiv.org", "abstract": "We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.", "year": 2021, "publicationdate": "2021-03-09", "externalids": {}, "doi_lower": null}
{"paper_id": 259991096, "title": "Meta-Transformer: A Unified Framework for Multimodal Learning", "author_names": ["Yiyuan Zhang", "Kaixiong Gong", "Kaipeng Zhang", "Hongsheng Li", "Y. Qiao", "Wanli Ouyang", "Xiangyu Yue"], "venue": "arXiv.org", "abstract": "Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.10802"}, "doi_lower": "10.48550/arxiv.2307.10802"}
{"paper_id": 216043032, "title": "Deep Learning for Fading Channel Prediction", "author_names": ["Wei Jiang", "H. Schotten"], "venue": "IEEE Open Journal of the Communications Society", "abstract": "Channel state information (CSI), which enables wireless systems to adapt their transmission parameters to instantaneous channel conditions and consequently achieve great performance boost, plays an increasingly vital role in mobile communications. However, getting accurate CSI is challenging due mainly to rapid channel variation caused by multi-path fading. The inaccuracy of CSI imposes a severe impact on the performance of a wide range of adaptive wireless systems, highlighting the significance of channel prediction that can combat outdated CSI effectively. The aim of this article is to shed light on the state of the art in this field and then go beyond by proposing a novel predictor that leverages the strong time-series prediction capability of deep recurrent neural networks incorporating long short-term memory or gated recurrent unit. In addition to an analytical comparison of computational complexity, performance evaluation in terms of prediction accuracy is carried out upon multi-antenna fading channels. Numerical results reveal that deep learning brings a notable performance gain compared with the conventional predictors built on shallow recurrent neural networks.", "year": 2020, "publicationdate": "2020-03-23", "externalids": {"DOI": "10.1109/OJCOMS.2020.2982513"}, "doi_lower": "10.1109/ojcoms.2020.2982513"}
{"paper_id": 251903467, "title": "Terahertz communications can work in rain and snow: impact of adverse weather conditions on channels at 140 GHz", "author_names": ["Priyangshu Sen", "Jacob Hall", "Michele Polese", "Vitaly Petrov", "Duschia M. Bodet", "Francesco Restuccia", "T. Melodia", "J. Jornet"], "venue": "mmNets", "abstract": "Next-generation wireless networks will leverage the spectrum above 100 GHz to enable ultra-high data rate communications over multi-GHz-wide bandwidths. The propagation environment at such high frequencies, however, introduces challenges throughout the whole protocol stack design, from physical layer signal processing to application design. Therefore, it is fundamental to develop a holistic understanding of the channel propagation and fading characteristics over realistic deployment scenarios and ultra-wide bands. In this paper, we conduct an extensive measurement campaign to evaluate the impact of weather conditions on a wireless link in the 130-150 GHz band through a channel sounding campaign with clear weather, rain, and snow in a typical urban backhaul scenario. We present a novel channel sounder design that captures signals with -82 dBm sensitivity and 20 GHz of bandwidth.We analyze link budget, capacity, as well as channel parameters such as the delay spread and the K-factor. Our experimental results indicate that in the considered context the adverse weather does not interrupt the link, but introduces some additional constraints (e.g., high delay spread and increase in path loss in snow conditions) that need to be accounted for in the design of reliable Sixth Generation (6G) communication links above 100 GHz.", "year": 2022, "publicationdate": "2022-08-29", "externalids": {"DOI": "10.1145/3555077.3556470"}, "doi_lower": "10.1145/3555077.3556470"}
{"paper_id": 198168878, "title": "Position Prediction Based Fast Beam Tracking Scheme for Multi-User UAV-mmWave Communications", "author_names": ["Yongning Ke", "Hui Gao", "Wenjun Xu", "Lixin Li", "Li Guo", "Zhiyong Feng"], "venue": "ICC 2019 - 2019 IEEE International Conference on Communications (ICC)", "abstract": "Unmanned aerial vehicle (UAV) millimeter-wave (mmWave) communication is emerging as a promising technique for future networks with flexible network topology and ultra-high data transmission rate. Within such full-dimensionally dynamic mmWave network, beam-tracking is challenging and critical, especially when all the UAVs are in motion for some collaborative tasks that require high-quality communications. In this paper, we propose a fast beam tracking scheme, which is built on an efficient position prediction of multiple moving UAVs. In particular, a Gaussian process based machine learning scheme is proposed to achieve fast and accurate UAV position prediction with quantifiable positional uncertainty. Based on the prediction results, the beam-tracking can be confined within some specific spatial regions centered on the predicted UAV positions. In contrast to the full-space searching based scheme, our proposed position prediction based beam tracking requires little system overhead and thus achieves high net spectrum efficiency. Moreover, we also propose a practical communication protocol embedding our beam-tracking scheme, which monitors the channel evolution and triggers the UAV position prediction for beam-tracking, transmit-receive beam pair selection and data transmission. Simulation results validate the advantages of our scheme over the existing works.", "year": 2019, "publicationdate": "2019-05-01", "externalids": {"DOI": "10.1109/ICC.2019.8761775"}, "doi_lower": "10.1109/icc.2019.8761775"}
{"paper_id": 245576164, "title": "Multi-cell Multi-beam Prediction using Auto-encoder LSTM for mmWave systems", "author_names": ["Syed Hashim Ali Shah", "S. Rangan"], "venue": "IEEE Transactions on Wireless Communications", "abstract": "Millimeter wave (mmWave) systems rely on communication in narrow beams for directional and spatial multiplexing gains. A key challenge in realizing these systems is beam tracking, particularly in environments with high mobility and blockage. Additionally, in wide-area mmWave cellular systems, user equipment (UE) devices must often simultaneously track signals from multiple cells, since links to individual cells can be unreliable. Models of the channel dynamics across multiple cells and multiple beams are difficult to derive from first principles. In this work, we propose a fully data-driven approach based on a novel auto-encoder integrated long short term memory (LSTM) network, which predicts multiple beams from multiple cells, one time step in the future. The key innovation is to use an auto-encoder pre-processing step, which reduces the dimensionality of the input – the main challenge in multi-cell, multi-beam tracking. The prediction capability of the proposed network is verified and compared to common baseline predictors as well as popular machine learning (ML) based neural network predictors in realistic system-level simulations using a commercial ray-tracer. We observe that predictions from the proposed network, which utilizes auto-encoders for dimensionality reduction, offers significantly better best beam accuracy and lower beam misalignment loss than common baseline approaches. We also discuss outage prediction and proactive beam switching as applications of the multi-cell multi-beam prediction.", "year": 2021, "publicationdate": "2021-12-29", "externalids": {"DOI": "10.1109/TWC.2022.3183632"}, "doi_lower": "10.1109/twc.2022.3183632"}
{"paper_id": 244508405, "title": "Comparison of Machine Learning Techniques Applied to Traffic Prediction of Real Wireless Network", "author_names": ["D. Alekseeva", "Nikolai Stepanov", "A. Veprev", "A. Sharapova", "E. Lohan", "A. Ometov"], "venue": "IEEE Access", "abstract": "Today, the traffic amount is growing inexorably due to the increase in the number of devices on the network. Researchers analyze traffic by identifying sophisticated dependencies, anomalies, and novel traffic patterns to improve the system performance. One of the fast development niches in this domain is related to Classic and Deep Machine Learning techniques that are supposed to improve the network operation in the most complex heterogeneous environment. In this work, we first outline existing applications of Machine Learning in the communications domain and further list the most significant challenges and potential solutions while implementing those. Finally, we compare different classical methods predicting the traffic on the LTE network Edge by utilizing such techniques as Linear Regression, Gradient Boosting, Random Forest, Bootstrap Aggregation (Bagging), Huber Regression, Bayesian Regression, and Support Vector Machines (SVM). We develop the corresponding Machine Learning environment based on a public cellular traffic dataset and present a comparison table of the quality metrics and execution time for each model. After the analysis, the SVM method proved to allow for a much faster training compared to other algorithms. Gradient Boosting showed the best quality of predictions as it has the most efficient data determination. Random forest shows the worst result since it depends on the number of features that may be limited. The probabilistic approach-based Bayesian regression method showed slightly worse results than Gradient Boosting, but its training time was shorter. The performance evaluation demonstrated good results for linear models with the Huber loss function, which optimizes the model parameters better. As a standalone contribution, we offer the source code of the analyzed algorithms in Open Access.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2021.3129850"}, "doi_lower": "10.1109/access.2021.3129850"}
{"paper_id": 246480158, "title": "AFB: Improving Communication Load Forecasting Accuracy with Adaptive Feature Boosting", "author_names": ["Chengming Hu", "Xi Chen", "Ju Wang", "Hang Li", "Jikun Kang", "Yi Tian Xu", "Xue Liu", "Di Wu", "Seowoo Jang", "Intaik Park", "Gregory Dudek"], "venue": "Global Communications Conference", "abstract": "Prediction of key system characteristics, such as the communication load, is required to overcome the delays in wireless communication systems. State-of-The-Art (SOTA) approaches mostly apply existing Neural Network (NN) structures, and extract latent features purely based on their sensitivity to the forecasting accuracy. This way of feature extraction may neglect some non-obvious yet informative dimensions in the model input, leading to inaccurate forecasting results. In this paper, we present an Adaptive Feature Boosting (AFB) approach, which integrates multiple AutoEncoders (AEs) to automatically extract robust and comprehensive latent features for communication load forecasting. The recurrent and residual connections among the AEs make sure that the extracted latent features are representative for all input dimensions. With more comprehensive information extracted from the history, the forecasting accuracy is thus improved. We evaluate AFB against existing approaches on a real-world dataset that contains Call Detail Records (CDRs) of the Milan city over a period of two months. The evaluation shows that our AFB-based approach achieves 35.2% more accurate load forecasting results than the SOTA deep approaches.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {"DOI": "10.1109/GLOBECOM46510.2021.9685424"}, "doi_lower": "10.1109/globecom46510.2021.9685424"}
{"paper_id": 252022896, "title": "Weather-Aware Fiber-Wireless Traffic Prediction Using Graph Convolutional Networks", "author_names": ["Mariam Abdullah", "Jiayuan He", "K. Wang"], "venue": "IEEE Access", "abstract": "In recent years, there has been an increased demand for better and faster communication networks to meet the needs of Beyond 5G network. The fiber-wireless integrated network has been widely investigated, taking advantage of the large capacity and low transmission loss properties of the optical fiber to extend the coverage of wireless networks and the cellular networks in particular. To optimize the resource allocation in these networks, machine learning (ML) techniques have been proposed, which aim to predict the cellular traffic in advance to allow proactive resource allocation. Existing works mainly consider two factors in traffic prediction: the spatial correlation amongst nearby based stations and the temporal dynamics observed in historical records. In this paper, we study a crucial and yet unexplored aspect, i.e., meteorological factors, such as rain, wind, or temperature. We first perform statistical analysis on a real-world dataset, the results of which confirm the strong impact of meteorological factors upon traffic volume. Thereafter, we propose a traffic prediction model that captures the temporal, spatial, and meteorological patterns simultaneously. The proposed model learns the network traffic patterns through a novel graph convolutional network - gated recurrent unit (GCN-GRU) cell. The GCN-GRU has a hierarchical structure with two child GRUs capturing the temporal dynamics in traffic and meteorological records respectively and a parent GRU capturing the unified impact of historical traffic and meteorological data upon future traffic. The spatial correlation among traffic records is captured by a graphical convolution module within the proposed GCN-GRU cell. We conduct extensive experiments on real-world datasets. The results confirm the effectiveness of the proposed model, with a performance improvement by up to 24.8% achieved using the hierarchical GCN-GRU model.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2022.3203420"}, "doi_lower": "10.1109/access.2022.3203420"}
{"paper_id": 2058474, "title": "Enhancing QoE-Aware Wireless Edge Caching With Software-Defined Wireless Networks", "author_names": ["C. Liang", "Ying He", "F. R. Yu", "Nan Zhao"], "venue": "IEEE Transactions on Wireless Communications", "abstract": "Software-defined networking and in-network caching are promising technologies in the next generation wireless networks. In this paper, we propose enhancing the quality of experience (QoE)-aware wireless edge caching with bandwidth provisioning in software-defined wireless networks (SDWNs). Specifically, we design a novel mechanism to jointly provide proactive caching, bandwidth provisioning, and adaptive video streaming. The caches are requested to retrieve data in advance dynamically according to the behaviors of users, the current traffic, and the resource status. Then, we formulate a novel optimization problem regarding the QoE-aware bandwidth provisioning in SDWNs with jointly considering in-network caching strategy. The caching problem is decoupled from the bandwidth provisioning problem by deploying the dual-decomposition method. Additionally, we relax the binary variables to real numbers so that those two problems are formulated as a linear problem and a convex problem, respectively, which can be solved efficiently. Simulation results are presented to show that the latency is decreased and the utilization of caches is improved in the proposed scheme.", "year": 2017, "publicationdate": "2017-08-04", "externalids": {"DOI": "10.1109/TWC.2017.2734081"}, "doi_lower": "10.1109/twc.2017.2734081"}
{"paper_id": 25208151, "title": "A Survey on QoE-oriented Wireless Resources Scheduling", "author_names": ["Ivo Sousa", "Tiago Rosa Maria Paula Queluz", "A. Rodrigues"], "venue": "Journal of Network and Computer Applications", "abstract": null, "year": 2017, "publicationdate": "2017-05-22", "externalids": {"DOI": "10.1016/j.jnca.2020.102594"}, "doi_lower": "10.1016/j.jnca.2020.102594"}
{"paper_id": 256105061, "title": "SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis", "author_names": ["Imtiaz Karim", "Kazi Samin Mubasshir", "Mirza Masfiqur Rahman", "Elisa Bertino"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "5G is the 5th generation cellular network protocol. It is the state-of-the-art global wireless standard that enables an advanced kind of network designed to connect virtually everyone and everything with increased speed and reduced latency. Therefore, its development, analysis, and security are critical. However, all approaches to the 5G protocol development and security analysis, e.g., property extraction, protocol summarization, and semantic analysis of the protocol specifications and implementations are completely manual. To reduce such manual effort, in this paper, we curate SPEC5G the first-ever public 5G dataset for NLP research. The dataset contains 3,547,586 sentences with 134M words, from 13094 cellular network specifications and 13 online websites. By leveraging large-scale pre-trained language models that have achieved state-of-the-art results on NLP tasks, we use this dataset for security-related text classification and summarization. Security-related text classification can be used to extract relevant security-related properties for protocol testing. On the other hand, summarization can help developers and practitioners understand the high level of the protocol, which is itself a daunting task. Our results show the value of our 5G-centric dataset in 5G protocol analysis automation. We believe that SPEC5G will enable a new research direction into automatic analyses for the 5G cellular network protocol and numerous related downstream tasks. Our data and code are publicly available.", "year": 2023, "publicationdate": "2023-01-22", "externalids": {"DOI": "10.48550/arXiv.2301.09201"}, "doi_lower": "10.48550/arxiv.2301.09201"}
{"paper_id": 270226866, "title": "TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP Specifications", "author_names": ["Rasoul Nikbakht", "Mohamed Benzaghta", "Giovanni Geraci"], "venue": "2024 IEEE Globecom Workshops (GC Wkshps)", "abstract": "Understanding telecom standards involves sorting through numerous technical documents, such as those produced by the 3rd Generation Partnership Project (3GPP), which is time-consuming and labor-intensive. While large language models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive dataset is crucial for their effective pre-training and fine-tuning. In this paper, we introduce TSpec-LLM, an open-source comprehensive dataset covering all 3GPP documents from Release 8 to Release 19 (1999–2023). To evaluate its efficacy, we first select a representative sample of 3GPP documents, create corresponding technical questions, and assess the baseline performance of various LLMs. We then incorporate a retrieval-augmented generation (RAG) framework to enhance LLM capabilities by retrieving relevant context from the TSpec-LLM dataset. Our evaluation shows that using a naive-RAG framework on TSpec-LLM improves the accuracy of GPT-3.5, Gemini 1.0 Pro, and GPT-4 from 44%, 46%, and 51% to 71%, 75%, and 72%, respectively.", "year": 2024, "publicationdate": "2024-06-03", "externalids": {"DOI": "10.1109/GCWkshp64532.2024.11101012"}, "doi_lower": "10.1109/gcwkshp64532.2024.11101012"}
{"paper_id": 261696561, "title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models", "author_names": ["Yukai Miao", "Yu Bai", "Li Chen", "Dan Li", "Haifeng Sun", "Xizheng Wang", "Ziqiu Luo", "Yanyu Ren", "Dapeng Sun", "Xiuting Xu", "Qi Zhang", "Chao Xiang", "Xinchi Li"], "venue": "arXiv.org", "abstract": "Nowadays, the versatile capabilities of Pre-trained Large Language Models (LLMs) have attracted much attention from the industry. However, some vertical domains are more interested in the in-domain capabilities of LLMs. For the Networks domain, we present NetEval, an evaluation set for measuring the comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is designed for evaluating the commonsense knowledge and inference ability in NetOps in a multi-lingual context. NetEval consists of 5,732 questions about NetOps, covering five different sub-domains of NetOps. With NetEval, we systematically evaluate the NetOps capability of 26 publicly available LLMs. The results show that only GPT-4 can achieve a performance competitive to humans. However, some open models like LLaMA 2 demonstrate significant potential.", "year": 2023, "publicationdate": "2023-09-11", "externalids": {"DOI": "10.48550/arXiv.2309.05557"}, "doi_lower": "10.48550/arxiv.2309.05557"}
{"paper_id": 267703896, "title": "Unlocking Telecom Domain Knowledge Using LLMs", "author_names": ["Sujoy Roychowdhury", "Nishkarsh Jain", "Sumit Soman"], "venue": "International Conference on Communication Systems and Networks", "abstract": "Conversational assistants have become increasingly popular as they use Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) for domain context. In this work, we present an end-to-end solution that leverages RAG for telecom domain Question Answering (QA) on standards documents. We highlight that retrieval quality is important, along with an efficient indexing mechanism for the document embeddings. We also index images and tables for QA on standards documents. Our Telecom Knowledge Assistant is useful for handling specific queries from telecom domain experts, as well as for novice learners. The developed approach and solution are amenable to adapt for other domains as well.", "year": 2024, "publicationdate": "2024-01-03", "externalids": {"DOI": "10.1109/COMSNETS59351.2024.10427044"}, "doi_lower": "10.1109/comsnets59351.2024.10427044"}
{"paper_id": 271064960, "title": "ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open Radio Access Networks", "author_names": ["Pranshav Gajjar", "Vijay K Shah"], "venue": "Consumer Communications and Networking Conference", "abstract": "Large Language Models (LLMs) can revolutionize how we deploy and operate Open Radio Access Networks (0-RAN) by enhancing network analytics, anomaly detection, and code generation and significantly increasing the efficiency and reliability of a plethora of 0- RAN tasks. In this paper, we present ORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the performance of Large Language Models (LLMs) within the context of O-RAN. Our benchmark consists of 13,952 meticulously curated multiple-choice questions generated from 116 O-RAN specification documents. We leverage a novel three- stage LLM framework, and the questions are categorized into three distinct difficulties to cover a wide spectrum of 0 RAN- related knowledge. We thoroughly evaluate the performance of several state-of-the-art LLMs, including Gemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a Retrieval- Augmented Generation (RAG)-based pipeline that demonstrates superior performance on ORAN-Bench-13K compared to other tested closed-source models. Our findings indicate that current popular LLM models are not proficient in O-RAN, highlighting the need for specialized models. We observed a noticeable performance improvement when incorporating the RAG-based ORANSight pipeline, with a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on average 21.55% and 22.59% better than the other tested LLMs.", "year": 2024, "publicationdate": "2024-07-08", "externalids": {"DOI": "10.1109/CCNC54725.2025.10975994"}, "doi_lower": "10.1109/ccnc54725.2025.10975994"}
{"paper_id": 265294960, "title": "LLM aided semi-supervision for Extractive Dialog Summarization", "author_names": ["Nishant Mishra", "Gaurav Sahu", "Iacer Calixto", "Ameen Abu-Hanna", "I. H. L. A. Umc", "Department of Biomedical Informatics", "U. Amsterdam", "Amsterdam Public Health", "Methodology", "Amsterdam", "The Netherlands.", "U. Waterloo", "Servicenow Research", "U. Columbia"], "venue": "arXiv.org", "abstract": "Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \\tweetsumm dataset, and show that using 10% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.", "year": 2023, "publicationdate": "2023-11-19", "externalids": {"DOI": "10.48550/arXiv.2311.11462"}, "doi_lower": "10.48550/arxiv.2311.11462"}
{"paper_id": 267412980, "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges", "author_names": ["Taicheng Guo", "Xiuying Chen", "Yaqi Wang", "Ruidi Chang", "Shichao Pei", "N. Chawla", "Olaf Wiest", "Xiangliang Zhang"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.", "year": 2024, "publicationdate": "2024-01-21", "externalids": {"DOI": "10.48550/arXiv.2402.01680"}, "doi_lower": "10.48550/arxiv.2402.01680"}
{"paper_id": 276993300, "title": "JavaLLM: A Fine-Tuned LLM for Java Programming Education", "author_names": ["Jingying Zhang", "Kang Liu"], "venue": "International Symposium on Computer Science and Intelligent Control", "abstract": "The integration of Large Language Models (LLMs) into education marks a significant advancement toward personalized and adaptive learning environments, particularly in programming education. Addressing the limitations of existing LLMs in specialized domains like Java programming, this paper introduces JavaLLM — a model specifically tailored for Java programming education. Built upon a robust codeLLM and fine-tuned using extensive, high-quality Java-focused datasets, JavaLLM demonstrates superior performance in code generation and Java-specific question answering. Through rigorous evaluation and iterative refinement, JavaLLM facilitates a transformative classroom experience, enhancing the quality of teaching and enabling a personalized learning journey for students in Java programming courses. This innovation paves the way for smarter, more tailored educational approaches, leveraging AI’s generative capabilities to meet the evolving demands of modern education.", "year": 2024, "publicationdate": "2024-09-06", "externalids": {"DOI": "10.1109/ISCSIC64297.2024.00064"}, "doi_lower": "10.1109/iscsic64297.2024.00064"}
{"paper_id": 257771744, "title": "ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization", "author_names": ["Zheheng Luo", "Qianqian Xie", "S. Ananiadou"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.15621"}, "doi_lower": "10.48550/arxiv.2303.15621"}
{"paper_id": 269447864, "title": "Limits and risks of GPT-3 applications", "author_names": ["Anna Strasser", "Matthew Crosby", "Eric Schwitzgebel"], "venue": "Annual Meeting of the Cognitive Science Society", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258564349, "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance", "author_names": ["Lingjiao Chen", "M. Zaharia", "James Y. Zou"], "venue": "Trans. Mach. Learn. Res.", "abstract": "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {"DOI": "10.48550/arXiv.2305.05176"}, "doi_lower": "10.48550/arxiv.2305.05176"}
