{"paper_id": 231603388, "title": "Persistent Anti-Muslim Bias in Large Language Models", "author_names": ["Abubakar Abid", "Maheen Farooqi", "James Y. Zou"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.", "year": 2021, "publicationdate": "2021-01-14", "externalids": {"DOI": "10.1145/3461702.3462624"}, "doi_lower": "10.1145/3461702.3462624"}
{"paper_id": 250390701, "title": "Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of DistilBERT", "author_names": ["Jaimeen Ahn", "Hwaran Lee", "Jinhwa Kim", "Alice Oh"], "venue": "GEBNLP", "abstract": "Knowledge distillation is widely used to transfer the language understanding of a large model to a smaller model.However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model.This paper studies what causes gender bias to increase after the knowledge distillation process.Moreover, we suggest applying a variant of the mixup on knowledge distillation, which is used to increase generalizability during the distillation process, not for augmentation.By doing so, we can significantly reduce the gender bias amplification after knowledge distillation.We also conduct an experiment on the GLUE benchmark to demonstrate that even if the mixup is applied, it does not have a significant adverse effect on the model‚Äôs performance.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.gebnlp-1.27"}, "doi_lower": "10.18653/v1/2022.gebnlp-1.27"}
{"paper_id": 237491723, "title": "Mitigating Language-Dependent Ethnic Bias in BERT", "author_names": ["Jaimeen Ahn", "Alice H. Oh"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.", "year": 2021, "publicationdate": "2021-09-13", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.42"}, "doi_lower": "10.18653/v1/2021.emnlp-main.42"}
{"paper_id": 249017555, "title": "Challenges in Measuring Bias via Open-Ended Language Generation", "author_names": ["Afra Feyza Aky√ºrek", "Muhammed Yusuf Kocyigit", "Sejin Paik", "Derry Tanti Wijaya"], "venue": "GEBNLP", "abstract": "Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups‚Äîposing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under https://github.com/feyzaakyurek/bias-textgen.", "year": 2022, "publicationdate": "2022-05-23", "externalids": {"DOI": "10.48550/arXiv.2205.11601"}, "doi_lower": "10.48550/arxiv.2205.11601"}
{"paper_id": 258762715, "title": "Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model", "author_names": ["Chantal Amrhein", "Florian Schottmann", "Rico Sennrich", "Samuel Laubli"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data through linguistic rules. However, this approach is not practical for languages with more complex morphology than English. We hypothesise that creating training data in the reverse direction, i.e. starting from gender-fair text, is easier for morphologically complex languages and show that it matches the performance of state-of-the-art rewriting models for English. To eliminate the rule-based nature of data creation, we instead propose using machine translation models to create gender-biased text from real gender-fair text via round-trip translation. Our approach allows us to train a rewriting model for German without the need for elaborate handcrafted rules. The outputs of this model increased gender-fairness as shown in a human evaluation study.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.48550/arXiv.2305.11140"}, "doi_lower": "10.48550/arxiv.2305.11140"}
{"paper_id": 247519021, "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists", "author_names": ["Giuseppe Attanasio", "Debora Nozza", "Dirk Hovy", "Elena Baralis"], "venue": "Findings", "abstract": "Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance.Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected.Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy.We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.", "year": 2022, "publicationdate": "2022-03-17", "externalids": {"DOI": "10.48550/arXiv.2203.09192"}, "doi_lower": "10.48550/arxiv.2203.09192"}
{"paper_id": 254823489, "title": "Constitutional AI: Harmlessness from AI Feedback", "author_names": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "John Kernion", "Andy Jones", "A. Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Carol Chen", "Catherine Olsson", "Chris Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "E. Perez", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Kamilƒó Luko≈°i≈´tƒó", "Liane Lovitt", "M. Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noem'i Mercado", "Nova Dassarma", "R. Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "T. Henighan", "Tristan Hume", "Sam Bowman", "Zac Hatfield-Dodds", "Benjamin Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom B. Brown", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.08073"}, "doi_lower": "10.48550/arxiv.2212.08073"}
{"paper_id": 235358955, "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models", "author_names": ["Soumya Barikeri", "Anne Lauscher", "Ivan Vulic", "Goran Glavas"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.", "year": 2021, "publicationdate": "2021-06-07", "externalids": {"DOI": "10.18653/v1/2021.acl-long.151"}, "doi_lower": "10.18653/v1/2021.acl-long.151"}
{"paper_id": 113402716, "title": "Fairness and Machine Learning Limitations and Opportunities", "author_names": ["Solon Barocas", "Moritz Hardt", "Arvind Narayanan"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 225094152, "title": "Unmasking Contextual Stereotypes: Measuring and Mitigating BERT‚Äôs Gender Bias", "author_names": ["Marion Bartl", "M. Nissim", "Albert Gatt"], "venue": "GEBNLP", "abstract": "Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.", "year": 2020, "publicationdate": "2020-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 54040749, "title": "Hurtlex: A Multilingual Lexicon of Words to Hurt", "author_names": ["Elisa Bassignana", "Valerio Basile", "V. Patti"], "venue": "CLICIT", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {"DOI": "10.4000/BOOKS.AACCADEMIA.3085"}, "doi_lower": "10.4000/books.aaccademia.3085"}
{"paper_id": 145275273, "title": "Racial Identification by Speech", "author_names": ["J. Baugh"], "venue": "", "abstract": null, "year": 2000, "publicationdate": "2000-12-01", "externalids": {"DOI": "10.1215/00031283-75-4-362"}, "doi_lower": "10.1215/00031283-75-4-362"}
{"paper_id": 60510016, "title": "Language documentation and language typology", "author_names": ["Oliver Bond"], "venue": "", "abstract": null, "year": 2010, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 52255687, "title": "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science", "author_names": ["Emily M. Bender", "Batya Friedman"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.", "year": 2018, "publicationdate": "2018-12-01", "externalids": {"DOI": "10.1162/tacl_a_00041"}, "doi_lower": "10.1162/tacl_a_00041"}
{"paper_id": 262580630, "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú", "author_names": ["Emily M. Bender", "Timnit Gebru", "Angelina McMillan-Major", "Shmargaret Shmitchell"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.", "year": 2021, "publicationdate": "2021-03-03", "externalids": {"DOI": "10.1145/3442188.3445922"}, "doi_lower": "10.1145/3442188.3445922"}
{"paper_id": 213647029, "title": "Race after technology. Abolitionist tools for the new Jim Code", "author_names": ["S. Merz"], "venue": "Ethnic and Racial Studies", "abstract": "If there is one positive consequence stemming from the election of Donald Trump to the White House, it is the surge of critical investigations examining the relationships between big tech and democ...", "year": 2020, "publicationdate": "2020-01-27", "externalids": {"DOI": "10.1080/01419870.2020.1715454"}, "doi_lower": "10.1080/01419870.2020.1715454"}
{"paper_id": 53622943, "title": "How Stereotypes Are Shared Through Language: A Review and Introduction of the Social Categories and Stereotypes Communication (SCSC) Framework", "author_names": ["Camiel J. Beukeboom", "C. Burgers"], "venue": "Review of Communication Research", "abstract": "Language use plays a crucial role in the consensualization of stereotypes within cultural groups. Based on an integrative review of the literature on stereotyping and biased language use, we propose the Social Categories and Stereotypes Communication (SCSC) framework. The framework integrates largely independent areas of literature and explicates the linguistic processes through which social-category stereotypes are shared and maintained. We distinguish two groups of biases in language use that jointly feed and maintain three fundamental cognitive variables in (shared) social-category cognition: perceived category entitativity, stereotype content, and perceived essentialism of associated stereotypic characteristics. These are: (1) Biases in linguistic labels used to denote categories, within which we discuss biases in (a) label content and (b) linguistic form of labels; (2) Biases in describing behaviors and characteristics of categorized individuals, within which we discuss biases in (a) communication content (i.e., what information is communicated), and (b) linguistic form of descriptions (i.e., how is information formulated). Together, these biases create a self-perpetuating cycle in which social-category stereotypes are shared and maintained. The framework allows for a better understanding of stereotype maintaining biases in natural language. We discuss various opportunities for further research.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.12840/ISSN.2255-4165.017"}, "doi_lower": "10.12840/issn.2255-4165.017"}
{"paper_id": 252532134, "title": "Re-contextualizing Fairness in NLP: The Case of India", "author_names": ["Shaily Bhatt", "Sunipa Dev", "Partha P. Talukdar", "Shachi Dave", "Vinodkumar Prabhakaran"], "venue": "AACL", "abstract": "Recent research has revealed undesirable biases in NLP data and models. However, these efforts focus of social disparities in West, and are not directly portable to other geo-cultural contexts. In this paper, we focus on NLP fairness in the context of India. We start with a brief account of the prominent axes of social disparities in India. We build resources for fairness evaluation in the Indian context and use them to demonstrate prediction biases along some of the axes. We then delve deeper into social stereotypes for Region and Religion, demonstrating its prevalence in corpora and models. Finally, we outline a holistic research agenda to re-contextualize NLP fairness research for the Indian context, accounting for Indian societal context, bridging technological gaps in NLP capabilities and resources, and adapting to Indian cultural values. While we focus on India, this framework can be generalized to other geo-cultural contexts.", "year": 2022, "publicationdate": "2022-09-25", "externalids": {"DOI": "10.48550/arXiv.2209.12226"}, "doi_lower": "10.48550/arxiv.2209.12226"}
{"paper_id": 232099112, "title": "Algorithmic injustice: a relational ethics approach", "author_names": ["Abeba Birhane"], "venue": "Patterns", "abstract": null, "year": 2021, "publicationdate": "2021-02-01", "externalids": {"DOI": "10.1016/j.patter.2021.100205"}, "doi_lower": "10.1016/j.patter.2021.100205"}
{"paper_id": 252355515, "title": "Power to the People? Opportunities and Challenges for Participatory AI", "author_names": ["Abeba Birhane", "William S. Isaac", "Vinodkumar Prabhakaran", "M. D'iaz", "M. C. Elish", "Iason Gabriel", "Shakir Mohamed"], "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "abstract": "Participatory approaches to artificial intelligence (AI) and machine learning (ML) are gaining momentum: the increased attention comes partly with the view that participation opens the gateway to an inclusive, equitable, robust, responsible and trustworthy AI. Among other benefits, participatory approaches are essential to understanding and adequately representing the needs, desires and perspectives of historically marginalized communities. However, there currently exists lack of clarity on what meaningful participation entails and what it is expected to do. In this paper we first review participatory approaches as situated in historical contexts as well as participatory methods and practices within the AI and ML pipeline. We then introduce three case studies in participatory AI. Participation holds the potential for beneficial, emancipatory and empowering technology design, development and deployment while also being at risk for concerns such as cooptation and conflation with other activities. We lay out these limitations and concerns and argue that as participatory AI/ML becomes in vogue, a contextual and nuanced understanding of the term as well as consideration of who the primary beneficiaries of participatory activities ought to be constitute crucial factors to realizing the benefits and opportunities that participation brings.", "year": 2022, "publicationdate": "2022-09-15", "externalids": {"DOI": "10.1145/3551624.3555290"}, "doi_lower": "10.1145/3551624.3555290"}
{"paper_id": 263311003, "title": "Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools", "author_names": ["Emily Black", "Rakshit Naidu", "R. Ghani", "Kit T. Rodolfa", "Daniel E. Ho", "Hoda Heidari"], "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "abstract": "While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue‚Äôs root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of operationalizing this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowledge find it challenging to hypothesize how various design choices influence model behavior. We then consult the fair-ML literature to understand the progress to date toward operationalizing the pipeline-aware approach: we systematically collect and organize the prior work that attempts to detect, measure, and mitigate various sources of unfairness through the ML pipeline. We utilize this extensive categorization of previous contributions to sketch a research agenda for the community. We hope this work serves as the stepping stone toward a more comprehensive set of resources for ML researchers, practitioners, and students interested in exploring, designing, and testing pipeline-oriented approaches to algorithmic fairness.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.1145/3617694.3623259"}, "doi_lower": "10.1145/3617694.3623259"}
{"paper_id": 235866012, "title": "Sociolinguistically Driven Approaches for Just Natural Language Processing", "author_names": ["Su Lin Blodgett"], "venue": "", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.7275/20410631"}, "doi_lower": "10.7275/20410631"}
{"paper_id": 218971825, "title": "Language (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP", "author_names": ["Su Lin Blodgett", "Solon Barocas", "Hal Daum'e", "Hanna M. Wallach"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We survey 146 papers analyzing ‚Äúbias‚Äù in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ‚Äúbias‚Äù is an inherently normative process. We further find that these papers‚Äô proposed quantitative techniques for measuring or mitigating ‚Äúbias‚Äù are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ‚Äúbias‚Äù in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ‚Äúbias‚Äù---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements‚Äîand to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {"DOI": "10.18653/v1/2020.acl-main.485"}, "doi_lower": "10.18653/v1/2020.acl-main.485"}
{"paper_id": 236460302, "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets", "author_names": ["Su Lin Blodgett", "Gilsinia Lopez", "Alexandra Olteanu", "Robert Sim", "Hanna M. Wallach"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system‚Äôs behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens‚Äîoriginating from the social sciences‚Äîto inventory a range of pitfalls that threaten these benchmarks‚Äô validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.81"}, "doi_lower": "10.18653/v1/2021.acl-long.81"}
{"paper_id": 11281155, "title": "Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English", "author_names": ["Su Lin Blodgett", "Brendan T. O'Connor"], "venue": "arXiv.org", "abstract": "We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.", "year": 2017, "publicationdate": "2017-06-30", "externalids": {}, "doi_lower": null}
{"paper_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "author_names": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y. Zou", "Venkatesh Saligrama", "A. Kalai"], "venue": "Neural Information Processing Systems", "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "year": 2016, "publicationdate": "2016-07-21", "externalids": {}, "doi_lower": null}
{"paper_id": 237091588, "title": "On the Opportunities and Risks of Foundation Models", "author_names": ["Rishi Bommasani", "Drew A. Hudson", "E. Adeli", "R. Altman", "Simran Arora", "Sydney von Arx", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "E. Brunskill", "Erik Brynjolfsson", "S. Buch", "Dallas Card", "Rodrigo Castellon", "Niladri S. Chatterji", "Annie S. Chen", "Kathleen A. Creel", "Jared Davis", "Dora Demszky", "Chris Donahue", "M. Doumbouya", "Esin Durmus", "Stefano Ermon", "J. Etchemendy", "Kawin Ethayarajh", "L. Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah D. Goodman", "S. Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas F. Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "G. Keeling", "Fereshte Khani", "O. Khattab", "Pang Wei Koh", "M. Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "J. Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "E. Mitchell", "Zanele Munyikwa", "Suraj Nair", "A. Narayan", "D. Narayanan", "Benjamin Newman", "Allen Nie", "Juan Carlos Niebles", "H. Nilforoshan", "Julian Nyarko", "Giray Ogut", "Laurel J. Orr", "Isabel Papadimitriou", "J. Park", "C. Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Robert Reich", "Hongyu Ren", "Frieda Rong", "Yusuf H. Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher R'e", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "K. Srinivasan", "Alex Tamkin", "Rohan Taori", "A. Thomas", "Florian Tram√®r", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "M. Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang"], "venue": "arXiv.org", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.", "year": 2021, "publicationdate": "2021-08-16", "externalids": {}, "doi_lower": null}
{"paper_id": 248986638, "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements", "author_names": ["Conrad Borchers", "D. Gala", "Ben Gilburt", "Eduard Oravkin", "Wilfried Bounsi", "Yuki M. Asano", "Hannah Rose Kirk"], "venue": "GEBNLP", "abstract": "The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3, with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias.", "year": 2022, "publicationdate": "2022-05-23", "externalids": {"DOI": "10.48550/arXiv.2205.11374"}, "doi_lower": "10.48550/arxiv.2205.11374"}
{"paper_id": 102352788, "title": "Identifying and Reducing Gender Bias in Word-Level Language Models", "author_names": ["Shikha Bordia", "Samuel R. Bowman"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora‚ÄîPenn Treebank, WikiText-2, and CNN/Daily Mail‚Äîresulting in similar conclusions.", "year": 2019, "publicationdate": "2019-04-05", "externalids": {"DOI": "10.18653/v1/N19-3002"}, "doi_lower": "10.18653/v1/n19-3002"}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 261115681, "title": "tech solutionism", "author_names": ["Tom D. Uhlig"], "venue": "Freie Assoziation", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.30820/1434-7849-2023-1-115"}, "doi_lower": "10.30820/1434-7849-2023-1-115"}
{"paper_id": 258236466, "title": "On the Independence of Association Bias and Empirical Fairness in Language Models", "author_names": ["Laura Cabello", "Anna Katrine van Zee", "Anders S√∏gaard"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness‚Äîor such probes ‚Äòinto representational biases‚Äô are said to be ‚Äòmotivated by fairness‚Äô‚Äîsuggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.", "year": 2023, "publicationdate": "2023-04-20", "externalids": {"DOI": "10.1145/3593013.3594004"}, "doi_lower": "10.1145/3593013.3594004"}
{"paper_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "author_names": ["Aylin Caliskan", "J. Bryson", "Arvind Narayanan"], "venue": "Science", "abstract": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs‚Äîfor example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.", "year": 2016, "publicationdate": "2016-08-25", "externalids": {"DOI": "10.1126/science.aal4230"}, "doi_lower": "10.1126/science.aal4230"}
{"paper_id": 247762845, "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations", "author_names": ["Yang Trista Cao", "Yada Pruksachatkun", "Kai-Wei Chang", "Rahul Gupta", "Varun Kumar", "J. Dhamala", "A. Galstyan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.", "year": 2022, "publicationdate": "2022-03-25", "externalids": {"DOI": "10.48550/arXiv.2203.13928"}, "doi_lower": "10.48550/arxiv.2203.13928"}
{"paper_id": 249319807, "title": "Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models", "author_names": ["Yang Trista Cao", "Anna Sotnikova", "Hal Daum'e", "Rachel Rudinger", "L. Zou"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "NLP models trained on text have been shown to reproduce human stereotypes, which can magnify harms to marginalized groups when systems are deployed at scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et al. (2016) from social psychology as a framework for the systematic study and discovery of stereotypic group-trait associations in language models (LMs). We introduce the sensitivity test (SeT) for measuring stereotypical associations from language models. To evaluate SeT and other measures using the ABC model, we collect group-trait judgments from U.S.-based subjects to compare with English LM stereotypes. Finally, we extend this framework to measure LM stereotyping of intersectional identities.", "year": 2022, "publicationdate": "2022-06-23", "externalids": {"DOI": "10.48550/arXiv.2206.11684"}, "doi_lower": "10.48550/arxiv.2206.11684"}
{"paper_id": 4421747, "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "author_names": ["Daniel Matthew Cer", "Mona T. Diab", "Eneko Agirre", "I. Lopez-Gazpio", "Lucia Specia"], "venue": "International Workshop on Semantic Evaluation", "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "year": 2017, "publicationdate": "2017-07-31", "externalids": {"DOI": "10.18653/v1/S17-2001"}, "doi_lower": "10.18653/v1/s17-2001"}
{"paper_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "author_names": ["Yu-Chu Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Kaijie Zhu", "Hao Chen", "Linyi Yang", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Weirong Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qian Yang", "Xingxu Xie"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‚Äòwhere‚Äô and ‚Äòhow‚Äô questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1145/3641289"}, "doi_lower": "10.1145/3641289"}
{"paper_id": 258960243, "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models", "author_names": ["Myra Cheng", "Esin Durmus", "Dan Jurafsky"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling.Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones.We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18189"}, "doi_lower": "10.48550/arxiv.2305.18189"}
{"paper_id": 232185104, "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders", "author_names": ["Pengyu Cheng", "Weituo Hao", "Siyang Yuan", "Shijing Si", "L. Carin"], "venue": "International Conference on Learning Representations", "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.", "year": 2021, "publicationdate": "2021-03-11", "externalids": {}, "doi_lower": null}
{"paper_id": 1443041, "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "author_names": ["Alexandra Chouldechova"], "venue": "Big Data", "abstract": "Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance.", "year": 2016, "publicationdate": "2016-10-24", "externalids": {"DOI": "10.1089/big.2016.0047"}, "doi_lower": "10.1089/big.2016.0047"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier Garc√≠a", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark D√≠az", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 259096160, "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions", "author_names": ["John Joon Young Chung", "Ece Kamar", "Saleema Amershi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user‚Äôs domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.18653/v1/2023.acl-long.34"}, "doi_lower": "10.18653/v1/2023.acl-long.34"}
{"paper_id": 233864576, "title": "A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations", "author_names": ["Pierre Colombo", "C. Clavel", "P. Piantanida"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi‚Äôs divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.", "year": 2021, "publicationdate": "2021-05-06", "externalids": {"DOI": "10.18653/v1/2021.acl-long.511"}, "doi_lower": "10.18653/v1/2021.acl-long.511"}
{"paper_id": 207880568, "title": "Unsupervised Cross-lingual Representation Learning at Scale", "author_names": ["Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzm√°n", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.", "year": 2019, "publicationdate": "2019-11-05", "externalids": {"DOI": "10.18653/v1/2020.acl-main.747"}, "doi_lower": "10.18653/v1/2020.acl-main.747"}
{"paper_id": 210444739, "title": "Language and Discrimination: Generating Meaning, Perceiving Identities, and Discriminating Outcomes", "author_names": ["Justin T. Craft", "K. Wright", "Rachel Elizabeth Weissler", "R. Queen"], "venue": "Annual Review of Linguistics", "abstract": "Humans are remarkably efficient at parsing basic linguistic cues and show an equally impressive ability to produce and parse socially indexed cues from the language(s) they encounter. In this review, we focus on the ways in which questions of justice and equality are linked to these two abilities. We discuss how social and linguistic cues are theorized to become correlated with each other, describe listeners' perceptual abilities regarding linguistic and social cognition, and address how, in the context of these abilities, language mediates individuals‚Äô negotiations with institutions and their agents‚Äînegotiations that often lead to discrimination or linguistic injustice. We review research that reports inequitable outcomes as a function of language use across education, employment, media, justice systems, housing markets, and health care institutions. Finally, we present paths forward for linguists to help fight against these discriminatory realities.", "year": 2020, "publicationdate": "2020-01-14", "externalids": {"DOI": "10.1146/annurev-linguistics-011718-011659"}, "doi_lower": "10.1146/annurev-linguistics-011718-011659"}
{"paper_id": 143944342, "title": "Cognitive bias: The trouble with analytic and heuristic reasoning.", "author_names": ["Christopher A. Was"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {"DOI": "10.1037/A0010653"}, "doi_lower": "10.1037/a0010653"}
{"paper_id": 210835680, "title": "Detecting Gender Stereotypes: Lexicon vs. Supervised Learning Methods", "author_names": ["Jenna Cryan", "Shiliang Tang", "Xinyi Zhang", "Miriam J. Metzger", "Haitao Zheng", "Ben Y. Zhao"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Biases in language influence how we interact with each other and society at large. Language affirming gender stereotypes is often observed in various contexts today, from recommendation letters and Wikipedia entries to fiction novels and movie dialogue. Yet to date, there is little agreement on the methodology to quantify gender stereotypes in natural language (specifically the English language). Common methodology (including those adopted by companies tasked with detecting gender bias) rely on a lexicon approach largely based on the original BSRI study from 1974. In this paper, we reexamine the role of gender stereotype detection in the context of modern tools, by comparatively analyzing efficacy of lexicon-based approaches and end-to-end, ML-based approaches prevalent in state-of-the-art natural language processing systems. Our efforts using a large dataset show that even compared to an updated lexicon-based approach, end-to-end classification approaches are significantly more robust and accurate, even when trained by moderately sized corpora.", "year": 2020, "publicationdate": "2020-04-21", "externalids": {"DOI": "10.1145/3313831.3376488"}, "doi_lower": "10.1145/3313831.3376488"}
{"paper_id": 235658325, "title": "Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics", "author_names": ["Paula Czarnowska", "Yogarshi Vyas", "Kashif Shah"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model‚Äôs behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.", "year": 2021, "publicationdate": "2021-06-28", "externalids": {"DOI": "10.1162/tacl_a_00425"}, "doi_lower": "10.1162/tacl_a_00425"}
{"paper_id": 208617790, "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "author_names": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "J. Yosinski", "Rosanne Liu"], "venue": "International Conference on Learning Representations", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "year": 2019, "publicationdate": "2019-09-25", "externalids": {}, "doi_lower": null}
{"paper_id": 238634750, "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations", "author_names": ["A. Davani", "M. D'iaz", "Vinodkumar Prabhakaran"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators‚Äô judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.", "year": 2021, "publicationdate": "2021-10-12", "externalids": {"DOI": "10.1162/tacl_a_00449"}, "doi_lower": "10.1162/tacl_a_00449"}
{"paper_id": 250426284, "title": "FairDistillation: Mitigating Stereotyping in Language Models", "author_names": ["Pieter Delobelle", "Bettina Berendt"], "venue": "ECML/PKDD", "abstract": "Large pre-trained language models are successfully being used in a variety of tasks, across many languages. With this ever-increasing usage, the risk of harmful side effects also rises, for example by reproducing and reinforcing stereotypes. However, detecting and mitigating these harms is difficult to do in general and becomes computationally expensive when tackling multiple languages or when considering different biases. To address this, we present FairDistillation: a cross-lingual method based on knowledge distillation to construct smaller language models while controlling for specific biases. We found that our distillation method does not negatively affect the downstream performance on most tasks and successfully mitigates stereotyping and representational harms. We demonstrate that FairDistillation can create fairer language models at a considerably lower cost than alternative approaches.", "year": 2022, "publicationdate": "2022-07-10", "externalids": {"DOI": "10.48550/arXiv.2207.04546"}, "doi_lower": "10.48550/arxiv.2207.04546"}
{"paper_id": 250390561, "title": "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models", "author_names": ["Pieter Delobelle", "E. Tokpo", "T. Calders", "Bettina Berendt"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify ‚Äòbias‚Äô and ‚Äòfairness‚Äô in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.naacl-main.122"}, "doi_lower": "10.18653/v1/2022.naacl-main.122"}
{"paper_id": 245005939, "title": "Whose Ground Truth? Accounting for Individual and Collective Identities Underlying Dataset Annotation", "author_names": ["Emily L. Denton", "M. D'iaz", "Ian Kivlichan", "Vinodkumar Prabhakaran", "Rachel Rosen"], "venue": "arXiv.org", "abstract": "Human annotations play a crucial role in machine learning (ML) research and development. However, the ethical considerations around the processes and decisions that go into building ML datasets has not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this space along two layers: (1) who the annotator is, and how the annotators' lived experiences can impact their annotations, and (2) the relationship between the annotators and the crowdsourcing platforms and what that relationship affords them. Finally, we put forth a concrete set of recommendations and considerations for dataset developers at various stages of the ML data pipeline: task formulation, selection of annotators, platform and infrastructure choices, dataset analysis and evaluation, and dataset documentation and release.", "year": 2021, "publicationdate": "2021-12-08", "externalids": {}, "doi_lower": null}
{"paper_id": 220525546, "title": "Bringing the People Back In: Contesting Benchmark Machine Learning Datasets", "author_names": ["Emily L. Denton", "A. Hanna", "Razvan Amironesei", "A. Smart", "Hilary Nicole", "M. Scheuerman"], "venue": "arXiv.org", "abstract": "In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to \"bring the people back in\" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.", "year": 2020, "publicationdate": "2020-07-14", "externalids": {}, "doi_lower": null}
{"paper_id": 201670701, "title": "On Measuring and Mitigating Biased Inferences of Word Embeddings", "author_names": ["Sunipa Dev", "Tao Li", "J. M. Phillips", "Vivek Srikumar"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Word embeddings carry stereotypical connotations from the text they are trained on, which can lead to invalid inferences in downstream models that rely on them. We use this observation to design a mechanism for measuring stereotypes using the task of natural language inference. We demonstrate a reduction in invalid inferences via bias mitigation strategies on static word embeddings (GloVe). Further, we show that for gender bias, these techniques extend to contextualized embeddings when applied selectively only to the static components of contextualized embeddings (ELMo, BERT).", "year": 2019, "publicationdate": "2019-08-25", "externalids": {"DOI": "10.1609/AAAI.V34I05.6267"}, "doi_lower": "10.1609/aaai.v34i05.6267"}
{"paper_id": 220281039, "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings", "author_names": ["Sunipa Dev", "Tao Li", "J. M. Phillips", "Vivek Srikumar"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.", "year": 2020, "publicationdate": "2020-06-30", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.411"}, "doi_lower": "10.18653/v1/2021.emnlp-main.411"}
{"paper_id": 248524791, "title": "Theories of ‚ÄúGender‚Äù in NLP Bias Research", "author_names": ["Hannah Devinney", "Jenny Bj√∂rklund", "H. Bj√∂rklund"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how ‚Äúgender‚Äù is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define ‚Äúbias.‚Äù Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.", "year": 2022, "publicationdate": "2022-05-05", "externalids": {"DOI": "10.1145/3531146.3534627"}, "doi_lower": "10.1145/3531146.3534627"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 231719337, "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation", "author_names": ["J. Dhamala", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Kai-Wei Chang", "Rahul Gupta"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.", "year": 2021, "publicationdate": "2021-01-27", "externalids": {"DOI": "10.1145/3442188.3445924"}, "doi_lower": "10.1145/3442188.3445924"}
{"paper_id": 259316226, "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models", "author_names": ["Harnoor Dhingra", "Preetiha Jayashanker", "Sayali S. Moghe", "Emma Strubell"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.", "year": 2023, "publicationdate": "2023-06-30", "externalids": {"DOI": "10.48550/arXiv.2307.00101"}, "doi_lower": "10.48550/arxiv.2307.00101"}
{"paper_id": 207852875, "title": "Queens Are Powerful Too: Mitigating Gender Bias in Dialogue Generation", "author_names": ["Emily Dinan", "Angela Fan", "Adina Williams", "Jack Urbanek", "Douwe Kiela", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.", "year": 2019, "publicationdate": "2019-11-10", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.656"}, "doi_lower": "10.18653/v1/2020.emnlp-main.656"}
{"paper_id": 54997157, "title": "Measuring and Mitigating Unintended Bias in Text Classification", "author_names": ["Lucas Dixon", "John Li", "Jeffrey Scott Sorensen", "Nithum Thain", "Lucy Vasserman"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.", "year": 2018, "publicationdate": "2018-12-27", "externalids": {"DOI": "10.1145/3278721.3278729"}, "doi_lower": "10.1145/3278721.3278729"}
{"paper_id": 237568724, "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus", "author_names": ["Jesse Dodge", "Ana Marasovic", "Gabriel Ilharco", "Dirk Groeneveld", "Margaret Mitchell", "Matt Gardner", "William Agnew"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.98"}, "doi_lower": "10.18653/v1/2021.emnlp-main.98"}
{"paper_id": 258174286, "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach", "author_names": ["Tommaso Dolci", "Fabio Azzalini", "M. Tanelli"], "venue": "Data Science and Engineering", "abstract": "The ever-increasing number of systems based on semantic text analysis is making natural language understanding a fundamental task: embedding-based language models are used for a variety of applications, such as resume parsing or improving web search results. At the same time, despite their popularity and widespread use, concern is rapidly growing due to their display of social bias and lack of transparency. In particular, they exhibit a large amount of gender bias, favouring the consolidation of social stereotypes. Recently, sentence embeddings have been introduced as a novel and powerful technique to represent entire sentences as vectors. We propose a new metric to estimate gender bias in sentence embeddings, named bias score . Our solution leverages semantic importance of words and previous research on bias in word embeddings, and it is able to discern between neutral and biased gender information at sentence level. Experiments on a real-world dataset demonstrate that our novel metric can identify gender stereotyped sentences. Furthermore, we employ bias score to detect and then remove or compensate for the more stereotyped entries in text corpora used to train sentence encoders, improving their degree of fairness. Finally, we prove that models retrained on fairer corpora are less prone to make stereotypical associations compared to their original counterpart, while preserving accuracy in natural language understanding tasks. Additionally, we compare our experiments with traditional methods for reducing bias in embedding-based language models.", "year": 2023, "publicationdate": "2023-04-15", "externalids": {"DOI": "10.1007/s41019-023-00211-0"}, "doi_lower": "10.1007/s41019-023-00211-0"}
{"paper_id": 238582879, "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting", "author_names": ["Zahra Fatemi", "Chen Xing", "Wenhao Liu", "Caiming Xiong"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model‚Äôs downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data.Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.", "year": 2021, "publicationdate": "2021-10-11", "externalids": {"DOI": "10.18653/v1/2023.acl-short.108"}, "doi_lower": "10.18653/v1/2023.acl-short.108"}
{"paper_id": 259262064, "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models", "author_names": ["Virginia K. Felkner", "Ho-Chun Herbert Chang", "Eugene Jang", "Jonathan May"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.15087"}, "doi_lower": "10.48550/arxiv.2306.15087"}
{"paper_id": 258041203, "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models", "author_names": ["Emilio Ferrara"], "venue": "First Monday", "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.5210/fm.v28i11.13346"}, "doi_lower": "10.5210/fm.v28i11.13346"}
{"paper_id": 258615411, "title": "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks", "author_names": ["Eve Fleisig", "Rediet Abebe", "D. Klein"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining whether a statement is offensive to the demographic group that it targets, when that group may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and by 33% at predicting variance among annotators, which provides a metric for model uncertainty downstream. We find that annotator ratings can be predicted using their demographic information and opinions on online content, without the need to track identifying annotator IDs that link each annotator to their ratings. We also find that use of non-invasive survey questions on annotators' online experiences helps to maximize privacy and minimize unnecessary collection of demographic information when predicting annotators' opinions.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.06626"}, "doi_lower": "10.48550/arxiv.2305.06626"}
{"paper_id": 259092939, "title": "FairPrism: Evaluating Fairness-Related Harms in Text Generation", "author_names": ["Eve Fleisig", "Aubrie Amstutz", "Chad Atalla", "Su Lin Blodgett", "Hal Daum√©", "Alexandra Olteanu", "Emily Sheng", "Dan Vann", "Hanna M. Wallach"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer specification of dataset coverage, and accounting for annotator disagreement and harms that are context-dependent. FairPrism‚Äôs annotations include the extent of stereotyping and demeaning harms, the demographic groups targeted, and appropriateness for different applications. The annotations also include specific harms that occur in interactive contexts and harms that raise normative concerns when the ‚Äúspeaker‚Äù is an AI system. Due to its precision and granularity, FairPrism can be used to diagnose (1) the types of fairness-related harms that AI text generation systems cause, and (2) the potential limitations of mitigation methods, both of which we illustrate through case studies. Finally, the process we followed to develop FairPrism offers a recipe for building improved datasets for measuring and mitigating harms caused by AI systems.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.343"}, "doi_lower": "10.18653/v1/2023.acl-long.343"}
{"paper_id": 226226666, "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms", "author_names": ["Maxwell Forbes", "Jena D. Hwang", "Vered Shwartz", "Maarten Sap", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as \"wanting to call cops on my neighbors\" are social norms that inform our conduct, such as \"It is expected that you report crimes.\" \nWe present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run a blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \nComprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.", "year": 2020, "publicationdate": "2020-11-01", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.48"}, "doi_lower": "10.18653/v1/2020.emnlp-main.48"}
{"paper_id": 266697295, "title": "re-The (Im)possibility of Fairness: Different Value Systems Require Different Mechanisms For Fair Decision Making", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 247613246, "title": "Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention", "author_names": ["Yacine Gaci", "B. Benatallah", "F. Casati", "K. Benabdeslem"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Natural Language Processing (NLP) models are found to exhibit discriminatory stereotypes across many social constructs, e.g. gender and race. In comparison to the progress made in reducing bias from static word embeddings, fairness in sentence-level text encoders received little consideration despite their wider applicability in contemporary NLP tasks. In this paper, we propose a debiasing method for pre-trained text encoders that both reduces social stereotypes, and inflicts next to no semantic damage. Unlike previous studies that directly manipulate the embeddings, we suggest to dive deeper into the operation of these encoders, and pay more attention to the way they pay attention to different social groups. We find that stereotypes are also encoded in the attention layer. Then, we work on model debiasing by redistributing the attention scores of a text encoder such that it forgets any preference to historically advantaged groups, and attends to all social classes with the same intensity. Our experiments confirm that reducing bias from attention effectively mitigates it from the model‚Äôs text representations.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.651"}, "doi_lower": "10.18653/v1/2022.emnlp-main.651"}
{"paper_id": 4930886, "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes", "author_names": ["Nikhil Garg", "L. Schiebinger", "Dan Jurafsky", "James Y. Zou"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts‚Äîe.g., the women‚Äôs movement in the 1960s and Asian immigration into the United States‚Äîand also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.", "year": 2017, "publicationdate": "2017-11-22", "externalids": {"DOI": "10.1073/pnas.1720347115"}, "doi_lower": "10.1073/pnas.1720347115"}
{"paper_id": 52880735, "title": "Counterfactual Fairness in Text Classification through Robustness", "author_names": ["Sahaj Garg", "Vincent Perot", "Nicole Limtiaco", "Ankur Taly", "Ed H. Chi", "Alex Beutel"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay\" is toxic while \"Some people are straight\" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {"DOI": "10.1145/3306618.3317950"}, "doi_lower": "10.1145/3306618.3317950"}
{"paper_id": 236477795, "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation", "author_names": ["Aparna Garimella", "Akhash Amarnath", "K. Kumar", "Akash Pramod Yalla", "Anandhavelu Natarajan", "Niyati Chhaya", "Balaji Vasan Srinivasan"], "venue": "Findings", "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classiÔ¨Åcation tasks, mitigating biases in only the representations may not suf-Ô¨Åce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in Ô¨Åll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the efÔ¨Åcacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.findings-acl.397"}, "doi_lower": "10.18653/v1/2021.findings-acl.397"}
{"paper_id": 253762006, "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique", "author_names": ["Aparna Garimella", "Rada Mihalcea", "Akhash Amarnath"], "venue": "AACL", "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.aacl-short.38"}, "doi_lower": "10.18653/v1/2022.aacl-short.38"}
{"paper_id": 4421027, "title": "Datasheets for datasets", "author_names": ["Timnit Gebru", "Jamie Morgenstern", "Briana Vecchione", "Jennifer Wortman Vaughan", "Hanna M. Wallach", "Hal Daum√©", "Kate Crawford"], "venue": "Communications of the ACM", "abstract": "Documentation to facilitate communication between dataset creators and consumers.", "year": 2018, "publicationdate": "2018-03-23", "externalids": {"DOI": "10.1145/3458723"}, "doi_lower": "10.1145/3458723"}
{"paper_id": 221878771, "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models", "author_names": ["Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith"], "venue": "Findings", "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning ‚Äúbad‚Äù words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.", "year": 2020, "publicationdate": "2020-09-24", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.301"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.301"}
{"paper_id": 259859044, "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models", "author_names": ["Somayeh Ghanbarzadeh", "Yan Huang", "Hamid Palangi", "R. C. Moreno", "Hamed Khanpour"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.10522"}, "doi_lower": "10.48550/arxiv.2307.10522"}
{"paper_id": 248780268, "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning", "author_names": ["Michael Gira", "Ruisu Zhang", "Kangwook Lee"], "venue": "LTEDI", "abstract": "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (‚Äúcatastrophic forgetting‚Äù), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.ltedi-1.8"}, "doi_lower": "10.18653/v1/2022.ltedi-1.8"}
{"paper_id": 268856721, "title": "NLP Systems That Can‚Äôt Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps", "author_names": ["Kristina Gligoric", "Myra Cheng", "Lucia Zheng", "Esin Durmus", "Dan Jurafsky"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "The use of words to convey speaker‚Äôs intent is traditionally distinguished from the ‚Äòmention‚Äô of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.", "year": 2024, "publicationdate": "2024-04-02", "externalids": {"DOI": "10.48550/arXiv.2404.01651"}, "doi_lower": "10.48550/arxiv.2404.01651"}
{"paper_id": 260538708, "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them", "author_names": ["Hila Gonen", "Yoav Goldberg"], "venue": "NAACL-HLT", "abstract": null, "year": 2019, "publicationdate": "2019-03-09", "externalids": {}, "doi_lower": null}
{"paper_id": 209379533, "title": "‚ÄúGood‚Äù isn‚Äôt good enough", "author_names": ["Ben Green"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 7840819, "title": "Measuring individual differences in implicit cognition: the implicit association test.", "author_names": ["A. Greenwald", "D. McGhee", "Jordan L. K. Schwartz"], "venue": "Journal of Personality and Social Psychology", "abstract": null, "year": 1998, "publicationdate": "1998-06-01", "externalids": {"DOI": "10.1037/0022-3514.74.6.1464"}, "doi_lower": "10.1037/0022-3514.74.6.1464"}
{"paper_id": 268097760, "title": "Moral Responsibility for Computing Artifacts: \"The Rules\"", "author_names": ["Keith W. Miller"], "venue": "IT Professional Magazine", "abstract": null, "year": 2011, "publicationdate": "2011-05-01", "externalids": {"DOI": "10.1109/MITP.2011.46"}, "doi_lower": "10.1109/mitp.2011.46"}
{"paper_id": 229152766, "title": "Parameter-Efficient Transfer Learning with Diff Pruning", "author_names": ["Demi Guo", "Alexander M. Rush", "Yoon Kim"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific ‚Äúdiff‚Äù vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5% of the pretrained model‚Äôs parameters per task and scales favorably in comparison to popular pruning approaches.", "year": 2020, "publicationdate": "2020-12-14", "externalids": {"DOI": "10.18653/v1/2021.acl-long.378"}, "doi_lower": "10.18653/v1/2021.acl-long.378"}
{"paper_id": 248780440, "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts", "author_names": ["Yue Guo", "Yi Yang", "A. Abbasi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models‚Äô understanding abilities, as shown using the GLUE benchmark.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.acl-long.72"}, "doi_lower": "10.18653/v1/2022.acl-long.72"}
{"paper_id": 247619104, "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal", "author_names": ["Umang Gupta", "J. Dhamala", "Varun Kumar", "Apurv Verma", "Yada Pruksachatkun", "Satyapriya Krishna", "Rahul Gupta", "Kai-Wei Chang", "G. V. Steeg", "A. Galstyan"], "venue": "Findings", "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model‚Äôs biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal‚Äîmodifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT‚Äì2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.", "year": 2022, "publicationdate": "2022-03-23", "externalids": {"DOI": "10.48550/arXiv.2203.12574"}, "doi_lower": "10.48550/arxiv.2203.12574"}
{"paper_id": 245537923, "title": "A Survey on Gender Bias in Natural Language Processing", "author_names": ["Karolina Sta≈Ñczak", "Isabelle Augenstein"], "venue": "arXiv.org", "abstract": "Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research.", "year": 2021, "publicationdate": "2021-12-28", "externalids": {}, "doi_lower": null}
{"paper_id": 202541569, "title": "It‚Äôs All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "author_names": ["R. Maudslay", "Hila Gonen", "Ryan Cotterell", "Simone Teufel"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.", "year": 2019, "publicationdate": "2019-09-02", "externalids": {"DOI": "10.18653/v1/D19-1530"}, "doi_lower": "10.18653/v1/d19-1530"}
{"paper_id": 252734135, "title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts", "author_names": ["Skyler Hallinan", "Alisa Liu", "Yejin Choi", "Maarten Sap"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo‚Äôs rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10543"}, "doi_lower": "10.48550/arxiv.2212.10543"}
{"paper_id": 231698605, "title": "Diverse Adversaries for Mitigating Bias in Training", "author_names": ["Xudong Han", "Timothy Baldwin", "Trevor Cohn"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training.", "year": 2021, "publicationdate": "2021-01-25", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.239"}, "doi_lower": "10.18653/v1/2021.eacl-main.239"}
{"paper_id": 247694107, "title": "Balancing out Bias: Achieving Fairness Through Balanced Training", "author_names": ["Xudong Han", "Timothy Baldwin", "Trevor Cohn"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not directly account for correlations between author demographics and linguistic variables, limiting their effectiveness. To achieve Equal Opportunity fairness, such as equal job opportunity without regard to demographics, this paper introduces a simple, but highly effective, objective for countering bias using balanced training.We extend the method in the form of a gated model, which incorporates protected attributes as input, and show that it is effective at reducing bias in predictions through demographic input perturbation, outperforming all other bias mitigation techniques when combined with balanced training.", "year": 2021, "publicationdate": "2021-09-16", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.779"}, "doi_lower": "10.18653/v1/2022.emnlp-main.779"}
{"paper_id": 247447211, "title": "Towards Equal Opportunity Fairness through Adversarial Learning", "author_names": ["Xudong Han", "Timothy Baldwin", "Trevor Cohn"], "venue": "arXiv.org", "abstract": "Adversarial training is a common approach for bias mitigation in natural language processing. Although most work on debiasing is motivated by equal opportunity, it is not explicitly captured in standard adversarial training. In this paper, we propose an augmented discriminator for adversarial training, which takes the target class as input to create richer features and more explicitly model equal opportunity. Experimental results over two datasets show that our method substantially improves over standard adversarial debiasing methods, in terms of the performance--fairness trade-off.", "year": 2022, "publicationdate": "2022-03-12", "externalids": {"DOI": "10.48550/arXiv.2203.06317"}, "doi_lower": "10.48550/arxiv.2203.06317"}
{"paper_id": 256827269, "title": "Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP", "author_names": ["Xudong Han", "Timothy Baldwin", "Trevor Cohn"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Modern NLP systems exhibit a range of biases, which a growing literature on model debiasing attempts to correct. However, current progress is hampered by a plurality of definitions of bias, means of quantification, and oftentimes vague relation between debiasing algorithms and theoretical measures of bias. This paper seeks to clarify the current situation and plot a course for meaningful progress in fair learning, with two key contributions: (1) making clear inter-relations among the current gamut of methods, and their relation to fairness theory; and (2) addressing the practical problem of model selection, which involves a trade-off between fairness and accuracy and has led to systemic issues in fairness research. Putting them together, we make several recommendations to help shape future work.", "year": 2023, "publicationdate": "2023-02-11", "externalids": {"DOI": "10.48550/arXiv.2302.05711"}, "doi_lower": "10.48550/arxiv.2302.05711"}
{"paper_id": 208921008, "title": "Towards a critical race methodology in algorithmic fairness", "author_names": ["A. Hanna", "Emily L. Denton", "A. Smart", "Jamila Smith-Loud"], "venue": "FAT*", "abstract": "We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.", "year": 2019, "publicationdate": "2019-12-08", "externalids": {"DOI": "10.1145/3351095.3372826"}, "doi_lower": "10.1145/3351095.3372826"}
{"paper_id": 7567061, "title": "Equality of Opportunity in Supervised Learning", "author_names": ["Moritz Hardt", "Eric Price", "N. Srebro"], "venue": "Neural Information Processing Systems", "abstract": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.", "year": 2016, "publicationdate": "2016-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 267060803, "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning", "author_names": ["Adib Hasan", "Ileana Rugina", "Alex Wang"], "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP", "abstract": "This paper investigates the impact of model compression on the way Large Language Models (LLMs) process prompts, particularly concerning jailbreak resistance. We show that moderate WANDA pruning can enhance resistance to jailbreaking attacks without fine-tuning, while maintaining performance on standard benchmarks. To systematically evaluate this safety enhancement, we introduce a dataset of 225 harmful tasks across five categories. Our analysis of LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning benefits correlate with initial model safety levels. We interpret these results by examining changes in attention patterns and perplexity shifts, demonstrating that pruned models exhibit sharper attention and increased sensitivity to artificial jailbreak constructs. We extend our evaluation to the AdvBench harmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much safer on AdvBench prompts than on our dataset when evaluated with manual jailbreak attempts, and that pruning is effective against both automated attacks and manual jailbreaking on Advbench.", "year": 2024, "publicationdate": "2024-01-19", "externalids": {"DOI": "10.48550/arXiv.2401.10862"}, "doi_lower": "10.48550/arxiv.2401.10862"}
{"paper_id": 258461365, "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks", "author_names": ["Lukas Hauzenberger", "Shahed Masoudian", "Deepak Kumar", "M. Schedl", "Navid Rekabsaz"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.", "year": 2022, "publicationdate": "2022-05-30", "externalids": {"DOI": "10.18653/v1/2023.findings-acl.386"}, "doi_lower": "10.18653/v1/2023.findings-acl.386"}
{"paper_id": 253157517, "title": "MABEL: Attenuating Gender Bias using Textual Entailment Data", "author_names": ["Jacqueline He", "Mengzhou Xia", "C. Fellbaum", "Danqi Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized representations. Key to our approach is the use of a contrastive learning objective on counterfactually augmented, gender-balanced entailment pairs from natural language inference (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs along opposite gender directions closer. We extensively evaluate our approach on intrinsic and extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in terms of fairness. It also preserves task performance after fine-tuning on downstream tasks. Together, these findings demonstrate the suitability of NLI data as an effective means of bias mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify that existing approaches often use evaluation settings that are insufficient or inconsistent. We make an effort to reproduce and compare previous methods, and call for unifying the evaluation settings across gender debiasing methods for better future comparison.", "year": 2022, "publicationdate": "2022-10-26", "externalids": {"DOI": "10.48550/arXiv.2210.14975"}, "doi_lower": "10.48550/arxiv.2210.14975"}
{"paper_id": 237634972, "title": "Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding", "author_names": ["Zexue He", "Bodhisattwa Prasad Majumder", "Julian McAuley"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to 'neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes.", "year": 2021, "publicationdate": "2021-09-24", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.352"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.352"}
{"paper_id": 252907344, "title": "Controlling Bias Exposure for Fair Interpretable Predictions", "author_names": ["Zexue He", "Yu Wang", "Julian McAuley", "Bodhisattwa Prasad Majumder"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for a profession, a fair trade-off between task performance and bias mitigation is difficult to achieve. Existing approaches perform this trade-off by eliminating bias information from the latent space, lacking control over how much bias is necessarily required to be removed. We argue that a favorable debiasing method should use sensitive information 'fairly', rather than blindly eliminating it (Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we provide a novel debiasing algorithm by adjusting the predictive model's belief to (1) ignore the sensitive information if it is not useful for the task; (2) use sensitive information minimally as necessary for the prediction (while also incurring a penalty). Experimental results on two text classification tasks (influenced by gender) and an open-ended generation task (influenced by race) indicate that our model achieves a desirable trade-off between debiasing and task performance along with producing debiased rationales as evidence.", "year": 2022, "publicationdate": "2022-10-14", "externalids": {"DOI": "10.48550/arXiv.2210.07455"}, "doi_lower": "10.48550/arxiv.2210.07455"}
{"paper_id": 51880858, "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses", "author_names": ["√örsula H√©bert-Johnson", "Michael P. Kim", "Omer Reingold", "G. Rothblum"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2018, "publicationdate": "2018-07-03", "externalids": {}, "doi_lower": null}
{"paper_id": 59599816, "title": "Parameter-Efficient Transfer Learning for NLP", "author_names": ["N. Houlsby", "A. Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "S. Gelly"], "venue": "International Conference on Machine Learning", "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.", "year": 2019, "publicationdate": "2019-02-02", "externalids": {}, "doi_lower": null}
{"paper_id": 207847197, "title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "author_names": ["Po-Sen Huang", "Huan Zhang", "Ray Jiang", "Robert Stanforth", "Johannes Welbl", "Jack W. Rae", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli"], "venue": "Findings", "abstract": "Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model‚Äôs latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.", "year": 2019, "publicationdate": "2019-11-08", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.7"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.7"}
{"paper_id": 259202452, "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models", "author_names": ["Yue Huang", "Qihui Zhang", "Philip S. Yu", "Lichao Sun"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.", "year": 2023, "publicationdate": "2023-06-20", "externalids": {"DOI": "10.48550/arXiv.2306.11507"}, "doi_lower": "10.48550/arxiv.2306.11507"}
{"paper_id": 218487466, "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "author_names": ["Ben Hutchinson", "Vinodkumar Prabhakaran", "Emily L. Denton", "Kellie Webster", "Yu Zhong", "Stephen Denuyl"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.", "year": 2020, "publicationdate": "2020-05-02", "externalids": {"DOI": "10.18653/v1/2020.acl-main.487"}, "doi_lower": "10.18653/v1/2020.acl-main.487"}
{"paper_id": 258740820, "title": "Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection", "author_names": ["Shadi Iskander", "Kira Radinsky", "Yonatan Belinkov"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10204"}, "doi_lower": "10.48550/arxiv.2305.10204"}
{"paper_id": 209202216, "title": "Measurement and Fairness", "author_names": ["Abigail Z. Jacobs", "Hanna M. Wallach"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.", "year": 2019, "publicationdate": "2019-12-11", "externalids": {"DOI": "10.1145/3442188.3445901"}, "doi_lower": "10.1145/3442188.3445901"}
{"paper_id": 235829093, "title": "Generating Gender Augmented Data for NLP", "author_names": ["N. Jain", "Maja Popovic", "Declan Groves", "Eva Vanmassenhove"], "venue": "GEBNLP", "abstract": "Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to ‚Äòtranslate‚Äô from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.", "year": 2021, "publicationdate": "2021-07-13", "externalids": {"DOI": "10.18653/v1/2021.gebnlp-1.11"}, "doi_lower": "10.18653/v1/2021.gebnlp-1.11"}
{"paper_id": 249282716, "title": "What changed? Investigating Debiasing Methods using Causal Mediation Analysis", "author_names": ["Su-Ha Jeoung", "Jana Diesner"], "venue": "GEBNLP", "abstract": "Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don‚Äôt understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.", "year": 2022, "publicationdate": "2022-06-01", "externalids": {"DOI": "10.48550/arXiv.2206.00701"}, "doi_lower": "10.48550/arxiv.2206.00701"}
{"paper_id": 218613632, "title": "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization", "author_names": ["Shengyu Jia", "Tao Meng", "Jieyu Zhao", "Kai-Wei Chang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models‚Äô top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.", "year": 2020, "publicationdate": "2020-05-13", "externalids": {"DOI": "10.18653/v1/2020.acl-main.264"}, "doi_lower": "10.18653/v1/2020.acl-main.264"}
{"paper_id": 235097594, "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning", "author_names": ["Xisen Jin", "Francesco Barbieri", "Brendan Kennedy", "Aida Mostafazadeh Davani", "Leonardo Neves", "Xiang Ren"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream fine-tuning. We find, in extensive experiments across hate speech detection, toxicity detection and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via fine-tuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.296"}, "doi_lower": "10.18653/v1/2021.naacl-main.296"}
{"paper_id": 250311509, "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning", "author_names": ["Przemyslaw K. Joniak", "Akiko Aizawa"], "venue": "GEBNLP", "abstract": "Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue. We demonstrate a novel framework for inspecting bias in pre-trained transformer-based language models via movement pruning. Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model. We implement our framework by pruning the model while fine-tuning it on the debasing objective. Optimized are only the pruning scores ‚Äì parameters coupled with the model‚Äôs weights that act as gates. We experiment with pruning attention heads, an important building block of transformers: we prune square blocks, as well as establish a new way of pruning the entire heads. Lastly, we demonstrate the usage of our framework using gender bias, and based on our findings, we propose an improvement to an existing debiasing method. Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.", "year": 2022, "publicationdate": "2022-07-06", "externalids": {"DOI": "10.48550/arXiv.2207.02463"}, "doi_lower": "10.48550/arxiv.2207.02463"}
{"paper_id": 220376623, "title": "Don‚Äôt ask if artificial intelligence is good or fair, ask how it shifts power", "author_names": ["Pratyusha Kalluri"], "venue": "Nature", "abstract": null, "year": 2020, "publicationdate": "2020-07-01", "externalids": {"DOI": "10.1038/d41586-020-02003-2"}, "doi_lower": "10.1038/d41586-020-02003-2"}
{"paper_id": 14637938, "title": "Data preprocessing techniques for classification without discrimination", "author_names": ["F. Kamiran", "T. Calders"], "venue": "Knowledge and Information Systems", "abstract": "Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.", "year": 2011, "publicationdate": "2011-12-03", "externalids": {"DOI": "10.1007/s10115-011-0463-8"}, "doi_lower": "10.1007/s10115-011-0463-8"}
{"paper_id": 231698657, "title": "Debiasing Pre-trained Contextualised Embeddings", "author_names": ["Masahiro Kaneko", "Danushka Bollegala"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.", "year": 2021, "publicationdate": "2021-01-23", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.107"}, "doi_lower": "10.18653/v1/2021.eacl-main.107"}
{"paper_id": 233241161, "title": "Unmasking the Mask - Evaluating Social Biases in Masked Language Models", "author_names": ["Masahiro Kaneko", "D. Bollegala"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Masked Language Models (MLMs) have shown superior performances in numerous downstream Natural Language Processing (NLP) tasks.\n Unfortunately, MLMs also demonstrate significantly worrying levels of social biases.\n We show that the previously proposed evaluation metrics for quantifying the social biases in MLMs are problematic due to the following reasons:\n (1) prediction accuracy of the masked tokens itself tend to be low in some MLMs,\n which leads to unreliable evaluation metrics, and \n (2) in most downstream NLP tasks, masks are not used; therefore prediction of the mask is not directly related to them, and \n (3) high-frequency words in the training data are masked more often, introducing noise due to this selection bias in the test cases.\n Therefore, we propose All Unmasked Likelihood (AUL), a bias evaluation measure that predicts all tokens in a test case given the MLM embedding of the unmasked input and AUL with Attention weights (AULA) to evaluate tokens based on their importance in a sentence.\n Our experimental results show that the proposed bias evaluation measures accurately detect different types of biases in MLMs, and unlike AUL and AULA, previously proposed measures for MLMs systematically overestimate the measured biases and are heavily influenced by the unmasked tokens in the context.", "year": 2021, "publicationdate": "2021-04-15", "externalids": {"DOI": "10.1609/aaai.v36i11.21453"}, "doi_lower": "10.1609/aaai.v36i11.21453"}
{"paper_id": 252222228, "title": "Debiasing Isn‚Äôt Enough! ‚Äì on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks", "author_names": ["Masahiro Kaneko", "D. Bollegala", "Naoaki Okazaki"], "venue": "International Conference on Computational Linguistics", "abstract": "We study the relationship between task-agnostic intrinsic and task-specific extrinsic social bias evaluation measures for MLMs, and find that there exists only a weak correlation between these two types of evaluation measures. Moreover, we find that MLMs debiased using different methods still re-learn social biases during fine-tuning on downstream tasks. We identify the social biases in both training instances as well as their assigned labels as reasons for the discrepancy between intrinsic and extrinsic bias evaluation measurements. Overall, our findings highlight the limitations of existing MLM bias evaluation measures and raise concerns on the deployment of MLMs in downstream applications using those measures.", "year": 2022, "publicationdate": "2022-10-06", "externalids": {"DOI": "10.48550/arXiv.2210.02938"}, "doi_lower": "10.48550/arxiv.2210.02938"}
{"paper_id": 29169376, "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness", "author_names": ["Michael Kearns", "Seth Neel", "Aaron Roth", "Zhiwei Steven Wu"], "venue": "International Conference on Machine Learning", "abstract": "The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent \"fairness gerrymandering\", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. \nWe then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.", "year": 2017, "publicationdate": "2017-11-14", "externalids": {}, "doi_lower": null}
{"paper_id": 258298563, "title": "Learn What NOT to Learn: Towards Generative Safety in Chatbots", "author_names": ["Leila Khalatbari", "Yejin Bang", "Dan Su", "Willy Chung", "Saeedeh Ghadimi", "H. Sameti", "Pascale Fung"], "venue": "arXiv.org", "abstract": "Conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. Prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. In this paper, we present a novel framework, named\"LOT\"(Learn NOT to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. Our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. The LOT framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. Our approach is memory and time-efficient during decoding and effectively reduces toxicity while preserving engagingness and fluency. Empirical results indicate that LOT reduces toxicity by up to four-fold while achieving four to six-fold higher rates of engagingness and fluency compared to baseline models. Our findings are further corroborated by human evaluation.", "year": 2023, "publicationdate": "2023-04-21", "externalids": {"DOI": "10.48550/arXiv.2304.11220"}, "doi_lower": "10.48550/arxiv.2304.11220"}
{"paper_id": 233444226, "title": "Dynabench: Rethinking Benchmarking in NLP", "author_names": ["Douwe Kiela", "Max Bartolo", "Yixin Nie", "Divyansh Kaushik", "Atticus Geiger", "Zhengxuan Wu", "Bertie Vidgen", "Grusha Prasad", "Amanpreet Singh", "Pratik Ringshia", "Zhiyi Ma", "Tristan Thrush", "Sebastian Riedel", "Zeerak Talat", "Pontus Stenetorp", "Robin Jia", "Mohit Bansal", "Christopher Potts", "Adina Williams"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.", "year": 2021, "publicationdate": "2021-04-07", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.324"}, "doi_lower": "10.18653/v1/2021.naacl-main.324"}
{"paper_id": 249062866, "title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents", "author_names": ["Hyunwoo Kim", "Youngjae Yu", "Liwei Jiang", "Ximing Lu", "Daniel Khashabi", "Gunhee Kim", "Yejin Choi", "Maarten Sap"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales.With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.", "year": 2022, "publicationdate": "2022-05-25", "externalids": {"DOI": "10.48550/arXiv.2205.12688"}, "doi_lower": "10.48550/arxiv.2205.12688"}
{"paper_id": 254926596, "title": "Critic-Guided Decoding for Controlled Text Generation", "author_names": ["Minbeom Kim", "Hwanhee Lee", "Kang Min Yoo", "Joonsuk Park", "Hwaran Lee", "Kyomin Jung"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework to train an LM-steering critic from non-differentiable reward models. And similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using called critic, improving training efficiency and stability. Evaluation of our method on three controlled generation tasks, namely topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.48550/arXiv.2212.10938"}, "doi_lower": "10.48550/arxiv.2212.10938"}
{"paper_id": 21670658, "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "author_names": ["S. Kiritchenko", "Saif M. Mohammad"], "venue": "International Workshop on Semantic Evaluation", "abstract": "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 ‚ÄòAffect in Tweets‚Äô. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.", "year": 2018, "publicationdate": "2018-05-11", "externalids": {"DOI": "10.18653/v1/S18-2005"}, "doi_lower": "10.18653/v1/s18-2005"}
{"paper_id": 4704285, "title": "Overcoming catastrophic forgetting in neural networks", "author_names": ["J. Kirkpatrick", "Razvan Pascanu", "Neil C. Rabinowitz", "J. Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "A. Grabska-Barwinska", "D. Hassabis", "C. Clopath", "D. Kumaran", "R. Hadsell"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.", "year": 2016, "publicationdate": "2016-12-02", "externalids": {"DOI": "10.1073/pnas.1611835114"}, "doi_lower": "10.1073/pnas.1611835114"}
{"paper_id": 249017743, "title": "Large Language Models are Zero-Shot Reasoners", "author_names": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "venue": "Neural Information Processing Systems", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 221655075, "title": "GeDi: Generative Discriminator Guided Sequence Generation", "author_names": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "N. Keskar", "Shafiq R. Joty", "R. Socher", "Nazneen Rajani"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "year": 2020, "publicationdate": "2020-09-14", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.424"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.424"}
{"paper_id": 246035444, "title": "Grep-BiasIR: A Dataset for Investigating Gender Representation Bias in Information Retrieval Results", "author_names": ["Klara Krieg", "Emilia Parada-Cabaleiro", "G. Medicus", "Oleg Lesota", "M. Schedl", "Navid Rekabsaz"], "venue": "Conference on Human Information Interaction and Retrieval", "abstract": "The provided contents by information retrieval (IR) systems can reflect the existing societal biases and stereotypes. Such biases in retrieval results can lead to further establishing and strengthening stereotypes in society and also in the systems. To facilitate the studies of gender bias in the retrieval results of IR systems, we introduce Gender Representation-Bias for Information Retrieval (Grep-BiasIR), a novel thoroughly-audited dataset consisting of 118 bias-sensitive neutral search queries. The set of queries covers a wide range of gender-related topics, for which a biased representation of genders in the search result can be considered as socially problematic. Each query is accompanied with one relevant and one non-relevant document, where the document is also provided in three variations of female, male, and neutral. The dataset is available at https://github.com/KlaraKrieg/GrepBiasIR.", "year": 2022, "publicationdate": "2022-01-19", "externalids": {"DOI": "10.1145/3576840.3578295"}, "doi_lower": "10.1145/3576840.3578295"}
{"paper_id": 256827819, "title": "Parameter-efficient Modularised Bias Mitigation via AdapterFusion", "author_names": ["Deepak Kumar", "Oleg Lesota", "George Zerveas", "Daniel Cohen", "Carsten Eickhoff", "M. Schedl", "Navid Rekabsaz"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model‚Äôs parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) ‚Äì a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models.", "year": 2023, "publicationdate": "2023-02-13", "externalids": {"DOI": "10.48550/arXiv.2302.06321"}, "doi_lower": "10.48550/arxiv.2302.06321"}
{"paper_id": 252907607, "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey", "author_names": ["Sachin Kumar", "Vidhisha Balachandran", "Lucille Njoo", "Antonios Anastasopoulos", "Yulia Tsvetkov"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works‚Äô taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different strategies‚Äô motivations, their limitations, and open problems for future research.", "year": 2022, "publicationdate": "2022-10-14", "externalids": {"DOI": "10.48550/arXiv.2210.07700"}, "doi_lower": "10.48550/arxiv.2210.07700"}
{"paper_id": 190000105, "title": "Measuring Bias in Contextualized Word Representations", "author_names": ["Keita Kurita", "Nidhi Vyas", "Ayush Pareek", "A. Black", "Yulia Tsvetkov"], "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing", "abstract": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.", "year": 2019, "publicationdate": "2019-06-18", "externalids": {"DOI": "10.18653/v1/W19-3823"}, "doi_lower": "10.18653/v1/w19-3823"}
{"paper_id": 237440429, "title": "Sustainable Modular Debiasing of Language Models", "author_names": ["Anne Lauscher", "Tobias L√ºken", "Goran Glavas"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.411"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.411"}
{"paper_id": 236519346, "title": "Ethical Data Curation for AI: An Approach based on Feminist Epistemology and Critical Theories of Race", "author_names": ["Susan Leavy", "E. Siapera", "B. O‚ÄôSullivan"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "The potential for bias embedded in data to lead to the perpetuation of social injustice though Artificial Intelligence (AI) necessitates an urgent reform of data curation practices for AI systems, especially those based on machine learning. Without appropriate ethical and regulatory frameworks there is a risk that decades of advances in human rights and civil liberties may be undermined. This paper proposes an approach to data curation for AI, grounded in feminist epistemology and informed by critical theories of race and feminist principles. The objective of this approach is to support critical evaluation of the social dynamics of power embedded in data for AI systems. We propose a set of fundamental guiding principles for ethical data curation that address the social construction of knowledge, call for inclusion of subjugated and new forms of knowledge, support critical evaluation of theoretical concepts within data and recognise the reflexive nature of knowledge. In developing this ethical framework for data curation, we aim to contribute to a virtue ethics for AI and ensure protection of fundamental and human rights.", "year": 2021, "publicationdate": "2021-07-21", "externalids": {"DOI": "10.1145/3461702.3462598"}, "doi_lower": "10.1145/3461702.3462598"}
{"paper_id": 233296808, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "author_names": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this work, we explore ‚Äúprompt tuning,‚Äù a simple yet effective mechanism for learning ‚Äúsoft prompts‚Äù to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3‚Äôs few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ‚Äúcloses the gap‚Äù and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ‚Äúprefix tuning‚Äù of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient ‚Äúprompt ensembling.‚Äù We release code and model checkpoints to reproduce our experiments.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.243"}, "doi_lower": "10.18653/v1/2021.emnlp-main.243"}
{"paper_id": 15710851, "title": "The Winograd Schema Challenge", "author_names": ["H. Levesque", "E. Davis", "L. Morgenstern"], "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning", "abstract": null, "year": 2011, "publicationdate": "2011-03-20", "externalids": {}, "doi_lower": null}
{"paper_id": 237452751, "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation", "author_names": ["Shahar Levy", "Koren Lazar", "Gabriel Stanovsky"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.211"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.211"}
{"paper_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "author_names": ["M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdel-rahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "publicationdate": "2019-10-29", "externalids": {"DOI": "10.18653/v1/2020.acl-main.703"}, "doi_lower": "10.18653/v1/2020.acl-main.703"}
{"paper_id": 222141056, "title": "UNQOVERing Stereotypical Biases via Underspecified Questions", "author_names": ["Tao Li", "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Vivek Srikumar"], "venue": "Findings", "abstract": "While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.", "year": 2020, "publicationdate": "2020-10-06", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.311"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.311"}
{"paper_id": 259342087, "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases", "author_names": ["Yingji Li", "Mengnan Du", "Xin Wang", "Y. Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs‚Äô encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs‚Äô parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.", "year": 2023, "publicationdate": "2023-07-04", "externalids": {"DOI": "10.48550/arXiv.2307.01595"}, "doi_lower": "10.48550/arxiv.2307.01595"}
{"paper_id": 258967578, "title": "Fairness of ChatGPT", "author_names": ["Yunqi Li", "Yongfeng Zhang"], "venue": "arXiv.org", "abstract": "Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.18569"}, "doi_lower": "10.48550/arxiv.2305.18569"}
{"paper_id": 207996257, "title": "Towards Debiasing Sentence Representations", "author_names": ["Paul Pu Liang", "Irene Z Li", "Emily Zheng", "Y. Lim", "R. Salakhutdinov", "Louis-philippe Morency"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.", "year": 2020, "publicationdate": "2020-07-01", "externalids": {"DOI": "10.18653/v1/2020.acl-main.488"}, "doi_lower": "10.18653/v1/2020.acl-main.488"}
{"paper_id": 235623756, "title": "Towards Understanding and Mitigating Social Biases in Language Models", "author_names": ["Paul Pu Liang", "Chiyu Wu", "Louis-philippe Morency", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning", "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.", "year": 2021, "publicationdate": "2021-06-24", "externalids": {}, "doi_lower": null}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher R√©", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Y√ºksekg√∂n√ºl", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 235313967, "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts", "author_names": ["Alisa Liu", "Maarten Sap", "Ximing Lu", "Swabha Swayamdipta", "Chandra Bhagavatula", "Noah A. Smith", "Yejin Choi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with ‚Äúexpert‚Äù LMs and/or ‚Äúanti-expert‚Äù LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.", "year": 2021, "publicationdate": "2021-05-07", "externalids": {"DOI": "10.18653/v1/2021.acl-long.522"}, "doi_lower": "10.18653/v1/2021.acl-long.522"}
{"paper_id": 204838020, "title": "Does Gender Matter? Towards Fairness in Dialogue Systems", "author_names": ["Haochen Liu", "Jamell Dacon", "Wenqi Fan", "Hui Liu", "Zitao Liu", "Jiliang Tang"], "venue": "International Conference on Computational Linguistics", "abstract": "Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as ‚Äúgorillas‚Äù. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.", "year": 2019, "publicationdate": "2019-10-16", "externalids": {"DOI": "10.18653/V1/2020.COLING-MAIN.390"}, "doi_lower": "10.18653/v1/2020.coling-main.390"}
{"paper_id": 236493269, "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "author_names": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "venue": "ACM Computing Surveys", "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub ‚Äúprompt-based learning.‚Äù Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x‚Ä≤ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string xÃÇ, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia‚ÄìPretrain including constantly updated survey and paperlist.", "year": 2021, "publicationdate": "2021-07-28", "externalids": {"DOI": "10.1145/3560815"}, "doi_lower": "10.1145/3560815"}
{"paper_id": 233476528, "title": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "author_names": ["Ruibo Liu", "Chenyan Jia", "Jason Wei", "Guangxuan Xu", "Lili Wang", "Soroush Vosoughi"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in real-world settings. In this paper, we describe metrics for measuring political bias in GPT-2 generation and propose a reinforcement learning (RL) framework for mitigating political biases in generated text. By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence.", "year": 2021, "publicationdate": "2021-04-30", "externalids": {"DOI": "10.1609/aaai.v35i17.17744"}, "doi_lower": "10.1609/aaai.v35i17.17744"}
{"paper_id": 258832588, "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases", "author_names": ["Xin Liu", "Muhammad Khalifa", "Lu Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model‚Äôs output logits. Unlike prior work, BOLT maintains the generator‚Äôs autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), BOLT demonstrates significantly improved efficiency and fluency. On sentiment control, BOLT is 7x faster than competitive baselines, and more fluent in 74.4% of the evaluation samples according to human judges.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.48550/arXiv.2305.12018"}, "doi_lower": "10.48550/arxiv.2305.12018"}
{"paper_id": 198953378, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "author_names": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "year": 2019, "publicationdate": "2019-07-26", "externalids": {}, "doi_lower": null}
{"paper_id": 151930587, "title": "Implicit attitudes and the perception of sociolinguistic variation", "author_names": ["Brandon C. Loudermilk"], "venue": "", "abstract": null, "year": 2015, "publicationdate": "2015-12-15", "externalids": {"DOI": "10.1075/impact.39.06lou"}, "doi_lower": "10.1075/impact.39.06lou"}
{"paper_id": 225080424, "title": "Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday", "author_names": ["Tajana Ban Kirigin", "C. Talcott", "J. Guttman", "S. Kuznetsov", "B. T. Loo", "M. Okada"], "venue": "Logic, Language, and Security", "abstract": "Infinitary action logic is an extension of the multiplicativeadditive Lambek calculus with Kleene iteration, axiomatized by an œârule. Buszkowski and Palka (2007) show that this logic is Œ† 1 -complete. As shown recently by Kuznetsov and Speranski, the extension of infinitary action logic with the exponential modality is much harder: Œ† 1 complete. The raise of complexity is of course due to the contraction rule. We investigate fragments of infinitary action logic with exponential, which still include contraction, but have lower (e.g., arithmetically bounded) complexity. In this paper, we show an upper Œ† 1 bound for the fragment of infinitary action logic, in which the exponential can be applied only to formulae of implication depth 0 or 1.", "year": 2020, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-030-62077-6"}, "doi_lower": "10.1007/978-3-030-62077-6"}
{"paper_id": 249152301, "title": "Quark: Controllable Text Generation with Reinforced Unlearning", "author_names": ["Ximing Lu", "S. Welleck", "Liwei Jiang", "Jack Hessel", "Lianhui Qin", "Peter West", "Prithviraj Ammanabrolu", "Yejin Choi"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.", "year": 2022, "publicationdate": "2022-05-26", "externalids": {"DOI": "10.48550/arXiv.2205.13636"}, "doi_lower": "10.48550/arxiv.2205.13636"}
{"paper_id": 225067055, "title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints", "author_names": ["Ximing Lu", "Peter West", "Rowan Zellers", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Conditional text generation often requires lexical constraints, i.e., which words should or shouldn‚Äôt be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models ‚Äì supervised or not ‚Äì to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.", "year": 2020, "publicationdate": "2020-10-24", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.339"}, "doi_lower": "10.18653/v1/2021.naacl-main.339"}
{"paper_id": 21889700, "title": "A Unified Approach to Interpreting Model Predictions", "author_names": ["Scott M. Lundberg", "Su-In Lee"], "venue": "Neural Information Processing Systems", "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.", "year": 2017, "publicationdate": "2017-05-22", "externalids": {}, "doi_lower": null}
{"paper_id": 225075985, "title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction", "author_names": ["Xinyao Ma", "Maarten Sap", "Hannah Rashkin", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (‚Äú_She daydreams about being a doctor_‚Äù) while a man is portrayed as more proactive and powerful (‚Äú_He pursues his dream of being a doctor_‚Äù). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.", "year": 2020, "publicationdate": "2020-10-26", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.602"}, "doi_lower": "10.18653/v1/2020.emnlp-main.602"}
{"paper_id": 142809360, "title": "Linguistic Intergroup Bias: Stereotype Perpetuation Through Language", "author_names": ["A. Maass"], "venue": "", "abstract": null, "year": 1999, "publicationdate": null, "externalids": {"DOI": "10.1016/S0065-2601(08)60272-5"}, "doi_lower": "10.1016/s0065-2601(08)60272-5"}
{"paper_id": 252907402, "title": "InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions", "author_names": ["Bodhisattwa Prasad Majumder", "Zexue He", "Julian McAuley"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information 'fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.", "year": 2022, "publicationdate": "2022-10-14", "externalids": {"DOI": "10.48550/arXiv.2210.07440"}, "doi_lower": "10.48550/arxiv.2210.07440"}
{"paper_id": 239009591, "title": "Socially Aware Bias Measurements for Hindi Language Representations", "author_names": ["Vijit Malik", "Sunipa Dev", "A. Nishi", "Nanyun Peng", "Kai-Wei Chang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Language representations are an efficient tool used across NLP, but they are strife with encoded societal biases. These biases are studied extensively, but with a primary focus on English language representations and biases common in the context of Western society. In this work, we investigate the biases present in Hindi language representations such as caste and religion associated biases. We demonstrate how biases are unique to specific language representations based on the history and culture of the region they are widely spoken in, and also how the same societal bias (such as binary gender associated biases) when investigated across languages is encoded by different words and text spans. With this work, we emphasize on the necessity of social-awareness along with linguistic and grammatical artefacts when modeling language representations, in order to understand the biases encoded.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.76"}, "doi_lower": "10.18653/v1/2022.naacl-main.76"}
{"paper_id": 102350941, "title": "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings", "author_names": ["Thomas Manzini", "Y. Lim", "Yulia Tsvetkov", "A. Black"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.", "year": 2019, "publicationdate": "2019-04-01", "externalids": {"DOI": "10.18653/v1/N19-1062"}, "doi_lower": "10.18653/v1/n19-1062"}
{"paper_id": 273994217, "title": "Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing", "author_names": ["Justus Mattern", "Zhijing Jin", "Mrinmaya Sachan", "Rada Mihalcea", "Bernhard Sch√∂lkopf"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2212.10678"}, "doi_lower": "10.48550/arxiv.2212.10678"}
{"paper_id": 256503647, "title": "Using In-Context Learning to Improve Dialogue Safety", "author_names": ["Nicholas Meade", "Spandana Gella", "Devamanyu Hazarika", "Prakhar Gupta", "Di Jin", "Siva Reddy", "Yang Liu", "Dilek Z. Hakkani-T√ºr"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.", "year": 2023, "publicationdate": "2023-02-02", "externalids": {"DOI": "10.48550/arXiv.2302.00871"}, "doi_lower": "10.48550/arxiv.2302.00871"}
{"paper_id": 239015827, "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models", "author_names": ["Nicholas Meade", "Elinor Poole-Dayan", "Siva Reddy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model‚Äôs language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.", "year": 2021, "publicationdate": "2021-10-16", "externalids": {"DOI": "10.18653/v1/2022.acl-long.132"}, "doi_lower": "10.18653/v1/2022.acl-long.132"}
{"paper_id": 250390493, "title": "A Taxonomy of Bias-Causing Ambiguities in Machine Translation", "author_names": ["M. Mechura"], "venue": "GEBNLP", "abstract": "This paper introduces a taxonomy of phenomena which cause bias in machine translation, covering gender bias (people being male and/or female), number bias (singular you versus plural you) and formality bias (informal you versus formal you). Our taxonomy is a formalism for describing situations in machine translation when the source text leaves some of these properties unspecified (eg. does not say whether doctor is male or female) but the target language requires the property to be specified (eg. because it does not have a gender-neutral word for doctor). The formalism described here is used internally by a web-based tool we have built for detecting and correcting bias in the output of any machine translator.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.gebnlp-1.18"}, "doi_lower": "10.18653/v1/2022.gebnlp-1.18"}
{"paper_id": 201666566, "title": "A Survey on Bias and Fairness in Machine Learning", "author_names": ["Ninareh Mehrabi", "Fred Morstatter", "N. Saxena", "Kristina Lerman", "A. Galstyan"], "venue": "ACM Computing Surveys", "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.", "year": 2019, "publicationdate": "2019-08-23", "externalids": {"DOI": "10.1145/3457607"}, "doi_lower": "10.1145/3457607"}
{"paper_id": 259129801, "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "author_names": ["Katelyn X. Mei", "Sonia Fereidooni", "Aylin Caliskan"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson‚Äôs r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.1145/3593013.3594109"}, "doi_lower": "10.1145/3593013.3594109"}
{"paper_id": 240420063, "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey", "author_names": ["Bonan Min", "Hayley Ross", "Elior Sulem", "Amir Pouran Ben Veyseh", "Thien Huu Nguyen", "Oscar Sainz", "Eneko Agirre", "Ilana Heinz", "D. Roth"], "venue": "ACM Computing Surveys", "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.", "year": 2021, "publicationdate": "2021-11-01", "externalids": {"DOI": "10.1145/3605943"}, "doi_lower": "10.1145/3605943"}
{"paper_id": 221136077, "title": "Hate speech detection and racial bias mitigation in social media based on BERT model", "author_names": ["Marzieh Mozafari", "R. Farahbakhsh", "N. Crespi"], "venue": "PLoS ONE", "abstract": "Disparate biases associated with datasets and trained classifiers in hateful and abusive content identification tasks have raised many concerns recently. Although the problem of biased datasets on abusive language detection has been addressed more frequently, biases arising from trained classifiers have not yet been a matter of concern. In this paper, we first introduce a transfer learning approach for hate speech detection based on an existing pre-trained language model called BERT (Bidirectional Encoder Representations from Transformers) and evaluate the proposed model on two publicly available datasets that have been annotated for racism, sexism, hate or offensive content on Twitter. Next, we introduce a bias alleviation mechanism to mitigate the effect of bias in training set during the fine-tuning of our pre-trained BERT-based model for hate speech detection. Toward that end, we use an existing regularization method to reweight input samples, thereby decreasing the effects of high correlated training set‚Äô s n-grams with class labels, and then fine-tune our pre-trained BERT-based model with the new re-weighted samples. To evaluate our bias alleviation mechanism, we employed a cross-domain approach in which we use the trained classifiers on the aforementioned datasets to predict the labels of two new datasets from Twitter, AAE-aligned and White-aligned groups, which indicate tweets written in African-American English (AAE) and Standard American English (SAE), respectively. The results show the existence of systematic racial bias in trained classifiers, as they tend to assign tweets written in AAE from AAE-aligned group to negative classes such as racism, sexism, hate, and offensive more often than tweets written in SAE from White-aligned group. However, the racial bias in our classifiers reduces significantly after our bias alleviation mechanism is incorporated. This work could institute the first step towards debiasing hate speech and abusive language detection systems.", "year": 2020, "publicationdate": "2020-08-14", "externalids": {"DOI": "10.1371/journal.pone.0237861"}, "doi_lower": "10.1371/journal.pone.0237861"}
{"paper_id": 215828184, "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "author_names": ["Moin Nadeem", "Anna Bethke", "Siva Reddy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.", "year": 2020, "publicationdate": "2020-04-20", "externalids": {"DOI": "10.18653/v1/2021.acl-long.416"}, "doi_lower": "10.18653/v1/2021.acl-long.416"}
{"paper_id": 222090785, "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models", "author_names": ["Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel R. Bowman"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.", "year": 2020, "publicationdate": "2020-09-30", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.154"}, "doi_lower": "10.18653/v1/2020.emnlp-main.154"}
{"paper_id": 237142520, "title": "Mitigating harm in language models with conditional-likelihood filtration", "author_names": ["Helen Ngo", "Cooper D. Raterink", "J. Ara'ujo", "Ivan Zhang", "Carol Chen", "Adrien Morisot", "Nick Frosst"], "venue": "arXiv.org", "abstract": "Language models trained on large-scale unfiltered datasets curated from the open web acquire systemic biases, prejudices, and harmful views from their training data. We present a methodology for programmatically identifying and removing harmful text from web-scale datasets. A pretrained language model is used to calculate the log-likelihood of researcher-written trigger phrases conditioned on a specific document, which is used to identify and filter documents from the dataset. We demonstrate that models trained on this filtered dataset exhibit lower propensity to generate harmful text, with a marginal decrease in performance on standard language modeling benchmarks compared to unfiltered baselines. We provide a partial explanation for this performance gap by surfacing examples of hate speech and other undesirable content from standard language modeling benchmarks. Finally, we discuss the generalization of this method and how trigger phrases which reflect specific values can be used by researchers to build language models which are more closely aligned with their values.", "year": 2021, "publicationdate": "2021-08-04", "externalids": {}, "doi_lower": null}
{"paper_id": 235097294, "title": "HONEST: Measuring Hurtful Sentence Completion in Language Models", "author_names": ["Debora Nozza", "Federico Bianchi", "Dirk Hovy"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9% of the time, and in 4% to homosexuality when the target is male. The results raise questions about the use of these models in production settings.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.191"}, "doi_lower": "10.18653/v1/2021.naacl-main.191"}
{"paper_id": 249847943, "title": "Learning Fair Representation via Distributional Contrastive Disentanglement", "author_names": ["Changdae Oh", "Heeji Won", "Junhyuk So", "Taero Kim", "Yewon Kim", "Hosik Choi", "Kyungwoo Song"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Learning fair representation is crucial for achieving fairness or debiasing sensitive information. Most existing works rely on adversarial representation learning to inject some invariance into representation. However, adversarial learning methods are known to suffer from relatively unstable training, and this might harm the balance between fairness and predictiveness of representation. We propose a new approach, learningFAir Representation via distributional CONtrastive Variational AutoEncoder (FarconVAE), which induces the latent space to be disentangled into sensitive and non-sensitive parts. We first construct the pair of observations with different sensitive attributes but with the same labels. Then, FarconVAE enforces each non-sensitive latent to be closer, while sensitive latents to be far from each other and also far from the non-sensitive latent by contrasting their distributions. We provide a new type of contrastive loss motivated by Gaussian and Student-t kernels for distributional contrastive learning with theoretical analysis. Besides, we adopt a new swap-reconstruction loss to boost the disentanglement further. FarconVAE shows superior performance on fairness, pretrained model debiasing, and domain generalization tasks from various modalities, including tabular, image, and text.", "year": 2022, "publicationdate": "2022-06-17", "externalids": {"DOI": "10.1145/3534678.3539232"}, "doi_lower": "10.1145/3534678.3539232"}
{"paper_id": 259370845, "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model", "author_names": ["Ali Omrani", "Alireza S. Ziabari", "Charles Yu", "Preni Golazizian", "Brendan Kennedy", "M. Atari", "Heng Ji", "Morteza Dehghani"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Existing bias mitigation methods require social-group-specific word pairs (e.g., ‚Äúman‚Äù ‚Äì ‚Äúwoman‚Äù) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) ‚Äî a theoretical framework developed in social psychology for understanding the content of stereotyping ‚Äî can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: ‚Äúgenuine‚Äù ‚Äì ‚Äúfake‚Äù; competence: ‚Äúsmart‚Äù ‚Äì ‚Äústupid‚Äù), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.227"}, "doi_lower": "10.18653/v1/2023.acl-long.227"}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pond√© de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 250390436, "title": "Choose Your Lenses: Flaws in Gender Bias Evaluation", "author_names": ["Hadas Orgad", "Yonatan Belinkov"], "venue": "GEBNLP", "abstract": "Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model‚Äôs performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. We find that only a few extrinsic metrics are measured in most studies, although more can be measured. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate how the choice of the dataset and its composition, as well as the choice of the metric, affect bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.18653/v1/2022.gebnlp-1.17"}, "doi_lower": "10.18653/v1/2022.gebnlp-1.17"}
{"paper_id": 259138920, "title": "BLIND: Bias Removal With No Demographics", "author_names": ["Hadas Orgad", "Yonatan Belinkov"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model‚Äôs success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.18653/v1/2023.acl-long.490"}, "doi_lower": "10.18653/v1/2023.acl-long.490"}
{"paper_id": 248177827, "title": "How Gender Debiasing Affects Internal Model Representations, and Why It Matters", "author_names": ["Hadas Orgad", "Seraphina Goldfarb-Tarrant", "Yonatan Belinkov"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models‚Äô internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available.", "year": 2022, "publicationdate": "2022-04-14", "externalids": {"DOI": "10.48550/arXiv.2204.06827"}, "doi_lower": "10.48550/arxiv.2204.06827"}
{"paper_id": 236460108, "title": "Probing Toxic Content in Large Pre-Trained Language Models", "author_names": ["Nedjma Djouhra Ousidhoum", "Xinran Zhao", "Tianqing Fang", "Yangqiu Song", "Dit-Yan Yeung"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.329"}, "doi_lower": "10.18653/v1/2021.acl-long.329"}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 250390445, "title": "Incorporating Subjectivity into Gendered Ambiguous Pronoun (GAP) Resolution using Style Transfer", "author_names": ["Kartikey Pant", "Tanvi Dadu"], "venue": "GEBNLP", "abstract": "The GAP dataset is a Wikipedia-based evaluation dataset for gender bias detection in coreference resolution, containing mostly objective sentences. Since subjectivity is ubiquitous in our daily texts, it becomes necessary to evaluate models for both subjective and objective instances. In this work, we present a new evaluation dataset for gender bias in coreference resolution, GAP-Subjective, which increases the coverage of the original GAP dataset by including subjective sentences. We outline the methodology used to create this dataset. Firstly, we detect objective sentences and transfer them into their subjective variants using a sequence-to-sequence model. Secondly, we outline the thresholding techniques based on fluency and content preservation to maintain the quality of the sentences. Thirdly, we perform automated and human-based analysis of the style transfer and infer that the transferred sentences are of high quality. Finally, we benchmark both GAP and GAP-Subjective datasets using a BERT-based model and analyze its predictive performance and gender bias.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.gebnlp-1.28"}, "doi_lower": "10.18653/v1/2022.gebnlp-1.28"}
{"paper_id": 257079734, "title": "Never Too Late to Learn: Regularizing Gender Bias in Coreference Resolution", "author_names": ["SunYoung Park", "Kyuri Choi", "Haeun Yu", "Youngjoong Ko"], "venue": "Web Search and Data Mining", "abstract": "Leveraging pre-trained language models (PLMs) as initializers for efficient transfer learning has become a universal approach for text-related tasks. However, the models not only learn the language understanding abilities but also reproduce prejudices for certain groups in the datasets used for pre-training. Recent studies show that the biased knowledge acquired from the datasets affects the model predictions on downstream tasks. In this paper, we mitigate and analyze the gender biases in PLMs with coreference resolution, which is one of the natural language understanding (NLU) tasks. PLMs exhibit two types of gender biases: stereotype and skew. The primary causes for the biases are the imbalanced datasets with more male examples and the stereotypical examples on gender roles. While previous studies mainly focused on the skew problem, we aim to mitigate both gender biases in PLMs while maintaining the model's original linguistic capabilities. Our method employs two regularization terms, Stereotype Neutralization (SN) and Elastic Weight Consolidation (EWC). The models trained with the methods show to be neutralized and reduce the biases significantly on the WinoBias dataset compared to the public BERT. We also invented a new gender bias quantification metric called the Stereotype Quantification (SQ) score. In addition to the metrics, embedding visualizations were used to interpret how our methods have successfully debiased the models.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.1145/3539597.3570473"}, "doi_lower": "10.1145/3539597.3570473"}
{"paper_id": 239010011, "title": "BBQ: A hand-built bias benchmark for question answering", "author_names": ["Alicia Parrish", "Angelica Chen", "Nikita Nangia", "Vishakh Padmakumar", "Jason Phang", "Jana Thompson", "Phu Mon Htut", "Sam Bowman"], "venue": "Findings", "abstract": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model‚Äôs biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model‚Äôs outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.165"}, "doi_lower": "10.18653/v1/2022.findings-acl.165"}
{"paper_id": 218470208, "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "author_names": ["Jonas Pfeiffer", "Aishwarya Kamath", "Andreas R√ºckl√©", "Kyunghyun Cho", "Iryna Gurevych"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.39"}, "doi_lower": "10.18653/v1/2021.eacl-main.39"}
{"paper_id": 258309492, "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research", "author_names": ["Luiza Amador Pozzobon", "B. Ermi≈ü", "Patrick Lewis", "Sara Hooker"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https://github.com/for-ai/black-box-api-challenges.", "year": 2023, "publicationdate": "2023-04-24", "externalids": {"DOI": "10.48550/arXiv.2304.12397"}, "doi_lower": "10.48550/arxiv.2304.12397"}
{"paper_id": 258440108, "title": "The Other Side of Compression: Measuring Bias in Pruned Transformers", "author_names": ["I. Proskurina", "G. Metzler", "Julien Velcin"], "venue": "International Symposium on Intelligent Data Analysis", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-031-30047-9_29"}, "doi_lower": "10.1007/978-3-031-30047-9_29"}
{"paper_id": 208248333, "title": "Automatically Neutralizing Subjective Bias in Text", "author_names": ["Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "S. Kurohashi", "Dan Jurafsky", "Diyi Yang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity ‚Äî introducing attitudes via framing, presupposing truth, and casting doubt ‚Äî remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (‚Äúneutralizing‚Äù biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque concurrent system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable modular algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.", "year": 2019, "publicationdate": "2019-11-21", "externalids": {"DOI": "10.1609/aaai.v34i01.5385"}, "doi_lower": "10.1609/aaai.v34i01.5385"}
{"paper_id": 249062690, "title": "Perturbation Augmentation for Fairer NLP", "author_names": ["Rebecca Qian", "Candace Ross", "Jude Fernandes", "Eric Michael Smith", "Douwe Kiela", "Adina Williams"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.", "year": 2022, "publicationdate": "2022-05-25", "externalids": {"DOI": "10.48550/arXiv.2205.12586"}, "doi_lower": "10.48550/arxiv.2205.12586"}
{"paper_id": 170078973, "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "author_names": ["Yusu Qian", "Urwa Muaz", "Ben Zhang", "J. Hyun"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.", "year": 2019, "publicationdate": "2019-05-30", "externalids": {"DOI": "10.18653/v1/P19-2031"}, "doi_lower": "10.18653/v1/p19-2031"}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 11816014, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author_names": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL", "year": 2016, "publicationdate": "2016-06-16", "externalids": {"DOI": "10.18653/v1/D16-1264"}, "doi_lower": "10.18653/v1/d16-1264"}
{"paper_id": 258840972, "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models", "author_names": ["Leonardo Ranaldi", "Elena Sofia Ruzzetti", "D. Venditti", "Dario Onorati", "F. M. Zanzotto"], "venue": "STARSEM", "abstract": "Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training are emerging as the next big revolution in natural language processing and understanding. These CtB-LLMs are democratizing access to trainable Very Large-Language Models (VLLMs) and, thus, may represent the building blocks of many NLP systems solving downstream tasks. Hence, a little or a large bias in CtB-LLMs may cause huge harm. In this paper, we performed a large investigation of the bias of three families of CtB-LLMs, and we showed that debiasing techniques are effective and usable. Indeed, according to current tests, the LLaMA and the OPT families have an important bias in gender, race, religion, and profession. In contrast to the analysis for other LMMs, we discovered that bias depends not on the number of parameters but on the perplexity. Finally, the debiasing of OPT using LORA reduces bias up to 4.12 points in the normalized stereotype score.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.18653/v1/2024.starsem-1.30"}, "doi_lower": "10.18653/v1/2024.starsem-1.30"}
{"paper_id": 215786522, "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection", "author_names": ["Shauli Ravfogel", "Yanai Elazar", "Hila Gonen", "Michael Twiton", "Yoav Goldberg"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.", "year": 2020, "publicationdate": "2020-04-16", "externalids": {"DOI": "10.18653/v1/2020.acl-main.647"}, "doi_lower": "10.18653/v1/2020.acl-main.647"}
{"paper_id": 233423485, "title": "Societal Biases in Retrieved Contents: Measurement Framework and Adversarial Mitigation of BERT Rankers", "author_names": ["Navid Rekabsaz", "Simone Kopeinik", "M. Schedl"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Societal biases resonate in the retrieved contents of information retrieval (IR) systems, resulting in reinforcing existing stereotypes. Approaching this issue requires established measures of fairness in respect to the representation of various social groups in retrieval results, as well as methods to mitigate such biases, particularly in the light of the advances in deep ranking models. In this work, we first provide a novel framework to measure the fairness in the retrieved text contents of ranking models. Introducing a ranker-agnostic measurement, the framework also enables the disentanglement of the effect on fairness of collection from that of rankers. To mitigate these biases, we propose AdvBert, a ranking model achieved by adapting adversarial bias mitigation for IR, which jointly learns to predict relevance and remove protected attributes. We conduct experiments on two passage retrieval collections (MSMARCO Passage Re-ranking and TREC Deep Learning 2019 Passage Re-ranking), which we extend by fairness annotations of a selected subset of queries regarding gender attributes. Our results on the MSMARCO benchmark show that, (1) all ranking models are less fair in comparison with ranker-agnostic baselines, and (2) the fairness of Bert rankers significantly improves when using the proposed AdvBert models. Lastly, we investigate the trade-off between fairness and utility, showing that we can maintain the significant improvements in fairness without any significant loss in utility.", "year": 2021, "publicationdate": "2021-04-28", "externalids": {"DOI": "10.1145/3404835.3462949"}, "doi_lower": "10.1145/3404835.3462949"}
{"paper_id": 218470518, "title": "Do Neural Ranking Models Intensify Gender Bias?", "author_names": ["Navid Rekabsaz", "M. Schedl"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Concerns regarding the footprint of societal biases in information retrieval (IR) systems have been raised in several previous studies. In this work, we examine various recent IR models from the perspective of the degree of gender bias in their retrieval results. To this end, we first provide a bias measurement framework which includes two metrics to quantify the degree of the unbalanced presence of gender-related concepts in a given IR model's ranking list. To examine IR models by means of the framework, we create a dataset of non-gendered queries, selected by human annotators. Applying these queries to the MS MARCO Passage retrieval collection, we then measure the gender bias of a BM25 model and several recent neural ranking models. The results show that while all models are strongly biased toward male, the neural models, and in particular the ones based on contextualized embedding models, significantly intensify gender bias. Our experiments also show an overall increase in the gender bias of neural models when they exploit transfer learning, namely when they use (already biased) pre-trained embeddings.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.1145/3397271.3401280"}, "doi_lower": "10.1145/3397271.3401280"}
{"paper_id": 13756572, "title": "Gender Bias in Coreference Resolution", "author_names": ["Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these ‚ÄúWinogender schemas,‚Äù we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.", "year": 2018, "publicationdate": "2018-04-25", "externalids": {"DOI": "10.18653/v1/N18-2002"}, "doi_lower": "10.18653/v1/n18-2002"}
{"paper_id": 218628872, "title": "Masked Language Model Scoring", "author_names": ["Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "K. Kirchhoff"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model‚Äôs WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL‚Äôs unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.", "year": 2019, "publicationdate": "2019-10-31", "externalids": {"DOI": "10.18653/v1/2020.acl-main.240"}, "doi_lower": "10.18653/v1/2020.acl-main.240"}
{"paper_id": 218665313, "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning", "author_names": ["Victor Sanh", "Thomas Wolf", "Alexander M. Rush"], "venue": "Neural Information Processing Systems", "abstract": "Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.", "year": 2020, "publicationdate": "2020-05-15", "externalids": {}, "doi_lower": null}
{"paper_id": 196211238, "title": "The Risk of Racial Bias in Hate Speech Detection", "author_names": ["Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We investigate how annotators‚Äô insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet‚Äôs dialect they are significantly less likely to label the tweet as offensive.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1163"}, "doi_lower": "10.18653/v1/p19-1163"}
{"paper_id": 233240748, "title": "First the Worst: Finding Better Gender Translations During Beam Search", "author_names": ["D. Saunders", "Rosie Sallis", "B. Byrne"], "venue": "Findings", "abstract": "Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach‚Äôs utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.", "year": 2021, "publicationdate": "2021-04-15", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.301"}, "doi_lower": "10.18653/v1/2022.findings-acl.301"}
{"paper_id": 227276498, "title": "Intra-Processing Methods for Debiasing Neural Networks", "author_names": ["Yash Savani", "Colin White", "G. NaveenSundar"], "venue": "Neural Information Processing Systems", "abstract": "As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then fine-tune to a specific use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms specifically for this fine-tuning use-case has largely been neglected. In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning. All of our techniques can be used for all popular group fairness measures such as equalized odds or statistical parity difference. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at https://github.com/abacusai/intraprocessing_debiasing.", "year": 2020, "publicationdate": "2020-06-15", "externalids": {}, "doi_lower": null}
{"paper_id": 232075876, "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP", "author_names": ["Timo Schick", "Sahana Udupa", "Hinrich Sch√ºtze"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract ‚ö† This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model‚Äôs parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1", "year": 2021, "publicationdate": "2021-02-28", "externalids": {"DOI": "10.1162/tacl_a_00434"}, "doi_lower": "10.1162/tacl_a_00434"}
{"paper_id": 246824056, "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do", "author_names": ["P. Schramowski", "Cigdem Turan", "Nico Andersen", "C. Rothkopf", "K. Kersting"], "venue": "Nature Machine Intelligence", "abstract": "Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many natural language processing tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show here that recent LMs also contain human-like biases of what is right and wrong to do, reflecting existing ethical and moral norms of society. We show that these norms can be captured geometrically by a ‚Äòmoral direction‚Äô which can be computed, for example, by a PCA, in the embedding space. The computed ‚Äòmoral direction‚Äô can rate the normativity (or non-normativity) of arbitrary phrases without explicitly training the LM for this task, reflecting social norms well. We demonstrate that computing the ‚Äômoral direction‚Äô can provide a path for attenuating or even preventing toxic degeneration in LMs, showcasing this capability on the RealToxicityPrompts testbed. Large language models identify patterns in the relations between words and capture their relations in an embedding space. Schramowski and colleagues show that a direction in this space can be identified that separates ‚Äòright‚Äô and ‚Äòwrong‚Äô actions as judged by human survey participants.", "year": 2021, "publicationdate": "2021-03-08", "externalids": {"DOI": "10.1038/s42256-022-00458-8"}, "doi_lower": "10.1038/s42256-022-00458-8"}
{"paper_id": 252968208, "title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks", "author_names": ["Nikil Selvam", "Sunipa Dev", "Daniel Khashabi", "Tushar Khot", "Kai-Wei Chang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias. We hope these troubling observations motivate more robust measures of social biases.", "year": 2022, "publicationdate": "2022-10-18", "externalids": {"DOI": "10.48550/arXiv.2210.10040"}, "doi_lower": "10.48550/arxiv.2210.10040"}
{"paper_id": 209461005, "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview", "author_names": ["Deven Santosh Shah", "H. A. Schwartz", "Dirk Hovy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.", "year": 2019, "publicationdate": "2019-11-09", "externalids": {"DOI": "10.18653/v1/2020.acl-main.468"}, "doi_lower": "10.18653/v1/2020.acl-main.468"}
{"paper_id": 253840076, "title": "Does Representational Fairness Imply Empirical Fairness?", "author_names": ["Aili Shen", "Xudong Han", "Trevor Cohn", "Timothy Baldwin", "Lea Frermann"], "venue": "AACL/IJCNLP", "abstract": "NLP technologies can cause unintended harms if learned representations encode sensitive attributes of the author, or predictions systematically vary in quality across groups. Popular debiasing approaches, like adversarial training, remove sensitive information from representations in order to reduce disparate performance, however the relation between representational fairness and empirical (performance) fairness has not been systematically studied. This paper fills this gap, and proposes a novel debiasing method building on contrastive learning to encourage a latent space that separates instances based on target label, while mixing instances that share protected attributes. Our results show the effectiveness of our new method and, more importantly, show across a set of diverse de-biasing methods that representational fairness does not imply empirical fairness . This work highlights the importance of aligning and understanding the relation of the optimization objec-tive and final fairness target. Our code is available at: https://github.com/AiliAili/ contrastive_learning_repo .", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.findings-aacl.8"}, "doi_lower": "10.18653/v1/2022.findings-aacl.8"}
{"paper_id": 218470535, "title": "Towards Controllable Biases in Language Generation", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "Findings", "abstract": "We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios: 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former scenario enables us to detect the types of biases present in the model. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second scenario is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.291"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.291"}
{"paper_id": 235097342, "title": "‚ÄúNice Try, Kiddo‚Äù: Investigating Ad Hominems in Dialogue Responses", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Ad hominem attacks are those that target some feature of a person‚Äôs character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person‚Äôs credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.60"}, "doi_lower": "10.18653/v1/2021.naacl-main.60"}
{"paper_id": 234337004, "title": "Societal Biases in Language Generation: Progress and Challenges", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.", "year": 2021, "publicationdate": "2021-05-10", "externalids": {"DOI": "10.18653/v1/2021.acl-long.330"}, "doi_lower": "10.18653/v1/2021.acl-long.330"}
{"paper_id": 202537041, "title": "The Woman Worked as a Babysitter: On Biases in Language Generation", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.", "year": 2019, "publicationdate": "2019-09-03", "externalids": {"DOI": "10.18653/v1/D19-1339"}, "doi_lower": "10.18653/v1/d19-1339"}
{"paper_id": 251371589, "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage", "author_names": ["Kurt Shuster", "Jing Xu", "M. Komeili", "Da Ju", "Eric Michael Smith", "Stephen Roller", "Megan Ung", "Moya Chen", "Kushal Arora", "Joshua Lane", "Morteza Behrooz", "W.K.F. Ngan", "Spencer Poff", "Naman Goyal", "Arthur Szlam", "Y-Lan Boureau", "M. Kambadur", "J. Weston"], "venue": "arXiv.org", "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {"DOI": "10.48550/arXiv.2208.03188"}, "doi_lower": "10.48550/arxiv.2208.03188"}
{"paper_id": 259370533, "title": "Learning to Generate Equitable Text in Dialogue from Biased Training Data", "author_names": ["Anthony B. Sicilia", "Malihe Alikhani"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The ingrained principles of fairness in a dialogue system‚Äôs decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.48550/arXiv.2307.04303"}, "doi_lower": "10.48550/arxiv.2307.04303"}
{"paper_id": 235097394, "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers", "author_names": ["Andrew Silva", "Pradyumna Tambwekar", "M. Gombolay"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.189"}, "doi_lower": "10.18653/v1/2021.naacl-main.189"}
{"paper_id": 253224433, "title": "‚ÄúI‚Äôm sorry to hear that‚Äù: Finding New Biases in Language Models with a Holistic Descriptor Dataset", "author_names": ["Eric Michael Smith", "Melissa Hall", "M. Kambadur", "Eleonora Presani", "Adina Williams"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.", "year": 2022, "publicationdate": "2022-05-18", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.625"}, "doi_lower": "10.18653/v1/2022.emnlp-main.625"}
{"paper_id": 235489789, "title": "Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets", "author_names": ["Irene Solaiman", "Christy Dennison"], "venue": "Neural Information Processing Systems", "abstract": "Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.", "year": 2021, "publicationdate": "2021-06-18", "externalids": {}, "doi_lower": null}
{"paper_id": 6844431, "title": "Dropout: a simple way to prevent neural networks from overfitting", "author_names": ["Nitish Srivastava", "Geoffrey E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of machine learning research", "abstract": null, "year": 2014, "publicationdate": null, "externalids": {"DOI": "10.5555/2627435.2670313"}, "doi_lower": "10.5555/2627435.2670313"}
{"paper_id": 248780439, "title": "Upstream Mitigation Is \n Not\n All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models", "author_names": ["Ryan Steed", "Swetasudha Panda", "Ari Kobren", "Michael L. Wick"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "A few large, homogenous, pre-trained models undergird many machine learning systems ‚Äî and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier‚Äôs discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.acl-long.247"}, "doi_lower": "10.18653/v1/2022.acl-long.247"}
{"paper_id": 258947011, "title": "MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions", "author_names": ["Hao Sun", "Zhexin Zhang", "Fei Mi", "Yasheng Wang", "W. Liu", "Jianwei Cui", "Bin Wang", "Qun Liu", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users‚Äô values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.18653/v1/2023.acl-long.123"}, "doi_lower": "10.18653/v1/2023.acl-long.123"}
{"paper_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "author_names": ["Mingjie Sun", "Zhuang Liu", "Anna Bair", "J. Z. Kolter"], "venue": "International Conference on Learning Representations", "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.", "year": 2023, "publicationdate": "2023-06-20", "externalids": {"DOI": "10.48550/arXiv.2306.11695"}, "doi_lower": "10.48550/arxiv.2306.11695"}
{"paper_id": 231924754, "title": "They, Them, Theirs: Rewriting with Gender-Neutral English", "author_names": ["Tony Sun", "Kellie Webster", "Apurva Shah", "William Yang Wang", "Melvin Johnson"], "venue": "arXiv.org", "abstract": "Responsible development of technology involves applications being inclusive of the diverse set of users they hope to support. An important part of this is understanding the many ways to refer to a person and being able to fluently change between the different forms as needed. We perform a case study on the singular they, a common way to promote gender inclusion in English. We define a re-writing task, create an evaluation benchmark, and show how a model can be trained to produce gender-neutral English with<1% word error rate with no human-labeled data. We discuss the practical applications and ethical considerations of the task, providing direction for future work into inclusive natural language systems.", "year": 2021, "publicationdate": "2021-02-12", "externalids": {}, "doi_lower": null}
{"paper_id": 235436386, "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle", "author_names": ["Harini Suresh", "J. Guttag"], "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "abstract": "As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.", "year": 2019, "publicationdate": "2019-01-28", "externalids": {"DOI": "10.1145/3465416.3483305"}, "doi_lower": "10.1145/3465416.3483305"}
{"paper_id": 202781363, "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations", "author_names": ["Y. Tan", "Elisa Celis"], "venue": "Neural Information Processing Systems", "abstract": "Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.", "year": 2019, "publicationdate": "2019-11-04", "externalids": {}, "doi_lower": null}
{"paper_id": 259095603, "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "author_names": ["Himanshu Thakur", "Atishay Jain", "Praneetha Vaddamanu", "Paul Pu Liang", "Louis-philippe Morency"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04597"}, "doi_lower": "10.48550/arxiv.2306.04597"}
{"paper_id": 246210255, "title": "Text Style Transfer for Bias Mitigation using Masked Language Modeling", "author_names": ["E. Tokpo", "T. Calders"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.", "year": 2022, "publicationdate": "2022-01-21", "externalids": {"DOI": "10.18653/v1/2022.naacl-srw.21"}, "doi_lower": "10.18653/v1/2022.naacl-srw.21"}
{"paper_id": 238856730, "title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "author_names": ["Megan Ung", "Jing Xu", "Y-Lan Boureau"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Current open-domain conversational models can easily be made to talk in inadequate ways. Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures. However, current state-of-the-art models tend to react to feedback with defensive or oblivious responses. This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future. This work proposes SaFeRDialogues, a task and dataset of graceful responses to conversational feedback about safety failures.We collect a dataset of 8k dialogues demonstrating safety failures, feedback signaling them, and a response acknowledging the feedback. We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation, without sacrificing engagingness or general conversational ability.", "year": 2021, "publicationdate": "2021-10-14", "externalids": {"DOI": "10.18653/v1/2022.acl-long.447"}, "doi_lower": "10.18653/v1/2022.acl-long.447"}
{"paper_id": 221949175, "title": "Towards Debiasing NLU Models from Unknown Biases", "author_names": ["Prasetya Ajie Utama", "N. Moosavi", "Iryna Gurevych"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.", "year": 2020, "publicationdate": "2020-09-25", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.613"}, "doi_lower": "10.18653/v1/2020.emnlp-main.613"}
{"paper_id": 237494677, "title": "NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives", "author_names": ["Eva Vanmassenhove", "Chris Emmery", "D. Shterionov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18% on synthetic, in-domain and out-domain test sets.", "year": 2021, "publicationdate": "2021-09-13", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.704"}, "doi_lower": "10.18653/v1/2021.emnlp-main.704"}
{"paper_id": 250390459, "title": "HeteroCorpus: A Corpus for Heteronormative Language Detection", "author_names": ["Juan V√°squez", "Gemma Bel-Enguix", "Scott Andersen", "Sergio-Luis Ojeda-Trueba"], "venue": "GEBNLP", "abstract": "In recent years, plenty of work has been done by the NLP community regarding gender bias detection and mitigation in language systems. Yet, to our knowledge, no one has focused on the difficult task of heteronormative language detection and mitigation. We consider this an urgent issue, since language technologies are growing increasingly present in the world and, as it has been proven by various studies, NLP systems with biases can create real-life adverse consequences for women, gender minorities and racial minorities and queer people. For these reasons, we propose and evaluate HeteroCorpus; a corpus created specifically for studying heterononormative language in English. Additionally, we propose a baseline set of classification experiments on our corpus, in order to show the performance of our corpus in classification tasks.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.gebnlp-1.23"}, "doi_lower": "10.18653/v1/2022.gebnlp-1.23"}
{"paper_id": 49561627, "title": "Fairness Definitions Explained", "author_names": ["Sahil Verma", "J. Rubin"], "venue": "2018 IEEE/ACM International Workshop on Software Fairness (FairWare)", "abstract": "Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.", "year": 2018, "publicationdate": "2018-05-29", "externalids": {"DOI": "10.1145/3194770.3194776"}, "doi_lower": "10.1145/3194770.3194776"}
{"paper_id": 149962887, "title": "Indigenous data, indigenous methodologies and indigenous data sovereignty", "author_names": ["M. Walter", "Michele Suina"], "venue": "International Journal of Social Research Methodology", "abstract": "ABSTRACT The field of Indigenous methodologies has grown strongly since Tuhiwai Smith‚Äôs 1999 groundbreaking book Decolonizing Indigenous Methodologies. For the most part however, there has been a marked absence of quantitative methodologies with the methods aligned with Indigenous methodologies predominantly qualitative. This article proposes that the absence of an Indigenous presence from Indigenous data production has resulted in an overwhelming statistical narrative of deficit for dispossessed Indigenous peoples around the globe. Using the theoretical concept of Indigenous Lifeworlds this article builds on the core premises of Walter and Andersen‚Äôs 2013 book Indigenous quantitative methodologies. Arguing for a fundamental disturbance of the Western logics of statistical data the article details recent developments in the field including the emergence of the Indigenous Data Sovereignty movement. The article also explores Indigenous quantitative methodologies in practice using the case study of a Tribal Epidemiology Centre in New Mexico.", "year": 2018, "publicationdate": "2018-10-08", "externalids": {"DOI": "10.1080/13645579.2018.1531228"}, "doi_lower": "10.1080/13645579.2018.1531228"}
{"paper_id": 60441316, "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model", "author_names": ["Alex Wang", "Kyunghyun Cho"], "venue": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation", "abstract": "We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.", "year": 2019, "publicationdate": "2019-02-11", "externalids": {"DOI": "10.18653/v1/W19-2304"}, "doi_lower": "10.18653/v1/w19-2304"}
{"paper_id": 235097505, "title": "Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack", "author_names": ["Liwen Wang", "Yuanmeng Yan", "Keqing He", "Yanan Wu", "Weiran Xu"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.293"}, "doi_lower": "10.18653/v1/2021.naacl-main.293"}
{"paper_id": 257220117, "title": "Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling", "author_names": ["Rui Wang", "Pengyu Cheng", "Ricardo Henao"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "Pretrained language models (PLMs), such as GPT2, have achieved remarkable empirical performance in text generation tasks. However, pretrained on large-scale natural language corpora, the generated text from PLMs may exhibit social bias against disadvantaged demographic groups. To improve the fairness of PLMs in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. In this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. Moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. We also propose a distillation mechanism that preserves the language modeling ability of the PLMs after debiasing. Empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability.", "year": 2023, "publicationdate": "2023-02-25", "externalids": {"DOI": "10.48550/arXiv.2302.13136"}, "doi_lower": "10.48550/arxiv.2302.13136"}
{"paper_id": 254877589, "title": "Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite", "author_names": ["Xun Wang", "Tao Ge", "Allen Mao", "Yuki Li", "Furu Wei", "Si-Qing Chen"], "venue": "arXiv.org", "abstract": "We introduce \\textsc{PoliteRewrite} -- a dataset for polite language rewrite which is a novel sentence rewrite task. Compared with previous text style transfer tasks that can be mostly addressed by slight token- or phrase-level edits, polite language rewrite requires deep understanding and extensive sentence-level edits over an offensive and impolite sentence to deliver the same message euphemistically and politely, which is more challenging -- not only for NLP models but also for human annotators to rewrite with effort. To alleviate the human effort for efficient annotation, we first propose a novel annotation paradigm by a collaboration of human annotators and GPT-3.5 to annotate \\textsc{PoliteRewrite}. The released dataset has 10K polite sentence rewrites annotated collaboratively by GPT-3.5 and human, which can be used as gold standard for training, validation and test; and 100K high-quality polite sentence rewrites by GPT-3.5 without human review. We wish this work (The dataset (10K+100K) will be released soon) could contribute to the research on more challenging sentence rewrite, and provoke more thought in future on resource annotation paradigm with the help of the large-scaled pretrained models.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10190"}, "doi_lower": "10.48550/arxiv.2212.10190"}
{"paper_id": 256827116, "title": "Counter-GAP: Counterfactual Bias Evaluation through Gendered Ambiguous Pronouns", "author_names": ["Zhongbin Xie", "Vid Kocijan", "Thomas Lukasiewicz", "Oana-Maria Camburu"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. In this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. To overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct Counter-GAP, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. We further identify a bias cancellation problem in previous group-level metrics on Counter-GAP, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. Our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method.", "year": 2023, "publicationdate": "2023-02-11", "externalids": {"DOI": "10.48550/arXiv.2302.05674"}, "doi_lower": "10.48550/arxiv.2302.05674"}
{"paper_id": 222310622, "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models", "author_names": ["Kellie Webster", "Xuezhi Wang", "Ian Tenney", "Alex Beutel", "Emily Pitler", "Ellie Pavlick", "Jilin Chen", "Slav Petrov"], "venue": "arXiv.org", "abstract": "Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.", "year": 2020, "publicationdate": "2020-10-12", "externalids": {}, "doi_lower": null}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 249872629, "title": "Taxonomy of Risks posed by Language Models", "author_names": ["Laura Weidinger", "Jonathan Uesato", "Maribeth Rauh", "Conor Griffin", "Po-Sen Huang", "John F. J. Mellor", "Amelia Glaese", "Myra Cheng", "Borja Balle", "Atoosa Kasirzadeh", "Courtney Biles", "S. Brown", "Zachary Kenton", "W. Hawkins", "T. Stepleton", "Abeba Birhane", "Lisa Anne Hendricks", "Laura Rimell", "William S. Isaac", "Julia Haas", "Sean Legassick", "G. Irving", "Iason Gabriel"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.", "year": 2022, "publicationdate": "2022-06-20", "externalids": {"DOI": "10.1145/3531146.3533088"}, "doi_lower": "10.1145/3531146.3533088"}
{"paper_id": 258537309, "title": "Compensatory Debiasing For Gender Imbalances In Language Models", "author_names": ["Tae-Jin Woo", "Woo-Jeoung Nam", "Yeong-Joon Ju", "Seong-Whan Lee"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "abstract": "Pre-trained language models (PLMs) learn gender bias from imbalances in human-written corpora. This bias leads to critical social issues when deploying PLMs in real-world scenarios. However, minimizing bias is limited by the trade-off due to the degradation of language modeling performance. It is particularly challenging to detach and remove biased representations in the embedding space because the learned linguistic knowledge entails bias. To address this problem, we propose a compensatory debiasing strategy to reduce gender bias while preserving linguistic knowledge. This strategy utilizes two types of sentences to distinguish biased knowledge: stereotype and non-stereotype sentences. We assign small angles and distances to pairs of representations of the two gender groups to mitigate bias for the stereotype sentences. At the same time, we maximize the agreement for the representations of the debiasing model and the original model to maintain linguistic knowledge for the non-stereotype sentences. To validate our approach, we measure the performance of the debiased model using the following evaluation metrics: SEAT, StereoSet, CrowS-Pairs, and GLUE. Our experimental results demonstrate that the model fine-tuned by our strategy has the lowest level of bias while retaining knowledge of PLMs.", "year": 2023, "publicationdate": "2023-06-04", "externalids": {"DOI": "10.1109/ICASSP49357.2023.10095658"}, "doi_lower": "10.1109/icassp49357.2023.10095658"}
{"paper_id": 222341902, "title": "Recipes for Safety in Open-domain Chatbots", "author_names": ["Jing Xu", "Da Ju", "Margaret Li", "Y-Lan Boureau", "J. Weston", "Emily Dinan"], "venue": "arXiv.org", "abstract": "Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.", "year": 2020, "publicationdate": "2020-10-14", "externalids": {}, "doi_lower": null}
{"paper_id": 253446867, "title": "ADEPT: A DEbiasing PrompT Framework", "author_names": ["Ke Yang", "Charles Yu", "Y. Fung", "Manling Li", "Heng Ji"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects.", "year": 2022, "publicationdate": "2022-11-10", "externalids": {"DOI": "10.48550/arXiv.2211.05414"}, "doi_lower": "10.48550/arxiv.2211.05414"}
{"paper_id": 259859034, "title": "Unlearning Bias in Language Models by Partitioning Gradients", "author_names": ["Charles Yu", "Sullam Jeoung", "Anish Kasi", "Pengfei Yu", "Heng Ji"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": ",", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.findings-acl.375"}, "doi_lower": "10.18653/v1/2023.findings-acl.375"}
{"paper_id": 259949745, "title": "Mixup-based Unified Framework to Overcome Gender Bias Resurgence", "author_names": ["Liu Yu", "Yuzhou Mao", "Jin Wu", "Fan Zhou"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Unwanted social biases are usually encoded in pretrained language models (PLMs). Recent efforts are devoted to mitigating intrinsic bias encoded in PLMs. However, the separate fine-tuning on applications is detrimental to intrinsic debiasing. A bias resurgence issue arises when fine-tuning the debiased PLMs on downstream tasks. To eliminate undesired stereotyped associations in PLMs during fine-tuning, we present a mixup-based framework Mix-Debias from a new unified perspective, which directly combines debiasing PLMs with fine-tuning applications. The key to Mix-Debias is applying mixup-based linear interpolation on counterfactually augmented downstream datasets, with expanded pairs from external corpora. Besides, we devised an alignment regularizer to ensure original augmented pairs and gender-balanced counterparts are spatially closer. Experimental results show that Mix-Debias can reduce biases in PLMs while maintaining a promising performance in applications.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {"DOI": "10.1145/3539618.3591938"}, "doi_lower": "10.1145/3539618.3591938"}
{"paper_id": 258832694, "title": "Should We Attend More or Less? Modulating Attention for Fairness", "author_names": ["A. Zayed", "Gon√ßalo Mordido", "S. Shabanian", "Sarath Chandar"], "venue": "arXiv.org", "abstract": "The advances in natural language processing (NLP) pose both opportunities and challenges. While recent progress enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. WARNING: This work uses language that is offensive.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.13088"}, "doi_lower": "10.48550/arxiv.2305.13088"}
{"paper_id": 253734850, "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness", "author_names": ["A. Zayed", "Prasanna Parthasarathi", "Gon√ßalo Mordido", "Hamid Palangi", "S. Shabanian", "Sarath Chandar"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model‚Äôs fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature.", "year": 2022, "publicationdate": "2022-11-20", "externalids": {"DOI": "10.48550/arXiv.2211.11109"}, "doi_lower": "10.48550/arxiv.2211.11109"}
{"paper_id": 9424845, "title": "Mitigating Unwanted Biases with Adversarial Learning", "author_names": ["B. Zhang", "Blake Lemoine", "Margaret Mitchell"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.", "year": 2018, "publicationdate": "2018-01-22", "externalids": {"DOI": "10.1145/3278721.3278779"}, "doi_lower": "10.1145/3278721.3278779"}
{"paper_id": 102352962, "title": "Gender Bias in Contextualized Word Embeddings", "author_names": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Ryan Cotterell", "Vicente Ordonez", "Kai-Wei Chang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo‚Äôs contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.", "year": 2019, "publicationdate": "2019-04-01", "externalids": {"DOI": "10.18653/v1/N19-1064"}, "doi_lower": "10.18653/v1/n19-1064"}
{"paper_id": 1389483, "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints", "author_names": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively„ÄÇ", "year": 2017, "publicationdate": "2017-07-29", "externalids": {"DOI": "10.18653/v1/D17-1323"}, "doi_lower": "10.18653/v1/d17-1323"}
{"paper_id": 4952494, "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods", "author_names": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.", "year": 2018, "publicationdate": "2018-04-18", "externalids": {"DOI": "10.18653/v1/N18-2003"}, "doi_lower": "10.18653/v1/n18-2003"}
{"paper_id": 231979430, "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "author_names": ["Tony Zhao", "Eric Wallace", "Shi Feng", "D. Klein", "Sameer Singh"], "venue": "International Conference on Machine Learning", "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.", "year": 2021, "publicationdate": "2021-02-19", "externalids": {}, "doi_lower": null}
{"paper_id": 259088837, "title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning", "author_names": ["Chujie Zheng", "Pei Ke", "Zheng Zhang", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Click outperforms strong baselines of controllable text generation and demonstrate the superiority of Click's sample construction strategy.", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03350"}, "doi_lower": "10.48550/arxiv.2306.03350"}
{"paper_id": 259370743, "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning", "author_names": ["Fan Zhou", "Yuzhou Mao", "Liu Yu", "Yi Yang", "Ting Zhong"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.232"}, "doi_lower": "10.18653/v1/2023.acl-long.232"}
{"paper_id": 247863188, "title": "VALUE: Understanding Dialect Disparity in NLU", "author_names": ["Caleb Ziems", "Jiaao Chen", "Camille Harris", "J. Anderson", "Diyi Yang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE. However, these benchmarks contain only textbook Standard American English (SAE). Other dialects have been largely overlooked in the NLP community. This leads to biased and inequitable NLU systems that serve only a sub-population of speakers. To understand disparities in current models and to facilitate more dialect-competent NLU systems, we introduce the VernAcular Language Understanding Evaluation (VALUE) benchmark, a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules. In this initial release (V.1), we construct rules for 11 features of African American Vernacular English (AAVE), and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner. Experiments show that these new dialectal features can lead to a drop in model performance.", "year": 2022, "publicationdate": "2022-04-06", "externalids": {"DOI": "10.48550/arXiv.2204.03031"}, "doi_lower": "10.48550/arxiv.2204.03031"}
{"paper_id": 184486914, "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "author_names": ["Ran Zmigrod", "Sabrina J. Mielke", "Hanna M. Wallach", "Ryan Cotterell"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Gender stereotypes are manifest in most of the world‚Äôs languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.", "year": 2019, "publicationdate": "2019-06-11", "externalids": {"DOI": "10.18653/v1/P19-1161"}, "doi_lower": "10.18653/v1/p19-1161"}
