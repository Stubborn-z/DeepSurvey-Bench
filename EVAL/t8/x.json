{
  "survey": "This survey paper provides a comprehensive examination of bias and fairness in large language models (LLMs), emphasizing the critical need for ongoing research and development to create fair and unbiased AI systems. It highlights the pervasive biases inherent in LLMs, particularly focusing on gender and cultural biases, and explores methodologies for their mitigation. The survey systematically addresses the evolution of gender stereotypes and attitudes towards ethnic minorities, especially within the United States, over the 20th and 21st centuries. It evaluates documentation and content quality in large text corpora, emphasizing dataset provenance and the effects of filtering on content diversity. Key mitigation strategies include data-centric techniques, model architecture adjustments, and algorithmic interventions, such as the Gender Equality Prompt (GEEP) and adversarial triggering, which effectively reduce biases without compromising model performance. The paper underscores the importance of interdisciplinary approaches and ethical frameworks in tackling bias, advocating for the integration of cultural and contextual nuances in bias mitigation strategies. It also emphasizes the expansion of benchmarks and evaluation frameworks to capture the nuanced dimensions of bias across diverse demographic factors and contexts. These findings highlight the necessity for sustainable practices in NLP research, focusing on the environmental and financial costs associated with AI development. By advancing interdisciplinary collaboration and promoting ongoing research, the survey aims to contribute to the development of ethical AI systems that align with societal standards and promote inclusivity across diverse applications.\n\nIntroduction Importance of Bias and Fairness in Large Language Models Bias and fairness are paramount in the development and deployment of large language models, given their extensive influence on applications such as conversational agents and decision-making systems [1]. Gender bias in natural language datasets presents significant challenges, as neural language models often inherit these biases, leading to biased text generation [2]. This situation underscores the urgent need for effective mitigation strategies to ensure equitable outcomes. Additionally, catastrophic forgetting during pre-training on small gender-neutral datasets can adversely affect model performance on downstream tasks, highlighting the necessity of incorporating fairness considerations in training processes [3]. The societal ramifications of biases in AI-generated text are profound, as they can perpetuate stereotypes and exacerbate existing social inequalities. The growing demand for gender-neutral and inclusive language in NLP applications necessitates benchmarks that assess gender-neutral rewriting methods, ensuring language models adhere to ethical standards and foster inclusivity [4]. Furthermore, addressing dialect disparities in Natural Language Understanding (NLU) systems is essential for developing models that are representative and equitable across linguistic groups [5]. In high-stakes domains such as education, criminology, finance, and healthcare, ensuring fairness in large language models is crucial to prevent the reinforcement of societal biases. This is vital for fostering public trust and developing AI systems that serve as inclusive and equitable tools. Addressing these challenges requires standardized evaluation methods and model selection processes to enhance fairness and trustworthiness in natural language generation, translation, and comprehension tasks. By focusing on both group and individual fairness metrics and understanding the trade-offs between fairness and accuracy, researchers aim to create AI systems that serve diverse and inclusive purposes across critical fields. This approach clarifies the relationship between debiasing algorithms and fairness theory, motivating further research to address existing biases in AI applications and promote responsible AI deployment [6,7,8,9]. Scope and Objectives of the Survey This survey systematically investigates the pervasive biases in large language models, focusing particularly on measuring and mitigating gender bias, an area inadequately addressed by previous methods [2]. It explores the evolution of gender stereotypes and attitudes towards ethnic minorities in the United States throughout the 20th and 21st centuries. A significant aspect of this survey is the evaluation of documentation and content quality in large text corpora derived from web scraping, emphasizing dataset provenance and the impact of filtering on content diversity. A key objective is to enhance the tuning of language models for specific downstream tasks through innovative strategies such as prompt tuning, thereby improving efficiency and task-specific performance. The survey also addresses the reliance of Natural Language Understanding (NLU) models on latent biases, which can hinder performance in tasks demanding genuine understanding. Furthermore, it introduces the Gender Equality Prompt (GEEP) method, which involves freezing the pre-trained model and learning gender-related prompts using gender-neutral data to enhance gender fairness without significant forgetting [3]. Safety concerns in large neural-based conversational models are another focal point, with the survey proposing a retrieval-based approach to generate safer outputs. It highlights the social biases present in sentence-level representations used in NLP, which can perpetuate stereotypes in real-world applications. The survey aims to accurately measure and identify implicit biases in contextualized word embeddings generated by neural language models like BERT, employing advanced techniques that surpass traditional cosine-based methods. Additionally, it explores bias quantification and mitigation strategies, including a case study on gender pronoun resolution, emphasizing the generalizability of these techniques to uncover various biases, such as racial and religious, in multiclass settings [10,11]. Moreover, the survey addresses dialectal variations in NLU, particularly focusing on African American Vernacular English (AAVE), to promote fairness in AI systems [5]. It underscores the importance of ensuring fairness in user interactions within dialogue systems, emphasizing the need to generate equitable text from biased training data. Through these objectives, the survey aspires to contribute to the development of fair and unbiased language models, fostering ethical AI systems that cater to diverse and inclusive purposes. Structure of the Survey The survey is meticulously structured to provide a comprehensive examination of bias and fairness in large language models, organizing current research and methodologies into distinct stages and domains. Initially, it delineates the stages of bias identification and measurement, utilizing tools such as the Implicit Association Test to assess biases within language models [12]. It further explores potential intervention strategies aimed at mitigating identified biases, ensuring equitable outcomes across diverse applications. In alignment with recent research, the survey considers the stages of model development, deployment, and evaluation, integrating environmental impact and ethical implications as pivotal criteria [13]. This holistic approach underscores the importance of responsible AI systems that are both ethically sound and environmentally sustainable. The organization extends into various fields, including language, vision, robotics, and reasoning, systematically addressing model architectures, training processes, and evaluation methodologies [14]. This multi-disciplinary framework facilitates a thorough investigation of biases across different AI domains, promoting cross-domain insights and solutions. Furthermore, the survey introduces benchmarks such as the Gender Neutral Editing Benchmark (GNEB), focusing on singular 'they' rewriting tasks and achieving high accuracy without reliance on human-labeled data [15]. This benchmark exemplifies the innovative methodologies employed to advance fairness in language models. Finally, the survey categorizes current methods and research into various domains and subdomains of AI, examining the manifestation of biases and proposing viable solutions [6]. Through this structured approach, the survey aims to provide a detailed and coherent narrative that aligns with the overarching goals of promoting fairness and mitigating bias in large language models.The following sections are organized as shown in . Background and Definitions Theoretical Frameworks and Definitions The study of bias and fairness in language models is grounded in theoretical frameworks that elucidate complex phenomena. Algorithmic bias, as discussed by [1], can produce outputs that diverge from user intent, complicating bias measurement across contexts. Gender bias, prevalent in language models, exacerbates fairness issues, leading to biased text generation [2]. These biases are particularly troubling in sensitive applications, where they risk reinforcing stereotypes and social inequalities, necessitating gender-neutral rewriting benchmarks [4]. Gender bias also affects datasets used for multilabel object classification and visual semantic role labeling, complicating fairness and leading to biased predictions [2]. Benchmarks like GLUE and SuperGLUE overlook dialectal differences, reducing performance for non-standard dialects and underscoring the need for robust frameworks to mitigate biases [5]. Bias in language models further involves the challenge of integrating knowledge from multiple tasks consistently [16]. Ethical implications of biases in pretrained models are significant, as they can perpetuate stereotypes and lead to unfair outcomes, highlighting the need for comprehensive debiasing methods [2]. These frameworks provide methodologies to enhance inclusivity and equitable outcomes by addressing biases in AI systems. They standardize evaluation and model selection processes, clarify debiasing methods' interrelations, and highlight fairness-accuracy trade-offs. They advocate for a holistic approach to algorithmic unfairness, advancing fair AI systems that ensure responsible decision-making [6,17,9]. Ethical AI and Fairness Ethical AI is intertwined with fairness, transparency, and accountability in language models. Ethical AI aims to prevent harmful stereotypes and biases, a challenge intensified by biased training data [18]. Integrating safety into generative model training and evaluation through human-and-model-in-the-loop frameworks represents a significant advancement in ethical standards [19], enhancing safety and promoting responsible deployment. A major barrier to fairness is the inadequate inclusion of target classes during training, limiting existing methods' effectiveness [8]. Addressing this requires frameworks sensitive to race's historical and social dimensions, fostering a nuanced understanding of algorithmic fairness [20]. The lack of cohesive frameworks for comprehending and mitigating predictive biases in NLP models leads to repetitive approaches, highlighting the need for innovative solutions [1]. The survey by [21] emphasizes the challenges in responsibly measuring language model performance, necessitating standardized evaluation methods. 'Constitutional AI'—training AI with rules to critique and revise outputs—illustrates embedding ethical considerations in AI systems, promoting fairness across applications. The distinction between association bias and empirical fairness underscores bias complexity and the need for independent evaluation [22]. Efforts to contextualize fairness in NLP must respect cultural norms, as shown by frameworks tailored to societal contexts. This cultural sensitivity is vital for inclusive AI systems. Benchmarks like BBQ are critical for identifying and mitigating biases, fostering ethical AI practices [21]. Innovative techniques are essential to minimize non-normative text generation from models like GPT-2 without sacrificing functionality. Approaches like fine-tuning with policy gradient reinforcement learning and a normative text classifier enhance fairness, reducing non-normative text by up to 61 Key Concepts in Bias Evaluation Evaluating bias in language models requires understanding metrics and benchmarks measuring bias dimensions. The complexity of bias evaluation is highlighted by diverse metrics capturing specific fairness aspects. The HolisticBias framework uses nearly 600 descriptor terms across 13 demographic axes, resulting in over 450,000 sentence prompts for nuanced bias assessment [23]. The Counter-GAP benchmark, with 4008 instances in 1002 quadruples, effectively measures gender bias [24], ensuring models handle gender-related language nuances. Metrics like Sentiment Score and Regard Score assess generated text bias concerning demographic groups [25], crucial for understanding language model outputs affecting group perceptions. Toxicity score quantifies harmfulness in generated outputs [26], critical in contexts where toxic language has significant implications. Evaluating social biases in masked language models (MLMs) focuses on intrinsic and extrinsic measures to understand embedded biases [27]. The CrowS-Pairs benchmark uses Bias Favorability and Stereotype Detection to measure model preference for stereotypical sentences [28]. Cosine similarity methods have been criticized for inconsistency in capturing social bias nuances, necessitating robust evaluation frameworks [11]. The Contextualized Embedding Association Test (CEAT) quantifies bias magnitude in neural models, offering a new evaluation framework [29]. Metrics like Group Fairness and Individual Fairness assess output differences across demographic groups [7], considering collective and individual biases. Gender-neutral rewriting task datasets include diverse examples requiring rewriting, facilitating robust evaluation across contexts [4], ensuring unbiased and inclusive outputs. These concepts establish a framework for evaluating bias in language models, emphasizing distinct bias definitions and measures, understanding fairness-accuracy relationships, addressing societal implications of representational biases, and recognizing normative aspects of bias analysis. They aim to improve ethical AI practices and promote equitable outcomes by proposing new benchmarks for identifying biases, integrating theoretical insights to reconcile bias with fairness, and centering efforts on real-life experiences of affected communities [30,31,9,32]. In recent years, the examination of algorithmic bias in language models has gained significant traction within the field of artificial intelligence. Understanding the complexities of this issue requires a thorough analysis of its underlying structures and implications. illustrates the hierarchical structure of algorithmic bias in language models, categorizing various sources, manifestations, and specific biases inherent in transformer-based models. This figure not only highlights key deficiencies in data representation, model architecture, and evaluation methodologies but also underscores the impact of stereotype reliance and neural representations on the manifestation of bias. Furthermore, it showcases innovative approaches aimed at mitigating bias in transformer-based models, thereby emphasizing the necessity for ongoing research and development to ensure fairness and accuracy within AI systems. By integrating these insights, we can better understand the multifaceted nature of algorithmic bias and work towards more equitable AI solutions. Algorithmic Bias in Language Models Sources of Algorithmic Bias Algorithmic bias in language models is primarily rooted in data representation deficiencies, model architecture limitations, and evaluation methodology shortcomings. Training datasets often amplify existing biases, skewing model predictions [33,34]. Current benchmarks inadequately assess group and individual fairness, complicating output evaluations [7]. Debiasing efforts typically focus on word-level biases, neglecting sentence-level biases, which limits pretrained encoder effectiveness [35]. Moreover, integrating human feedback is crucial for aligning outputs with user expectations, yet remains insufficiently addressed [36]. User interactions in models like BlenderBot 3 introduce biases affecting learning and evaluation [19]. Gender-neutral datasets are often narrow and limited, leading to significant information loss and performance decline [3]. Additionally, models frequently forget previously learned tasks when new ones are introduced, impairing performance [16]. Detoxification and debiasing are often treated separately, resulting in models that remain biased or toxic. The CTP-SHAP method employs chain-of-thought prompting and SHAP analysis to evaluate and improve outputs concerning sexual identities, underscoring the need for nuanced methodologies [18]. Addressing these diverse sources requires integrated frameworks considering data representation, model architecture, and sociocultural contexts. Solutions include new benchmarks, metrics, and a predictive bias framework to tackle bias origins rather than symptoms. Standardized evaluation and model selection methods are vital for balancing fairness and accuracy, alongside innovative approaches like regularization loss terms aimed at reducing gender bias [37,9,32,38,39]. illustrates the primary sources of algorithmic bias in language models, categorized into data representation, model architecture, and evaluation methodology. Each category highlights specific issues and references relevant studies addressing these challenges, thereby providing a visual context that complements the preceding discussion. Manifestations of Bias Bias in language models manifests through mechanisms affecting fairness and inclusivity. Stereotype reliance leads to accuracy discrepancies when correct answers align with social biases, as seen in the BBQ benchmark [21]. Historical language evolution tied to gender and ethnic stereotypes is reflected in word embeddings aligned with demographic data [40]. Bias is evident in sentence-level neural representations, where Sent-Debias identifies and mitigates social biases without sacrificing NLP task performance [41]. Multi-class classification tasks reveal biases in training data influencing predictions, resulting in gender-biased text generation [42]. These biases have profound implications in content generation and decision-making systems, perpetuating stereotypes and reinforcing inequalities. Models like GPT-2 show biases against countries with lower internet access, highlighting global ramifications [34]. Innovative mitigation approaches include PowerTransformer, using connotation frames and auxiliary tasks to correct biased language, and chain-of-thought prompting to enhance reasoning accuracy [43]. These methodologies aim to restructure language processing and generation, promoting equitable outputs. Manifestations of bias emphasize the need for robust methodologies to tackle these challenges. By refining debiasing strategies and enhancing output diversity, AI systems can yield fair results across applications, balancing fairness and accuracy and mitigating biases tied to sensitive attributes [6,44,45,9]. Bias in Transformer-Based Models Transformer-based models like BERT, GPT-3, and BART are pivotal in NLP due to their deep bidirectional representations and autoregressive text generation capabilities. Despite their sophistication, these models exhibit significant biases, particularly concerning gender and stereotyping, leading to harmful generalizations about race, religion, and other social constructs. Such biases challenge their deployment in applications where fairness and accuracy are critical, such as healthcare and legal systems. New benchmarks, metrics, and methods aim to reduce bias while preserving text generation integrity. Empirical studies using the Equity Evaluation Corpus reveal bias prevalence in sentiment analysis systems, and research introduces regularization techniques to minimize gender bias [37,46,47,32]. The Movement Pruning Framework mitigates bias by selectively pruning attention heads in transformer models, reducing gender bias without performance compromise [48]. Structural modifications in transformer architectures are crucial for addressing inherent biases. The Plug and Play Language Model (PPLM) enables controllable text generation by integrating pretrained models with attribute classifiers [49], fostering equitable text generation. Counterfactual Role Reversal Knowledge Distillation (CRR-KD) enhances bias mitigation by modifying teacher model outputs and augmenting training data, effectively reducing gender bias [50]. Knowledge distillation techniques refine predictions and improve fairness. Few-shot de-biasing techniques fine-tune models with curated debiased examples, achieving notable gender bias reductions [51]. Few-shot learning addresses bias with minimal data. Evaluations of transformer-based question answering models reveal stereotyping biases, emphasizing the persistent challenge in language understanding tasks [52]. Comprehensive strategies incorporating architectural adjustments and sophisticated training methodologies are necessary. These approaches highlight the need for ongoing research and development to enhance fairness and reduce bias. As AI systems increasingly impact sensitive environments and life-altering decisions, ensuring equitable outcomes across applications is paramount. Addressing the interplay between fairness and accuracy, standardizing evaluation methods, and clarifying bias definitions and quantifications are essential. Examining real-world applications motivates exploring future solutions enhancing fairness in AI systems [6,9]. Ethical Considerations in AI The ethical dimensions of artificial intelligence (AI) are significantly shaped by biases in language models, impacting fairness and accuracy across diverse social groups. These biases influence user experiences and ethical practices in AI, particularly in high-stakes contexts, necessitating a focus on inclusivity and equity. Impact on Social Groups Language model biases undermine fairness and accuracy, affecting social groups in critical domains where biased outputs can have severe consequences [7]. Disparities in content quality adversely impact minority groups, highlighting ethical concerns [35]. Models employing CTP-SHAP improve text generation by accurately representing queer identities, addressing marginalized communities' needs [18]. However, maintaining recognition performance while reducing bias amplification remains challenging [33]. Gender bias complexity necessitates mitigation techniques like posterior regularization to adjust predicted distributions [53]. The UDDIA framework exemplifies a dual approach to detoxification and debiasing, ensuring ethical quality in generated text and addressing biases' impact on social groups [54]. Aligning language models with user intentions is vital given the ethical implications of biased outputs [36]. Innovations like GEEP advance gender fairness while addressing issues like catastrophic forgetting [3]. These developments demonstrate AI systems' potential to sustainably reduce bias and enhance inclusivity. These findings underscore the need for comprehensive frameworks addressing bias implications within language models. Such frameworks should integrate efforts to mitigate predictive and social biases throughout the NLP pipeline. By organizing mitigation techniques, defining representational biases, and developing novel assessment approaches, these frameworks can enhance fairness and performance in real-world applications, preventing harmful stereotypes [37,39,38,32]. Advancing debiasing techniques and ethical AI practices is essential for fostering inclusive AI systems. Social and Cultural Biases Exploring social and cultural biases in language models reveals complex dimensions impacting AI fairness. Cultural biases in dialect representation pose challenges to equitable outputs [5]. Inadequate representation of dialects, such as African American Vernacular English (AAVE), perpetuates social inequalities [5]. Historical language evolution tied to gender and ethnic stereotypes illustrates persistent cultural biases [40]. Biased training data exacerbates cultural biases, leading to skewed predictions [34]. Innovative solutions like the PowerTransformer leverage connotation frames to correct biased language, promoting equitable text generation [42]. Chain of thought prompting enhances reasoning accuracy through structured prompts, offering a promising methodology for reducing bias [43]. Biases in models like GPT-2 highlight global ramifications of biased outputs [34]. These findings emphasize the need for frameworks addressing social and cultural biases in language models. As models are deployed in sensitive areas, recognizing and countering representational biases related to gender, race, and religion is vital. Proposed frameworks should define predictive bias, assess biases at contextual word levels, and emphasize the relationship between language and social hierarchies, aiming to improve fairness while centering on affected communities' experiences [30,39,38,32]. To illustrate these critical aspects, presents a comprehensive overview of the key elements of social and cultural biases in language models. This figure highlights the challenges associated with dialect representation, innovative solutions for mitigating bias, and the proposed frameworks designed to address these issues. Advancing debiasing techniques and ethical AI practices can mitigate bias effects and cultivate inclusive AI systems. Understanding Bias and Its Ethical Implications Addressing bias in AI systems requires understanding its ethical implications and effective mitigation strategies. Adapter modules offer a solution to parameter inefficiency, allowing task-specific parameters to address biases without extensive retraining [55]. This enhances model adaptability and fairness across applications. A critical ethical concern is AI's potential to censor legitimate counter-speech, stifling free expression [56]. Robust mechanisms are needed to distinguish harmful speech from legitimate discourse, ensuring AI fosters freedom of expression. Dataset construction plays a pivotal role in evaluating social biases, highlighting the need for comprehensive measures to evaluate biases effectively [57]. Considering annotator identities in dataset development promotes ethical guidelines throughout the data pipeline [58]. The absence of a unified framework for evaluating fairness exacerbates inconsistencies in research findings [9]. Standardized frameworks are needed for reliable bias evaluations across applications, fostering ethical AI practices. Interdisciplinary approaches are vital for advancing bias understanding and mitigation. Integrating perspectives from disciplines like Gender Studies enhances theorization in NLP research, offering insights into gender bias complexities. Computing the 'moral direction' in language models presents promising avenues for addressing biases [59]. These insights underscore the importance of ethical considerations in AI research. Interdisciplinary collaboration and ongoing discourse are needed to address bias implications, ensuring AI serves as inclusive tools across applications. Understanding bias sources, developing evaluation methods, and creating benchmarks like the Equity Evaluation Corpus can identify biases. Researchers are increasingly aware of AI systems' discriminatory potential in sensitive environments and work towards mitigating biases through improved fairness definitions and model selection strategies, balancing fairness with accuracy [6,9,46]. Evaluation of Fairness in Language Models Metrics and Benchmarks Evaluating fairness in language models requires diverse metrics and benchmarks to assess bias levels, mitigation strategies, and model reliability. Sentiment intensity metrics reveal disparities among demographic groups, highlighting the importance of identifying these disparities, particularly in models like those evaluated by [19], where human assessments and safety scores reflect real-world conversational performance. Gender fairness tasks utilize performance evaluations to compare outcomes before and after implementing strategies like the Gender Equality Prompt (GEEP), ensuring models address gender fairness alongside general linguistic tasks, including those in the GLUE benchmark [3]. Gender Augmented Data Generation (GADG) employs both automatic metrics and manual analyses to evaluate gender-balanced outputs, creating a reliable framework for assessing bias in generated text [2]. The use of adapter modules for performance measurement highlights model efficiency and fairness, achieving near state-of-the-art results with minimal additional parameters [55]. Metrics such as the Integrated Gradient method in the ChatGPT model demonstrate effective fairness evaluations by comparing disparities based on biased versus unbiased prompts [36]. Word Error Rate (WER) and similar metrics measure the fidelity of rewritten text against original versions, ensuring alignment with equitable standards [4]. Bias evaluation metrics applied in studies using methods like Counterfactual Role Reversal Knowledge Distillation reveal nuanced performance insights against baselines [2]. These metrics and benchmarks establish a robust evaluation framework crucial for promoting fairness in language models. They systematically measure social biases, unify diverse fairness metrics, and address trade-offs between fairness and accuracy. This comprehensive approach enables the identification and mitigation of representational biases in real-world applications, ensuring transparent evaluations across various scenarios while balancing performance and fairness [60,61,9,32,62]. Such assessments foster ethical AI practices and support equitable, unbiased results across diverse applications, aligning with user needs and ethical standards. Table provides a detailed summary of key benchmarks and their respective metrics, illustrating the diverse methodologies employed in the evaluation of fairness and bias within language models. Challenges in Fairness Evaluation Evaluating fairness in language models presents challenges due to ambiguity in bias definitions and quantification. Existing benchmarks often fail to encompass all bias forms, necessitating further research to develop comprehensive evaluation frameworks [1]. Limited benchmarks lead to incomplete understanding of model capabilities, underscoring the need for validation across diverse datasets. Capturing all relevant demographic factors and variability in model performance across contexts complicates fairness evaluation. Studies often struggle to establish strong correlations between different bias evaluation measures, potentially misinterpreting debiasing effectiveness [54]. The subjective nature of ethical assessments and difficulty in encapsulating the full spectrum of ethical considerations further complicate evaluation. Debiasing methods like UDDIA illustrate the trade-off between linguistic quality and fairness in generated text [54]. Approaches such as PowerTransformer assess performance through comparative analysis of generated text quality and bias correction effectiveness, highlighting challenges in ensuring fairness across applications [42]. Methodological inconsistencies result in unreliable comparisons and conclusions, complicating effective fairness evaluation [1]. These challenges underscore the need for comprehensive frameworks capable of evaluating fairness in AI systems, ensuring accuracy and equity across diverse applications. As AI systems make critical decisions in sensitive environments, preventing discriminatory behavior towards specific groups is vital. Recent research addresses these issues across machine learning and deep learning subdomains, identifying bias sources and proposing fairness taxonomies. The complexity of fairness research in NLP is compounded by diverse bias definitions and the challenge of balancing fairness with accuracy. Advancing fair AI practices necessitates standardized evaluation methods and clear interrelations among debiasing approaches, guiding researchers toward meaningful progress in bias mitigation [6,9]. Addressing these challenges is essential for developing ethical AI systems aligned with user needs and societal standards. Emerging Trends in Fairness Evaluation Advancements in fairness evaluation have introduced methodologies and frameworks aimed at overcoming limitations of traditional bias assessment approaches. A notable trend is the development of holistic evaluation frameworks incorporating a broad range of demographic factors, facilitating comprehensive bias assessment across diverse social groups [23]. These frameworks use extensive descriptor terms and demographic axes to enhance evaluation reliability. Innovative benchmarking tools, such as the HolisticBias framework, exemplify detailed bias measurement trends. This framework employs an extensive set of descriptors to evaluate biases across multiple demographic dimensions, providing inclusive assessment of language model outputs [23]. The Counter-GAP benchmark, designed to measure gender bias, represents a targeted approach to fairness evaluation, ensuring rigorous testing of gender-related nuances [24]. Intersectional bias evaluation considers compound effects of multiple demographic factors on model outputs, addressing real-world bias complexity by recognizing overlapping discrimination forms, promoting equitable evaluation [21]. Metrics like Sentiment Score and Regard Score reflect an emphasis on understanding language models' differential impact on perceptions across demographic groups [25]. Advanced computational techniques, such as the Contextualized Embedding Association Test (CEAT), mark significant advancements in bias evaluation methodologies. CEAT quantifies overall bias magnitude in neural language models, providing a robust framework for understanding intrinsic biases [29]. This method, alongside other emerging evaluation techniques, underscores the importance of developing tools that accurately capture bias complexities in AI systems. Emerging trends in fairness evaluation reflect a dynamic landscape characterized by comprehensive, intersectional, and technologically advanced methodologies. Advancing these approaches enhances accuracy and reliability of bias assessments, contributing to the development of ethical and inclusive AI systems. Mitigation Strategies for Bias Addressing bias in language models requires exploring effective mitigation strategies. Table presents a detailed classification of bias mitigation strategies in language models, illustrating the diverse approaches employed to enhance fairness and reduce bias across different AI applications. This section examines various approaches, beginning with data-centric techniques that prioritize refining training datasets to enhance data quality and representation, laying the foundation for more equitable AI systems. Data-Centric Mitigation Techniques Data-centric mitigation techniques are essential for reducing biases in language models. Strategies such as data augmentation and preprocessing enhance fairness. The CTP-SHAP method exemplifies a targeted approach that combines evaluation and refinement to address specific biases in LLM-generated text [18]. FairFil employs a neural network to debias outputs from pretrained sentence encoders using a fair filter to adjust representations [35]. Collecting datasets of labeler demonstrations improves models' alignment with user intent through fine-tuning with diverse perspectives [36]. The Gender Equality Prompt (GEEP) method enhances gender fairness while minimizing catastrophic forgetting by using a frozen model with gender-related prompts [3]. The Gender Equality Loss Function (GELF) modifies the loss function to equalize output probabilities for male and female words, promoting gender fairness [2]. Datasets that combine manually curated synthetic data (WinoBias+) with natural data from sources like OpenSubtitles and Reddit contribute to gender-neutral rewriting tasks, ensuring diverse and representative training data [4]. These strategies emphasize refining preprocessing and augmentation methodologies to develop AI systems that yield fair and unbiased results across various applications. Model Architecture and Training Adjustments Adjustments in model architecture and training processes are vital for mitigating biases in language models. Movement pruning, as described by [63], adaptively prunes weights with minimal changes during fine-tuning, reducing gender bias while maintaining model performance. Prompt tuning, explored by [64], enables efficient adaptation without tuning all model weights, facilitating task-specific knowledge integration while aligning with parameter efficiency, as highlighted by [55]. The introduction of long-term memory and continual learning capabilities in models like BlenderBot 3 enhances user engagement and safety, reflecting the significance of architectural innovations in ethical AI interactions [19]. AdapterFusion, described by [16], employs a two-stage learning algorithm to learn and combine task-specific parameters, leveraging knowledge from multiple tasks without destructive interference. The deployment of multilingual models, as demonstrated by [22], significantly improves cross-lingual tasks, particularly for low-resource languages, showcasing the potential of pretraining frameworks to mitigate biases related to language diversity. These model architecture and training adjustments illustrate a comprehensive suite of methodologies aimed at reducing bias in language models, enhancing ethical and equitable performance in AI systems. Recent efforts focus on identifying and mitigating biases across AI subdomains, emphasizing a taxonomy for fairness definitions and understanding trade-offs between fairness and accuracy. A holistic, pipeline-aware approach targets root causes of algorithmic unfairness, ensuring equitable text generation in applications like dialogue systems. Fairness research must consider geo-cultural contexts, as seen in studies from India, fostering alignment with societal standards and inclusivity. Algorithmic and Computational Interventions Algorithmic and computational interventions provide advanced strategies for mitigating bias in language models. Self-debiasing algorithms utilize textual descriptions of undesirable behaviors to reduce biased or toxic content generation, proving effective in debiasing [65,66]. Algorithms based on Lagrangian relaxation for collective inference reduce bias amplification without significant performance loss [33]. Adversarial triggering modifies input to reduce bias propagation, enhancing fairness [34]. The Adversarial Learning for Bias Mitigation (ALBM) framework employs an adversarial approach, training a predictor to forecast outcomes while concurrently training an adversary to predict a protected attribute, effectively reducing stereotyping in tasks like analogy completion and achieving near-equality of odds in classification tasks without compromising predictive accuracy [67,68]. These interventions showcase methodologies aimed at reducing bias in language models. By integrating innovative approaches, researchers can enhance the ethical performance of AI systems, addressing discriminatory behaviors in sensitive environments. The development of a taxonomy for fairness definitions and the examination of biases in real-world applications provide a foundation for aligning AI systems with societal standards, while standardizing evaluation and model selection processes can help balance fairness and accuracy. Innovative Techniques and Emerging Trends The landscape of bias mitigation in language models is evolving through innovative techniques and emerging trends that enhance fairness. Datasets like CrowS-Pairs specifically target stereotypes across nine bias types, providing nuanced insights into bias evaluation [28]. Techniques for reducing gendered correlations in models emphasize a comprehensive approach to gender bias, ensuring equitable outputs across applications [69]. The BBQ benchmark advocates for expanding bias benchmarks to include diverse contexts, crucial for advancing understanding and mitigation of biases in AI systems [21]. In sentiment analysis, systematic methods for evaluating biases enhance the reliability of outputs, promoting fairness where sentiment is critical [46]. Benchmarks emphasizing fairness in AI applications highlight the importance of equitable outcomes, serving as vital tools against harmful biases [7]. Future research should explore comprehensive debiasing techniques applicable to various language models and biases beyond nationality [34]. This research is essential for developing robust frameworks that address the multifaceted nature of bias in AI systems. These innovative techniques and emerging trends underscore the dynamic landscape of bias mitigation in language models. By integrating advancements in bias detection and fairness enhancement, researchers can significantly improve the ethical performance of AI systems, ensuring they serve as inclusive tools across diverse applications. Recent efforts in machine learning and NLP focus on identifying sources of bias and developing debiasing algorithms while balancing fairness with accuracy. Initiatives in equitable text generation demonstrate AI's potential to produce fair outputs from biased data, fostering user engagement and satisfaction. Standardizing fairness evaluation and model selection will guide future research in creating fair and effective AI systems. Applications and Case Studies Case Studies and Applications Real-world case studies provide valuable insights into the challenges and advancements in mitigating bias in language models. BlenderBot 3 exemplifies the integration of long-term memory and continual learning to enhance user engagement while addressing safety concerns, highlighting architectural innovations in ethical AI interactions [19]. The Gender Equality Prompt (GEEP) method utilizes gender-specific prompts in frozen models to mitigate gender bias and promote inclusivity across diverse groups [3]. Multilingual models for low-resource languages in cross-lingual tasks demonstrate the potential of pretraining frameworks to capture contextual nuances and reduce language-related biases [22]. Innovative strategies like the LOT (Learn NOT to) method in generative chatbots reduce non-normative text generation by learning from safe and unsafe language distributions, advancing ethical AI practices [21]. The PowerTransformer case study uses connotation frames and auxiliary tasks to rectify biased language, emphasizing the need to restructure language model outputs to address bias manifestations [42]. These examples offer critical insights into the practical challenges and solutions for bias mitigation, guiding researchers and practitioners in developing ethical AI systems aligned with societal values. Efforts to explore biases in AI subdomains, create fairness taxonomies, and adapt evaluations to cultural contexts, such as those in India, yield frameworks for addressing these challenges. Resources like the Equity Evaluation Corpus (EEC) further support bias examination and rectification in AI systems, contributing to equitable AI solutions [6,70,9,46]. Successful Implementations and Lessons Learned The implementation of bias mitigation strategies in language models has significantly advanced fairness and inclusivity, especially in sensitive domains like healthcare and legal systems. These strategies address biases related to gender, race, and social constructs by identifying representational bias sources and developing benchmarks for measurement. Empirical studies and human evaluations demonstrate these approaches' effectiveness in reducing bias while maintaining high-quality text generation, advancing the performance-fairness balance [9,32]. Notable successes include the Gender Equality Loss Function (GELF) and GEEP, which enhance gender fairness in language models, aligning outputs with gender neutrality while maintaining performance across linguistic tasks. The CrowS-Pairs dataset evaluates and mitigates stereotypes across demographic axes, guiding refined mitigation approaches for complex social biases [28]. Movement pruning within transformer architectures reduces gender bias while preserving model accuracy, highlighting structural modifications' role in achieving fairness [48]. The Plug and Play Language Model (PPLM) enables controllable text generation, steering outputs away from biased content through fine-tuning [49]. These implementations underscore the need for continuous innovation in bias mitigation methodologies. Integrating fairness into model architecture and training paradigms is crucial for addressing biases at multiple levels, from data preprocessing to algorithmic interventions. Ongoing refinement and evaluation ensure AI systems evolve towards greater inclusivity, aligning with ethical standards and societal expectations. Addressing biases and ensuring fairness, as highlighted in various studies, moves the field towards more inclusive AI, particularly in environments where AI systems influence critical decisions. Standardizing evaluation methods and model selection for fairness, alongside resources like the Equity Evaluation Corpus, provides benchmarks for examining biases and guiding equitable AI system development [6,8,9,46]. Addressing Bias in Real-World Applications Addressing bias in real-world applications of language models requires a comprehensive approach encompassing data collection, model training, and evaluation strategies. Integrating fairness considerations into training and evaluation processes is critical, as evidenced by models utilizing long-term memory and continual learning to enhance user engagement while addressing safety concerns [19]. Architectural innovations play a pivotal role in promoting ethical AI interactions. The GEEP method exemplifies practical solutions for improving gender fairness by employing frozen models and gender-related prompts, mitigating gender bias while minimizing catastrophic forgetting [3]. Multilingual models for low-resource languages in cross-lingual tasks reflect efforts to pre-train models that capture context and minimize reliance on biased data patterns [22]. Movement pruning within transformer architectures successfully reduces gender bias while preserving model accuracy, emphasizing structural modifications' importance in achieving fairness [48]. Utilizing datasets like CrowS-Pairs, which target stereotypes associated with nine types of bias, enriches practical approaches to addressing bias by providing nuanced insights into bias evaluation [28]. Leveraging such datasets allows researchers to understand how language models perpetuate biases, guiding the development of more refined mitigation strategies. These approaches underscore the need for comprehensive frameworks addressing bias implications within language models, integrating existing literature into a unified structure. Recognizing bias manifestations in machine learning applications across fields like healthcare and legal systems and differentiating between outcome and error disparities are crucial. These frameworks aim to redefine fairness in NLP by mitigating biases in text generation while retaining vital contextual information, advancing the performance-fairness Pareto frontier. Understanding bias origins, engaging with broader social hierarchies, and fostering collaboration between technologists and affected communities are essential [30,9,32,38,39]. Advancing debiasing techniques and promoting ethical AI practices can mitigate bias effects and foster inclusive AI systems aligned with societal standards and user expectations. Future Directions Advancing our understanding of large language models (LLMs) requires a thorough examination of biases and fairness issues associated with their deployment. This section identifies critical knowledge gaps emerging from rapid LLM advancements, particularly regarding ethical considerations and societal impacts. Addressing these gaps will guide future research and aid in developing more equitable AI systems. The subsequent subsection highlights inherent knowledge gaps in LLMs, emphasizing the need for comprehensive research frameworks that incorporate diverse languages and cultural contexts. Knowledge Gaps in Large Language Models The evolution of LLMs has revealed significant knowledge gaps that require further investigation to ensure ethical deployment and positive societal impact. A key concern involves the genealogies of machine learning datasets and their implications for algorithmic fairness, with pre-trained language models displaying inadequately addressed biases and long-term societal effects [71,40]. Standardized documentation practices and diverse filtering methods are crucial for identifying existing knowledge gaps [72]. Questions remain regarding ethical practices in dataset annotation and their long-term impacts on machine learning outcomes [58]. Expanding research to include diverse languages and refining performance benchmarks in varied linguistic contexts are essential [22]. Improving bias measurement methods for accuracy across broader contexts presents a promising exploration avenue [29]. Expanding datasets to encompass diverse languages and contexts can address these gaps [46]. Future research should refine methods like CTP-SHAP for marginalized groups [18]. Enhancements in safety mechanisms and diverse datasets in models like BlenderBot 3 could improve adaptability and address knowledge gaps [19]. Enhancing gender-neutral datasets and adapting methods like GEEP for other biases are crucial for exploration [3]. Challenges persist in determining best bias measurement practices and understanding experimental condition influences [1]. Future research could explore dataset expansions and model refinements to enhance performance across diverse contexts [4]. Collectively, these gaps underscore the complexities of advancing LLMs and call for interdisciplinary research efforts to promote ethical, unbiased, and culturally sensitive AI systems. Enhancements in Bias Mitigation Techniques Advancements in bias mitigation techniques for language models require thorough exploration of innovative methodologies to address various biases. Enhancing debiasing processes in word embeddings and exploring methods to address additional biases promote fairness across diverse applications [35]. Improving retrieval processes for safe response demonstrations in conversational models could refine bias mitigation strategies [36]. Emerging trends focus on refining debiasing processes for applicability across different languages and contexts, promoting inclusivity [7]. Future work should prioritize enhancing training efficiency to address a wider range of biases [1]. Exploring debiasing principles in dialogue contexts and investigating equity generation improvements could advance bias mitigation techniques [16]. Enhancements in translation systems and applying debiasing methods to other languages are crucial for addressing gender bias in NLP. Future research could improve contextual understanding and character portrayals, broadening bias mitigation efforts [35]. Enhancing the adaptive optimization process of frameworks like AdapterFusion to rectify complex attributes could yield substantial improvements [16]. Applying regularization techniques to other bias forms and investigating automated tuning methods for regularization parameters present promising exploration avenues. Refining filtering techniques for diverse linguistic contexts could enhance bias reduction efforts, contributing to equitable language model outputs [35]. Collectively, these enhancements represent a dynamic landscape in bias mitigation, striving for fairness and inclusivity in language models. Researchers can develop robust frameworks systematically addressing bias complexities across applications, ensuring ethical and equitable AI systems. This involves creating a comprehensive taxonomy of fairness definitions, standardizing evaluation and model selection processes, and utilizing benchmark datasets like the Equity Evaluation Corpus to identify and mitigate biases in AI systems. These efforts are crucial as AI systems increasingly influence life-changing decisions, with researchers balancing fairness and accuracy to prevent discriminatory behavior [6,9,46]. Expansion of Benchmarks and Evaluation Frameworks Expanding benchmarks and evaluation frameworks is vital for advancing the assessment of bias and fairness in language models. Current methodologies often fail to capture nuanced bias dimensions, necessitating comprehensive frameworks incorporating diverse demographic factors and contexts [1]. Introducing benchmarks like HolisticBias, which uses an extensive set of descriptor terms across multiple demographic axes, exemplifies the trend toward granular bias measurement [23]. This framework enhances inclusivity by evaluating biases across various demographic dimensions. The Counter-GAP benchmark, designed to measure gender bias, represents a targeted fairness evaluation approach, ensuring rigorous testing for gender-related nuances [24]. Intersectional bias evaluation methods address real-world biases' complexity by acknowledging overlapping discrimination forms [21]. This approach promotes equitable evaluation by capturing the multifaceted nature of bias. Metrics like Sentiment Score and Regard Score are increasingly used to evaluate bias in generated text, reflecting a growing emphasis on understanding how language models produce outputs affecting perceptions of different demographic groups [25]. Advanced computational techniques, such as the Contextualized Embedding Association Test (CEAT), represent significant advancements in bias evaluation methodologies, quantifying overall bias in neural language models and providing a robust framework for understanding intrinsic biases [29]. Collectively, these expansions in benchmarks and evaluation frameworks reflect a dynamic landscape in pursuing comprehensive, intersectional, and technologically advanced methodologies. Advancing these approaches aims to enhance bias assessment accuracy and reliability, ultimately contributing to developing more ethical and inclusive AI systems. Interdisciplinary Approaches and Ethical Frameworks Interdisciplinary collaboration is crucial for developing ethical frameworks in AI systems to address bias and fairness challenges. Emerging trends focus on robust evaluation techniques and addressing ethical concerns in LLM applications [73]. This integration is vital for embedding ethical considerations in technological development, guiding researchers toward effective debiasing strategies [6]. The survey emphasizes interdisciplinary collaboration to tackle bias in AI, highlighting the importance of engaging with diverse fields for a deeper understanding of bias and fairness [6]. Future research should prioritize creating clearer conceptual frameworks for bias, engaging with affected communities to reimagine power relations between technologists and these communities [71]. By incorporating insights from Gender Studies and viewing datasets as sociotechnical constructs, researchers can develop inclusive frameworks for gender theorization and operationalization in NLP, promoting equitable outcomes [71]. Emerging trends suggest further exploration into advanced prompt designs and integrating prompt-based learning with other machine learning techniques, underscoring interdisciplinary approaches' importance in enhancing AI systems [74]. Ongoing collaboration between dataset creators and users is essential for improving documentation practices, suggesting an interdisciplinary approach that enhances transparency and accountability in AI development [58]. This cooperation is vital for refining data collection and annotation processes, ensuring datasets accurately reflect diverse linguistic and cultural contexts [58]. These interdisciplinary approaches collectively illustrate a dynamic landscape in pursuing ethical AI systems. They emphasize standardizing fairness evaluation and model selection in NLP to address systemic biases while mitigating discriminatory behavior in AI applications across domains. By clarifying bias definitions and establishing a fairness taxonomy, researchers advance meaningful progress in ensuring AI systems make equitable decisions despite complex fairness and accuracy trade-offs. These efforts are crucial as AI technologies become increasingly integrated into sensitive environments impacting life-changing decisions [6,9]. By fostering collaboration across diverse fields, researchers can develop robust frameworks addressing bias and fairness complexities, ensuring AI systems serve as inclusive and equitable tools across applications. Refinement of Model Architectures and Pre-training Objectives Refining model architectures and pre-training objectives is crucial for mitigating bias in language models, ensuring equitable outputs across applications. Integrating adapter modules for efficient parameter tuning without altering the entire model is a promising direction [55]. This approach allows task-specific parameter incorporation, enabling models to adapt to various contexts while maintaining fairness. Movement Pruning exemplifies a structural modification that selectively prunes attention heads within transformer models, effectively reducing gender bias without compromising performance [48]. This method highlights architectural adjustments' potential to address inherent biases. Prompt tuning allows efficient adaptation by modifying prompts rather than entire model weights [64]. This technique aligns with the broader trend toward parameter-efficient learning, exemplified by adapter modules that add minimal parameters per task while maintaining state-of-the-art performance [55]. Developing long-term memory and continual learning capabilities in models like BlenderBot 3 illustrates architectural innovations' importance in promoting ethical AI interactions [19]. These enhancements ensure effective user engagement while addressing safety concerns, reflecting the significance of refining model architectures to enhance fairness. Deploying multilingual models at scale in cross-lingual tasks underscores pre-training frameworks' potential to reduce biases associated with language diversity [22]. This approach emphasizes refining pre-training objectives to accurately capture diverse linguistic contexts, promoting fairness across applications. Collectively, these refinements in model architectures and pre-training objectives represent a dynamic landscape in bias mitigation. By pursuing these advancements, researchers can develop robust frameworks addressing bias complexities across applications, ensuring ethical and equitable AI systems. This involves creating a comprehensive taxonomy of fairness definitions, clarifying interrelations among debiasing methods and fairness theories, and standardizing evaluation and model selection processes to balance fairness and accuracy in AI applications used in sensitive and impactful environments [6,9]. Cultural and Contextual Nuances in Bias Mitigation Recognizing cultural and contextual considerations in bias mitigation strategies is fundamental for developing equitable AI systems. Language models operate within diverse cultural contexts, necessitating a nuanced approach to bias mitigation that accounts for these variations [71]. Integrating cultural sensitivity into AI systems significantly enhances fairness and inclusivity, ensuring accurate reflection of different communities' sociocultural dynamics. A critical aspect of cultural and contextual bias is representing dialectal variations, such as African American Vernacular English (AAVE). Addressing these differences is vital for preventing language models from perpetuating social inequalities through biased representations [5]. Failure to capture dialectal nuances can result in models inaccurately portraying diverse linguistic communities, underscoring the need for robust frameworks to mitigate these biases effectively. Moreover, language's historical evolution associated with gender and ethnic stereotypes highlights cultural biases' persistence in language model outputs. Word embeddings aligned with demographic data capture these biases, demonstrating the need for frameworks to mitigate their impact [40]. Reliance on biased training data exacerbates cultural biases, leading to skewed representations in model predictions and reinforcing stereotypes [34]. Efforts to address cultural and contextual biases include innovative approaches like the PowerTransformer, which uses connotation frames and auxiliary tasks to correct biased language, promoting equitable text generation [42]. Additionally, the chain of thought prompting approach enhances reasoning accuracy and handles complex problems through structured prompts, offering a promising methodology for reducing bias by restructuring language models' text processing and generation [43]. These insights underscore the need for comprehensive frameworks addressing language models' cultural and contextual biases. By advancing debiasing techniques and promoting ethical AI practices, it is feasible to tackle bias challenges in AI systems. This involves standardizing evaluation methods and model selection processes to balance fairness and accuracy and developing a clear taxonomy for fairness definitions. Such efforts are crucial for mitigating discriminatory behaviors in AI applications, fostering inclusive systems aligning with societal standards and user expectations, and ensuring safe AI integration into sensitive environments where significant, life-changing decisions are made [6,9]. Conclusion This survey underscores the pervasive biases present in language models, highlighting the critical need for ongoing research to develop fair and unbiased AI systems. It demonstrates that gender bias can be effectively mitigated while maintaining factual accuracy, thus advancing fairness in language generation. The successful application of adversarial triggering to address nationality bias in GPT-2 further emphasizes the importance of continued exploration in this area. The GELF method's efficacy in reducing gender bias without increasing perplexity marks a significant improvement over existing debiasing strategies. The survey emphasizes the necessity of critically evaluating social norms and their implications, revealing substantial biases in masked language models and their impact on sentiment classifiers, particularly against marginalized groups. This underscores the urgent need for robust bias evaluation, especially at the contextual word level where racial biases are prevalent. The progress in bias mitigation techniques represents a meaningful step toward enhancing fairness in NLP models. Future research should focus on developing more inclusive datasets, improving NLP tools for diverse languages, and examining biases within broader socio-cultural contexts. Expanding bias mitigation techniques and broadening benchmarks to cover various forms of bias in dialogue generation are also crucial. Interdisciplinary collaboration remains essential in tackling the complex challenges of bias and fairness, fostering a comprehensive understanding and establishing ethical frameworks for AI systems. The findings highlight the urgent need for sustainable practices in NLP research, considering the environmental and financial costs associated with AI development. Through interdisciplinary collaboration and sustained research efforts, the field can effectively address the complexities of bias and fairness, ensuring AI systems serve as inclusive and equitable tools across diverse applications.",
  "reference": {
    "1": "2205.11601v1",
    "2": "1905.12801v2",
    "3": "2110.05367v3",
    "4": "2109.06105v1",
    "5": "2204.03031v2",
    "6": "1908.09635v3",
    "7": "2305.18569v2",
    "8": "2307.04303v1",
    "9": "2302.05711v1",
    "10": "1908.09369v3",
    "11": "1906.07337v1",
    "12": "1608.07187v4",
    "13": "2101.10098v1",
    "14": "2108.07258v3",
    "15": "2102.06788v1",
    "16": "2005.00247v3",
    "17": "2309.17337v1",
    "18": "2307.00101v1",
    "19": "2208.03188v3",
    "20": "2106.10328v2",
    "21": "2110.08193v2",
    "22": "1911.02116v2",
    "23": "2001.08764v2",
    "24": "1911.03064v3",
    "25": "1411.4413v2",
    "26": "1810.05201v1",
    "27": "1909.01326v2",
    "28": "2009.11462v2",
    "29": "2210.02938v1",
    "30": "2010.00133v1",
    "31": "2006.03955v5",
    "32": "2104.12405v2",
    "33": "2304.10153v1",
    "34": "2106.13219v1",
    "35": "1707.09457v1",
    "36": "2302.02463v3",
    "37": "2103.06413v1",
    "38": "2203.02155v1",
    "39": "1904.03035v1",
    "40": "1912.11078v2",
    "41": "1911.01485v1",
    "42": "1711.08412v1",
    "43": "2007.08100v1",
    "44": "2010.13816v1",
    "45": "2201.11903v6",
    "46": "2210.07455v2",
    "47": "2210.07440v2",
    "48": "1805.04508v1",
    "49": "2210.11471v1",
    "50": "2207.02463v1",
    "51": "1912.02164v4",
    "52": "2203.12574v1",
    "53": "2306.04597v1",
    "54": "2010.02428v3",
    "55": "2005.06251v1",
    "56": "2210.04492v2",
    "57": "1902.00751v2",
    "58": "2404.01651v1",
    "59": "2210.10040v2",
    "60": "2112.04554v1",
    "61": "2103.11790v3",
    "62": "2104.13640v2",
    "63": "2106.14574v1",
    "64": "2211.09110v2",
    "65": "2005.07683v2",
    "66": "2104.08691v2",
    "67": "2110.08527v3",
    "68": "2103.00453v2",
    "69": "1801.07593v1",
    "70": "2203.06317v2",
    "71": "2010.06032v2",
    "72": "2209.12226v5",
    "73": "2007.07399v1",
    "74": "2104.08758v2",
    "75": "2307.03109v9",
    "76": "2107.13586v1"
  },
  "chooseref": {
    "1": "2208.03188v3",
    "2": "2105.02685v1",
    "3": "1908.09635v3",
    "4": "2307.03109v9",
    "5": "2305.13862v2",
    "6": "1705.07874v2",
    "7": "2211.05414v3",
    "8": "2005.00247v3",
    "9": "2110.08527v3",
    "10": "1911.01485v1",
    "11": "1911.09709v3",
    "12": "1910.13461v1",
    "13": "2110.08193v2",
    "14": "1902.04094v2",
    "15": "1810.04805v2",
    "16": "2212.10563v2",
    "17": "2101.11718v1",
    "18": "2305.12018v1",
    "19": "2306.05550v1",
    "20": "1904.04047v3",
    "21": "2102.09690v2",
    "22": "2201.11903v6",
    "23": "2205.11601v1",
    "24": "2210.11471v1",
    "25": "2306.03350v1",
    "26": "2109.03858v2",
    "27": "2212.08073v1",
    "28": "2007.07399v1",
    "29": "2210.07455v2",
    "30": "1906.04571v3",
    "31": "1809.10610v2",
    "32": "2212.10938v1",
    "33": "2010.00133v1",
    "34": "1803.09010v8",
    "35": "2110.05719v1",
    "36": "2210.02938v1",
    "37": "2101.09523v1",
    "38": "2109.11708v1",
    "39": "2006.03955v5",
    "40": "2104.06390v1",
    "41": "2212.10543v2",
    "42": "2005.00372v3",
    "43": "2104.08758v2",
    "44": "1910.10486v3",
    "45": "2206.10744v1",
    "46": "2203.09192v1",
    "47": "1610.02413v1",
    "48": "1805.04508v1",
    "49": "2305.11140v1",
    "50": "1910.10683v4",
    "51": "2302.05711v1",
    "52": "2212.06803v1",
    "53": "1610.07524v1",
    "54": "2207.04546v2",
    "55": "2103.06413v1",
    "56": "2305.18569v2",
    "57": "2104.07429v2",
    "58": "2009.06367v2",
    "59": "1904.03310v1",
    "60": "1804.09301v1",
    "61": "1804.06876v1",
    "62": "2207.02463v1",
    "63": "2107.05987v1",
    "64": "2201.07754v3",
    "65": "2008.06460v2",
    "66": "2211.09110v2",
    "67": "2204.06827v2",
    "68": "1411.4413v2",
    "69": "1904.03035v1",
    "70": "2110.05367v3",
    "71": "2306.04140v1",
    "72": "2210.07440v2",
    "73": "2006.08564v2",
    "74": "1909.00871v3",
    "75": "2104.12405v2",
    "76": "2005.14165v4",
    "77": "2306.04597v1",
    "78": "2205.11916v4",
    "79": "2103.11790v3",
    "80": "2304.11220v2",
    "81": "2206.08743v1",
    "82": "2307.04303v1",
    "83": "2205.11374v1",
    "84": "1607.06520v1",
    "85": "2305.18189v1",
    "86": "1910.14659v3",
    "87": "1912.05511v3",
    "88": "2010.06032v2",
    "89": "1906.07337v1",
    "90": "1707.09457v1",
    "91": "1810.05201v1",
    "92": "2005.06251v1",
    "93": "2203.12574v1",
    "94": "2108.07790v3",
    "95": "2109.05704v2",
    "96": "1801.07593v1",
    "97": "1810.03993v2",
    "98": "2005.07683v2",
    "99": "2302.02463v3",
    "100": "2109.06105v1",
    "101": "2010.12884v2",
    "102": "2010.12820v2",
    "103": "2404.01651v1",
    "104": "2004.07667v2",
    "105": "2007.00049v2",
    "106": "1908.09369v3",
    "107": "2304.12397v1",
    "108": "2101.10098v1",
    "109": "2304.10153v1",
    "110": "2108.07258v3",
    "111": "2010.12864v2",
    "112": "1612.00796v2",
    "113": "2204.02311v5",
    "114": "1902.00751v2",
    "115": "2012.07463v2",
    "116": "2212.10190v1",
    "117": "2101.05783v2",
    "118": "2205.12586v2",
    "119": "1912.02164v4",
    "120": "2010.13816v1",
    "121": "2107.13586v1",
    "122": "1912.11078v2",
    "123": "2101.00190v1",
    "124": "1711.05144v5",
    "125": "2106.10328v2",
    "126": "2307.01595v1",
    "127": "2205.12688v2",
    "128": "2401.10862v3",
    "129": "2106.14574v1",
    "130": "2205.13636v2",
    "131": "1911.03842v2",
    "132": "2307.00101v1",
    "133": "1707.00061v1",
    "134": "2209.12226v5",
    "135": "2009.11462v2",
    "136": "2111.01243v1",
    "137": "2010.07079v3",
    "138": "2106.03521v1",
    "139": "1905.12801v2",
    "140": "2001.08764v2",
    "141": "1911.03064v3",
    "142": "1907.11692v1",
    "143": "1606.05250v3",
    "144": "2110.07518v2",
    "145": "2103.00453v2",
    "146": "1608.07187v4",
    "147": "2305.10204v1",
    "148": "2304.03738v3",
    "149": "2005.00813v1",
    "150": "2011.00620v3",
    "151": "2110.07871v2",
    "152": "2104.13640v2",
    "153": "2004.09456v1",
    "154": "2109.03646v1",
    "155": "2201.08643v1",
    "156": "2102.01672v3",
    "157": "2104.08691v2",
    "158": "2210.10040v2",
    "159": "1909.01326v2",
    "160": "2109.14047v1",
    "161": "2102.06788v1",
    "162": "2302.13136v1",
    "163": "2309.17337v1",
    "164": "2005.00268v2",
    "165": "1912.03593v1",
    "166": "2009.12303v4",
    "167": "2007.08100v1",
    "168": "2203.06317v2",
    "169": "2106.13219v1",
    "170": "2203.02155v1",
    "171": "2306.11507v1",
    "172": "2010.02428v3",
    "173": "2210.04492v2",
    "174": "2010.14534v1",
    "175": "2104.07496v1",
    "176": "1911.02116v2",
    "177": "2302.00871v3",
    "178": "2204.03031v2",
    "179": "2206.00701v1",
    "180": "2305.06626v5",
    "181": "2112.04554v1",
    "182": "2306.15087v2",
    "183": "1711.08412v1"
  }
}