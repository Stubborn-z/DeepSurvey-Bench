{
  "authors": [
    "Yutao Zhu",
    "Huaying Yuan",
    "Shuting Wang",
    "Jiongnan Liu",
    "Wenhan Liu",
    "Chenlong Deng",
    "Zhicheng Dou",
    "Ji-rong Wen"
  ],
  "literature_review_title": "Large Language Models for Information Retrieval: A Survey",
  "year": "2023",
  "date": "2023-08-14",
  "category": "cs.CL",
  "abstract": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[10pt,journal,compsoc]{IEEEtran} amsmath,amsfonts \\bA{A} \\bB{B} \\bC{C} \\bD{D} \\bE{E} \\bF{F} \\bG{G} \\bH{H} \\bI{I} \\bJ{J} \\bK{K} \\bL{L} \\bM{M} \\bN{N} \\bO{O} \\bP{P} \\bQ{Q} \\bR{R} \\bS{S} \\bT{T} \\bU{U} \\bV{V} \\bW{W} \\bX{X} \\bY{Y} \\bZ{Z} \\ba{a} \\bb{b} \\bc{c} \\bd{d} \\be{e} \\bg{g} \\bh{h} \\bi{i} \\bj{j} \\bk{k} \\bl{l} \\bm{m} \\bn{n} \\bo{o} \\bp{p} \\bq{q} \\br{r} \\bs{s} \\bt{t} \\bu{u} \\bv{v} \\bw{w} \\bx{x} \\by{y} \\bz{z} \\def{\\mathcal{A}} \\def{\\mathcal{B}} \\def{\\mathcal{C}} \\def{\\mathcal{D}} \\def{\\mathcal{E}} \\def{\\mathcal{F}} \\def{\\mathcal{G}} \\def{\\mathcal{H}} \\def{\\mathcal{I}} \\def{\\mathcal{J}} \\def{\\mathcal{K}} \\def{\\mathcal{L}} \\def{\\mathcal{M}} \\def{\\mathcal{N}} \\def{\\mathcal{O}} \\def{\\mathcal{P}} \\def{\\mathcal{Q}} \\def{\\mathcal{R}} \\def{\\mathcal{S}} \\def{\\mathcal{T}} \\def{\\mathcal{U}} \\def{\\mathcal{V}} \\def{\\mathcal{W}} \\def{\\mathcal{X}} \\def{\\mathcal{Y}} \\def{\\mathcal{Z}} \\ie{i.e.} \\eg{e.g.} \\softmax{softmax} \\sigmoid{sigmoid} \\R{R} \\argmax{arg\\,max} \\argmin{arg\\,min} \\ttt{\\texttt} \\usepackage[utf8]{inputenc} \\usepackage[numbers,sort&compress]{natbib} graphicx% multirow% amsmath,amssymb,amsfonts% booktabs% url caption \\usepackage[colorlinks,linkcolor=black,anchorcolor=black,citecolor=black]{hyperref} makecell balance ragged2e colortbl tcolorbox todonotes format=plain,labelformat=simple,labelsep=period \\mypara{5pt\\noindent\\textbf} \\dou[1]{red{[[@dou: #1]]}} \\fix[1]{green{[[@fix: #1]]}} \\ifCLASSINFOpdf \\else \\fi document Large Language Models for Information Retrieval: A Survey Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng \\\\ Haonan Chen, Zheng Liu, Zhicheng Dou, and Ji-Rong Wen \\thanks{All authors except Zheng Liu are from Gaoling School of Artificial Intelligence and School of Information, Renmin University of China. Zheng Liu is from Beijing Academy of Artificial Intelligence, China. \\\\ Contact e-mail: yutaozhu94@gmail.com, dou@ruc.edu.cn } % \\begin{justify justify IEEEkeywords Large Language Models; Information Retrieval; Query Rewriter; Reranking; Reader; Fine-tuning; Prompting; Agent IEEEkeywords} \\maketitle \\IEEEdisplaynontitleabstractindextext \\IEEEpeerreviewmaketitle \\IEEEraisesectionheading{",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "} figure*[t] \\centering \\includegraphics[width=.8\\linewidth]{fig/architecture.pdf} Overview of existing studies that apply LLMs into IR. (1) LLMs can be used to enhance traditional IR components, such as query rewriter, retriever, reranker, and reader. (2) LLMs can also be used as search agents to perform multiple IR tasks. figure* I{nformation} access is one of the fundamental daily needs of human beings. To fulfill the need for rapid acquisition of desired information, various information retrieval (IR) systems have been developed~DBLP:conf/acl/WuWXZL17,xiaoice,dpr,DBLP:journals/csur/DattaJLW08. Prominent examples include search engines such as Google, Bing, and Baidu, which serve as IR systems on the Internet, adept at retrieving relevant web pages in response to user queries, and provide convenient and efficient access to information on the Internet. It is worth noting that IR extends beyond web page retrieval. In dialogue systems (chatbots)~DBLP:conf/acl/WuWXZL17,DBLP:conf/emnlp/YuanZLLZHH19,DBLP:conf/ecir/ZhuNZDD21,DBLP:conf/sigir/ZhuNZDJD21,DBLP:journals/corr/abs-2108-07935, such as Microsoft Xiaoice~xiaoice, Apple Siri,Apple Siri, \\url{https://www.apple.com/siri/} and Google Assistant,Google Assistant, \\url{https://assistant.google.com/} IR systems play a crucial role in retrieving appropriate responses to user input utterances, thereby producing natural and fluent human-machine conversations. Similarly, in question-answering systems~dpr,rocketqa, IR systems are employed to select relevant clues essential for addressing user questions effectively. In image search engines~DBLP:journals/csur/DattaJLW08, IR systems excel at returning images that align with user input queries. Given the exponential growth of information, research and industry have become increasingly interested in the development of effective IR systems. The core function of an IR system is retrieval, which aims to determine the relevance between a user-issued query and the content to be retrieved, including various types of information such as texts, images, music, and more. For the scope of this survey, we concentrate solely on reviewing those text retrieval systems, in which query-document relevance is commonly measured by their matching score.The term ``document'' will henceforth refer to any text-based content subject to retrieve, including both long articles and short passages. Given that IR systems operate on extensive repositories, the efficiency of retrieval algorithms becomes of paramount importance. To improve the user experience, the retrieval performance is enhanced from both the upstream (query reformulation) and downstream (reranking and reading) perspectives. As an upstream technique, query reformulation is designed to refine user queries so that they are more effective at retrieving relevant documents~DBLP:journals/jiis/ArensKS96,DBLP:conf/cikm/HuangE09. With the recent surge in the popularity of conversational search, this technique has received increasing attention. On the downstream side, reranking approaches are developed to further adjust the document ranking~monobert,monot5,coca. In contrast to the retrieval stage, reranking is performed only on a limited set of relevant documents, already retrieved by the retriever. Under this circumstance, the emphasis is placed on achieving higher performance rather than keeping higher efficiency, allowing for the application of more complex approaches in the reranking process. Additionally, reranking can accommodate other specific requirements, such as personalization~DBLP:conf/sigir/TeevanDH05,DBLP:conf/sigir/BennettWCDBBC12,DBLP:conf/cikm/GeDJNW18,DBLP:conf/cikm/ZhouD0W21 and diversification~DBLP:conf/sigir/CarbonellG98,DBLP:conf/wsdm/AgrawalGHI09,DBLP:conf/sigir/LiuDWLW20,DBLP:conf/sigir/SuDZQW21. Following the retrieval and reranking stages, a reading component is incorporated to summarize the retrieved documents and deliver a concise document to users~RETRO,webgpt. While traditional IR systems typically require users to gather and organize relevant information themselves; however, the reading component is an integral part of new IR systems such as New Bing,New Bing, \\url{https://www.bing.com/new} improving users' browsing experience and saving valuable time. The trajectory of IR has traversed a dynamic evolution, transitioning from its origins in term-based methods to the integration of neural models. Initially, IR was anchored in term-based methods~DBLP:books/mg/SaltonG83 and Boolean logic, focusing on keyword matching for document retrieval. The paradigm gradually shifted with the introduction of vector space models~vector_space, unlocking the potential to capture nuanced semantic relationships between terms. This progression continued with statistical language models~DBLP:conf/cikm/SongC99,tfidf, refining relevance estimation through contextual and probabilistic considerations. The influential BM25 algorithm~DBLP:conf/trec/RobertsonWJHG94 played an important role during this phase, revolutionizing relevance ranking by accounting for term frequency and document length variations. The most recent chapter in IR's journey is marked by the ascendancy of neural models~DBLP:conf/cikm/GuoFAC16,dpr,ance,bert4ranking. These models excel at capturing intricate contextual cues and semantic nuances, reshaping the landscape of IR. However, these neural models still face challenges such as data scarcity, interpretability, and the potential generation of plausible yet inaccurate responses. Thus, the evolution of IR continues to be a journey of balancing traditional strengths (such as the BM25 algorithm's high efficiency) with the remarkable capability (such as semantic understanding) brought about by modern neural architectures. Large language models (LLMs) have recently emerged as transformative forces across various research fields, such as natural language processing (NLP)~gpt-2,gpt-3,llama, recommender systems~DBLP:journals/corr/abs-2305-07001,DBLP:journals/corr/abs-2305-08845,DBLP:journals/corr/abs-2306-10933,DBLP:journals/corr/abs-2307-02046, finance~bloomberggpt, and even molecule discovery~DBLP:journals/corr/abs-2306-06615. These cutting-edge LLMs are primarily based on the Transformer architecture and undergo extensive pre-training on diverse textual sources, including web pages, research articles, books, and codes. As their scale continues to expand (including both model size and data volume), LLMs have demonstrated remarkable advances in their capabilities. On the one hand, LLMs have exhibited unprecedented proficiency in language understanding and generation, resulting in responses that are more human-like and better aligned with human intentions. On the other hand, the larger LLMs have shown impressive emergent abilities when dealing with complex tasks~emergent_ability, such as generalization and reasoning skills. Leveraging the impressive power of LLMs can undoubtedly improve the performance of IR systems. By incorporating these advanced language models, IR systems can provide users with more accurate responses, ultimately reshaping the landscape of information access and retrieval. Initial efforts have been made to utilize the potential of LLMs in the development of novel IR systems. Notably, in terms of practical applications, New Bing is designed to improve the users' experience of using search engines by extracting information from disparate web pages and condensing it into concise summaries that serve as responses to user-generated queries. In the research community, LLMs have proven useful within specific modules of IR systems (such as retrievers), thereby enhancing the overall performance of these systems. Due to the rapid evolution of LLM-enhanced IR systems, it is essential to comprehensively review their most recent advancements and challenges. Our survey provides an insightful exploration of the intersection between LLMs and IR systems, covering key perspectives such as query rewriters, retrievers, rerankers, and readers (as shown in Figure~fig:overview).As yet, there has not been a formal definition for LLMs. In this paper, we mainly focus on models with more than 1B parameters. We also notice that some methods do not rely on such strictly defined LLMs, but due to their representativeness, we still include an introduction to them in this survey. We also include some recent studies that leverage LLMs as search agents to perform various IR tasks. This analysis enhances our understanding of LLMs' potential and limitations in advancing the IR field. For this survey, we create a Github repository by collecting the relevant papers and resources about applying LLMs for IR tasks (LLM4IR).\\url{https://github.com/RUC-NLPIR/LLM4IR-Survey} We will continue to update the repository with newer papers. This survey will also be periodically updated according to the development of this area. We notice that there are several surveys for PLMs, LLMs, and their applications (\\eg, AIGC or recommender systems)~DBLP:journals/csur/LiuYFJHN23,DBLP:journals/corr/abs-2003-08271,DBLP:journals/corr/abs-2303-04226,DBLP:conf/ijcai/LiTZW21,DBLP:journals/corr/abs-2301-00234,DBLP:conf/acl/0009C23,llm_survey. Compared with them, we focus on the techniques and methods for developing and applying LLMs for IR systems. In addition, we suggest reading the strategy report from the Chinese IR community~ir_perspective, which discusses the opportunity and future directions of IR in the era of LLMs, and we think it is an excellent supplement to this survey. The remaining part of this survey is organized as follows: Section~sec:bk introduces the background for IR and LLMs. Section~sec:qr,~sec:ret,~sec:rank,~sec:reader respectively review recent progress from the four perspectives of query rewriter, retriever, reranker, and reader, which are four key components of an IR system. Section~sec:agent introduces recent studies of search agents. Then, Section~sec:future discusses some potential directions in future research. Finally, we conclude the survey in Section~sec:conclu by summarizing the major findings.",
      "origin_cites_number": 22
    },
    {
      "section_title": "Background",
      "level": "1",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Information Retrieval",
      "level": "2",
      "content": "Information retrieval (IR), as an essential branch of computer science, aims to efficiently retrieve information relevant to user queries from a large repository. Generally, users interact with an IR system by submitting their queries in textual form. Subsequently, IR systems undertake the task of matching and ranking these user-supplied queries against an indexed database, thereby facilitating the retrieval of the most pertinent results. The field of IR has witnessed significant advancement with the emergence of various models over time. One such early model is the Boolean model, which employs Boolean logic operators to combine query terms and retrieve documents that satisfy specific conditions~DBLP:books/mg/SaltonG83. Based on the ``bag-of-words'' assumption, the vector space model~vector_space represents documents and queries as vectors in term-based space. Relevance estimation is then performed by assessing the lexical similarity between the query and document vectors. The efficiency of this model is further improved through the effective organization of text content using the inverted index. Moving towards more sophisticated approaches, statistical language models have been introduced to estimate the likelihood of term occurrences and incorporate context information, leading to more accurate and context-aware retrieval~DBLP:conf/cikm/SongC99, DBLP:journals/arist/LiuC05. In recent years, the neural IR paradigm has gained considerable attention in the research community~DBLP:conf/cikm/GuoFAC16, DBLP:journals/corr/MitraC17, DBLP:journals/corr/abs-2211-14876. By harnessing the powerful representation capabilities of neural networks, this paradigm can capture semantic relationships between queries and documents, thereby significantly enhancing retrieval performance. Researchers have identified several challenges with implications for the performance and effectiveness of IR systems, such as query ambiguity and retrieval efficiency. In light of these challenges, researchers have directed their attention toward crucial modules within the retrieval process, aiming to address specific issues and effectuate corresponding enhancements. The pivotal role of these modules in ameliorating the IR pipeline and elevating system performance cannot be overstated. In this survey, we focus on the following four modules, which have been greatly enhanced by LLMs. Query Rewriter is an essential IR module that seeks to improve the precision and expressiveness of user queries. Positioned at the early stage of the IR pipeline, this module assumes the crucial role of refining or modifying the initial query to align more accurately with the user's information requirements. As an integral part of query rewriter, query expansion techniques, with pseudo relevance feedback being a prominent example, represent the mainstream approach to achieving query expression refinement. In addition to its utility in improving search effectiveness across general scenarios, the query rewriter finds application in diverse specialized retrieval contexts, such as personalized search and conversational search, thus further demonstrating its significance. Retriever, as discussed here, is typically employed in the early stages of IR for document recall. The evolution of retrieval technologies reflects a constant pursuit of more effective and efficient methods to address the challenges posed by ever-growing text collections. In numerous experiments on IR systems over the years, the classical ``bag-of-words'' model BM25~DBLP:conf/trec/RobertsonWJHG94 has demonstrated its robust performance and high efficiency. In the wake of the neural IR paradigm's ascendancy, prevalent approaches have primarily revolved around projecting queries and documents into high-dimensional vector spaces, and subsequently computing their relevance scores through inner product calculations. This paradigmatic shift enables a more efficient understanding of query-document relationships, leveraging the power of vector representations to capture semantic similarities. Reranker, as another crucial module in the retrieval pipeline, primarily focuses on fine-grained reordering of documents within the retrieved document set. Different from the retriever, which emphasizes the balance of efficiency and effectiveness, the reranker module places a greater emphasis on the quality of document ranking. In pursuit of enhancing the search result quality, researchers delve into more complex matching methods than the traditional vector inner product, thereby furnishing richer matching signals to the reranker. Moreover, the reranker facilitates the adoption of specialized ranking strategies tailored to meet distinct user requirements, such as personalized and diversified search results. By integrating domain-specific objectives, the reranker module can deliver tailored and purposeful search results, enhancing the overall user experience. Reader has evolved as a crucial module with the rapid development of LLM technologies. Its ability to comprehend real-time user intent and generate dynamic responses based on the retrieved text has revolutionized the presentation of IR results. In comparison to presenting a list of candidate documents, the reader module organizes answer texts more intuitively, simulating the natural way humans access information. To enhance the credibility of generated responses, the integration of references into generated responses has been an effective technique of the reader module. Furthermore, researchers explore unifying the above modules to develop a novel LLM-driven search model known as the Search Agent. The search agent is distinguished by its simulation of an automated search and result understanding process, which furnishes users with accurate and readily comprehensible answers. WebGPT~webgpt serves as a pioneering work in this category, which models the search process as a sequence of actions of an LLM-based agent within a search engine environment, autonomously accomplishing the whole search pipeline. By integrating the existing search stack, search agents have the potential to become a new paradigm in future IR.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Large Language Models",
      "level": "2",
      "content": "Language models (LMs) are designed to understand or generate human language by taking into account the contextual information from word sequences. The evolution from statistical language models to neural language models makes it feasible to utilize LMs for representation learning beyond mere word sequence modeling. elmo first proposed to learn contextualized word representations through pre-training a bidirectional LSTM (biLSTM) network on large-scale corpora, followed by fine-tuning on specific downstream tasks. Similarly, bert proposed to pre-train a Transformer~transformer encoder with a specially designed Masked Language Modeling (MLM) task and Next Sentence Prediction (NSP) task on large corpora. These studies initiated a new era of pre-trained language models (PLMs), with the ``pre-training then fine-tuning'' paradigm emerging as the prevailing learning approach. Along this line, numerous generative PLMs (\\eg, GPT-2~gpt-2, BART~bart, and T5~t5) have been developed for text generation problems including summarization, machine translation, and dialogue generation. Recently, researchers have observed that increasing the scale of PLMs (\\eg, model size or data amount) can consistently improve their performance on downstream tasks (a phenomenon commonly referred to as the scaling law~scalinglaw_openai,scalinglaw_deepmind). Moreover, large-sized PLMs exhibit promising abilities (termed emergent abilities~emergent_ability) in addressing complex tasks, which are not evident in their smaller counterparts. Therefore, the research community refers to these large-sized PLMs as large language models (LLMs). Owing to their vast number of parameters, fine-tuning LLMs for specific tasks, such as IR, is often deemed impractical. Consequently, two prevailing methods for applying LLMs have been established: in-context learning (ICL) and parameter-efficient fine-tuning. ICL is one of the emergent abilities of LLMs~gpt-3 empowering them to comprehend and furnish answers based on the provided input context, rather than relying merely on their pre-training knowledge. This method requires only the formulation of the task description and demonstrations in natural language, which are then fed as input to the LLM. Notably, parameter tuning is not required for ICL. Additionally, the efficacy of ICL can be further augmented through the adoption of chain-of-thought prompting, involving multiple demonstrations (describe the chain of thought examples) to guide the model's reasoning process. ICL is the most commonly used method for applying LLMs to IR. Parameter-efficient fine-tuning~lora,DBLP:conf/acl/LiL20,DBLP:conf/emnlp/LesterAC21 aims to reduce the number of trainable parameters while maintaining satisfactory performance. LoRA~lora, for example, has been widely applied to open-source LLMs (\\eg, LLaMA and BLOOM) for this purpose. Recently, QLoRA~qlora has been proposed to further reduce memory usage by leveraging a frozen 4-bit quantized LLM for gradient computation. Despite the exploration of parameter-efficient fine-tuning for various NLP tasks, its implementation in IR tasks remains relatively limited, representing a potential avenue for future research. Recently, research has focused on enhancing LLM capabilities by improving their reasoning and inference-time processes. Large reasoning models (LRMs) are an evolution of LLMs specifically designed to excel at complex logical tasks such as mathematics and coding. These models often employ advanced training methodologies, including reinforcement learning, to develop sophisticated self-reflection and planning mechanisms. They are designed to generate a detailed ``thinking process'' before providing a final answer, which has shown improved performance on reasoning benchmarks. This focus on improving the reasoning process is highly relevant to test-time scaling, a paradigm that allocates additional computational resources during inference to improve model performance without increasing model size during pre-training. This can involve generating multiple outputs in parallel and selecting the best one, or employing sequential methods where the model iteratively refines its own output. By achieving promising results on expert-level human challenges, these LRMs provide a new paradigm for IR systems.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Query Rewriter",
      "level": "1",
      "content": "Query rewriter, functioning as an essential preprocessing component for search engines, increases the accuracy of retrieval systems through the refinement of initial queries~DBLP:journals/ipm/AzadD19. This mechanism, also known as query expansion or reformulation, holds a pivotal position in search engine operations. In the context of ad-hoc retrieval, the design of a query rewriter aims to mitigate the vocabulary mismatch problem by enriching original queries with semantically related terms. As conversational search evolves, query rewriters have evolved to interpret user intent and previous dialogues, thereby enabling context-sensitive queries. In this survey, the term ``query rewriter'' is used to refer to any technique that improves retrieval performance through query modification. Traditional query rewriting strategies primarily include techniques such as utilizing lexical knowledge bases~DBLP:journals/jasis/PeatW91,fellbaum1998wordnet,DBLP:journals/sigir/Fox80,DBLP:journals/jocch/ZoharLSD13,DBLP:journals/tois/GauchWR99 and pseudo-relevance feedback~li2007improving,DBLP:conf/ictir/XiongC15,DBLP:journals/nca/SinghS17. However, these methods are limited due to the inadequate capabilities of knowledge models and the presence of noisy signals from coarse matching between the query and the top-$k$ retrieved documents. LLMs, pretrained on vast datasets, demonstrate considerable advancements in the breadth of knowledge and language understanding, positioning them as an excellent resource for query rewriting tasks. In the subsequent sections, we provide a comprehensive review of recent research that applies LLMs to query rewriting.",
      "origin_cites_number": 21
    },
    {
      "section_title": "Rewriting Scenarios",
      "level": "2",
      "content": "In the realm of IR, a query rewriter is primarily designed to serve two distinct scenarios: ad-hoc retrieval and conversational search. Ad-hoc retrieval aims to bridge the semantic gap between a user's query and the potential documents. LLMs, with their extensive inherent knowledge, have proven effective in replacing traditional lexical knowledge databases~DBLP:journals/jasis/PeatW91,fellbaum1998wordnet,DBLP:journals/sigir/Fox80,DBLP:journals/jocch/ZoharLSD13,DBLP:journals/tois/GauchWR99. For conversational search, query rewriters aim to refine a query within a conversation's context, transforming it into isolated queries based on historical dialogues. A crucial requirement for conversational query rewriters is to address coreference resolution. Traditional query rewriting methods, trained on limited data, have shown suboptimal performance, as conversational search sessions tend to be diverse and long-tailed~DBLP:conf/emnlp/MaoDQMCC22,DBLP:conf/icml/DaiCZARGG22. This is particularly the case in more complex conversational search sessions. However, LLMs, with their robust context understanding capabilities, have demonstrated significant advantages in conversational query rewriting~kelong_conversational,Converser. Beyond traditional retrieval scenarios, query rewriting is also widely used in a variety of practical domains. In the context of agents, effectively identifying the most relevant tools for a given task becomes a key bottleneck as the toolset size grows, hindering reliable tool utilization. To address this, current study~chen-etal-2024-invoke propose generating a diverse set of synthetic queries that comprehensively cover different aspects of the query space associated with each tool document during the tool indexing phase. On the other hand, in clinical terminology normalization, recent study~fan-etal-2024-rrnorm also leverage large language models to decompose and reconstruct complex diagnostic mentions, improving the mapping to standard terms through a \"retrieve-and-rank\" framework to enhance overall performance. By leveraging the capabilities of LLMs, researchers have been able to generate a variety of formats for rewritten queries, such as questions~sigirAlaofi,ma2023query,An_Interactive_Query_Generation,ye2023enhancing,RaFe,kelong_conversational,Converser, wilson2025contextualizingsearchqueriesincontext, answer-incorporated passages~wang2023query2doc,craft_the_path,GRM,RoundInteraction,LameR,gao2022precise,jagerman2023query,CAR, KELLER, and keywords~GRF+PRF,keywords,BEQUE,ma2023query. Comprehensive details for each format are available in the following part.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Formats of Rewritten Queries",
      "level": "2",
      "content": "The intended format for rewritten queries can vary widely based on the specific needs and the downstream retrieval system. The ultimate goal is to improve the effectiveness of IR. Typically, the formats include questions, keywords, and answer-incorporated passages.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Questions",
      "level": "3",
      "content": "Rewriting original queries into similar form questions are a natural idea of query rewriting~sigirAlaofi,ye2023enhancing,RaFe. Query rewriters modify original queries to new questions to make it more precise, understandable, and aligned with the user's actual search intent. This can involve rephrasing, expanding, or simplifying the query. Recent research~sigirAlaofi has demonstrated the potential of using LLMs to generate query variants. Although these variants cannot cover the full range of human-generated ones, they do produce highly similar sets of relevant documents.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Keywords",
      "level": "3",
      "content": "Keywords serve as a high-level abstraction of the concepts contained within a query. Rewriting queries into keywords proves particularly effective when the downstream retriever is a sparse retriever. With specific instructions, LLMs can produce high-quality keywords or concepts for query rewriting~ma2023query, jagerman2023query, BEQUE, GRF+PRF. For example, BEQUE~BEQUE formulates new queries as keywords for effective product searches, and keyword_and_refine introduce a two-round query rewriting process, which first generates a set of high-quality seed keywords, then utilizes these keywords to enhance the query.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Answer-incorporated Passages",
      "level": "3",
      "content": "The semantic gap between short-form queries and long-form documents has been a persistent challenge. The advent of LLMs with their inherent question-answering capabilities has introduced a novel approach to query rewriting. This approach involves initially utilizing LLMs to generate comprehensive answers to the given queries. These detailed answers are then employed to retrieve relevant passages from the corpus, thereby effectively bridging the semantic divide between short queries and long candidate documents. The prompt employed for this mechanism is typically structured as follows: ``Given a question {query} and its potential answer passages {passages}, compose a passage that provides an answer to the question''~LameR,RoundInteraction. This approach enables a more nuanced and contextually relevant retrieval of information, enhancing the overall effectiveness of the query rewriting process~wang2023query2doc,craft_the_path,GRM,RoundInteraction,LameR,gao2022precise,jagerman2023query.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Approaches",
      "level": "2",
      "content": "The utilization of LLMs in query rewriting can be categorized into three primary methodologies: prompting, supervised fine-tuning, and reinforcement learning. The {prompting} approaches employ specific prompts to guide the LLM's output, providing flexibility and interpretability. The {supervised fine-tuning} techniques adapt pre-trained LLMs to the specific task of query rewriting. However, the scarcity of training data for query rewriting often poses a challenge. To address this issue, {reinforcement learning} methods utilize feedback from downstream applications, thereby improving the performance of query rewriters. In the following section, we will introduce these three methods in detail.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Prompting",
      "level": "3",
      "content": "Prompting in LLMs refers to the technique of providing a specific instruction or context to guide the model's generation of text. The prompt serves as a conditioning signal and influences the language generation process of the model. Existing prompting strategies can be roughly categorized into three groups: zero-shot prompting, few-shot prompting, and chain-of-thought (CoT) prompting~cot. $\\bullet$ Zero-shot prompting. Zero-shot prompting involves instructing the model to generate texts on a specific topic without any prior exposure to training examples in that domain or topic. The model relies on its pre-existing knowledge and language understanding to generate coherent and contextually relevant expanded terms for original queries. Experiments show that zero-shot prompting is a simple yet effective method for query rewriter~sigirAlaofi,wenhao_generate,GRF+PRF,LameR,RoundInteraction,jagerman2023query. $\\bullet$ Few-shot prompting. Few-shot prompting, also known as in-context learning, involves providing the model with a limited set of examples or demonstrations related to the desired task or domain~sigirAlaofi,wenhao_generate,jagerman2023query,wang2023query2doc. These examples serve as a form of explicit instruction, allowing the model to adapt its language generation to specific tasks or domains. Query2Doc~wang2023query2doc prompts LLMs to write a document that answers the query with some demo query-document pairs provided by the ranking dataset, such as MSMARCO~MSMARCO and NQ~nq. This work experiments with a single prompt. To further study the impact of different prompt designing, recent works~jagerman2023query have explored eight different prompts, such as prompting LLMs to generate query expansion terms instead of entire pseudo documents and CoT prompting. Some illustrative prompts are shown in Table~tab:prompting_examples. The experiments validate that Query2Doc is more effective than many other prompt-based methods. $\\bullet$ Chain-of-thought prompting. CoT prompting~cot is a strategy that involves iterative prompting, where the model is provided with a sequence of instructions or partial outputs~sigirAlaofi,jagerman2023query,wu2025cotkrchainofthoughtenhancedknowledge,baek2024craftingpathrobustquery. In conversational search, the process of query rewriting is multi-turn, which means queries should be refined step-by-step with the interaction between search engines and users. This process naturally coincides with the CoT process. As shown in Figure~fig:query_rewrite_conv, users can conduct the CoT process by adding some instructions during each turn, such as ``Based on all previous turns, xxx''. While in ad-hoc search, there is only one round in query rewriting, so CoT could only be accomplished in a simple and coarse way. For example, as shown in Table~tab:prompting_examples, researchers add ``Give the rationale before answering'' in the instructions to prompt LLMs to think deeply~jagerman2023query.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Supervised Fine-tuning",
      "level": "3",
      "content": "Prompting methods directly leverage LLMs' strong capabilities to expand or rewrite queries. Though prompting method is effective, LLMs are not naturally designed for query rewriting task. To further tailor LLMs for this task, supervised fine-tuning (SFT) has emerged as a promising approach. A crucial aspect of this methodology is the creation of an appropriate training dataset. The process of gathering this dataset varies significantly depending on the application scenario. In the context of e-commerce search, a wealth of supervised training data for query rewriting is naturally available. This data, sourced from the previous-generation rewriting policies of the e-commerce system, significantly simplifies the construction of the SFT dataset~BEQUE. Conversely, in an ad-hoc retrieval scenario, the acquisition of query rewrite training data is often a challenge. To address this issue, researchers usually employ implicit feedback and reinforcement learning to train the query rewriter.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Reinforcement Learning",
      "level": "3",
      "content": "Query rewriters typically serve as intermediaries for retrieval systems, and as such, they lack a dedicated or independent loss function for optimization. In this context, reinforcement learning (RL) presents an alternative training paradigm. The query rewriter can receive feedback signals from donwstream components, such as ranking models~RaFe or LLM readers~ma2023query. For instance, ranking scores can be utilized to construct good-bad pairs for direct preference optimization~dpo training. Similarly, ma2023query propose to generate answers from LLMs and then uses the results of a QA evaluation as training signals. Another approach, BEQUE~BEQUE, introduces an offline feedback system that assigns a quality score to each query based on the set of products it retrieves. Recently, inspired by the rule-based reward system proposed by DeepSeek-R1~r1, some studies have explored using retrieval metrics as the reward to optimize query generators. For example, DeepRetrieval~jiang2025deepretrievalhackingrealsearch proposes a RL approach that trains LLMs for query generation by using retrieval metrics as rewards without the need for supervised data. This method reinforces the query generator to produce queries that maximize retrieval performance. These RL mechanisms align the objective of query rewriters more closely with the goals of downstream tasks, thereby enhancing the overall performance of the system.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Limitations",
      "level": "2",
      "content": "Despite the potential of LLMs in query rewriting, they still suffer from several limitations. We discuss two primary challenges that arise with their use in this context.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Concept Drifts",
      "level": "3",
      "content": "One significant issue is the introduction of unrelated information, or concept drift, which may occur due to the LLM's vast knowledge base and propensity to generate detailed yet sometimes redundant content. While this can potentially enrich the query, it may also lead to irrelevant or off-target results. This issue has been reported in various studies~An_Interactive_Query_Generation,CAR,BEQUE. These works emphasize the necessity for a balanced approach to LLM-based query rewriting, maintaining the core and focus of the original query while utilizing the LLM's capabilities to enhance and clarify it. This balance is critical for effective search and IR applications.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Correlation between Retrieval Performance and Expansion Effects",
      "level": "3",
      "content": "A recent comprehensive study~DBLP:journals/corr/when_do_generative has investigated various expansion techniques and downstream ranking models, revealing a significant negative correlation between retrieval performance and expansion benefits. Specifically, expansion tends to improve the scores of weaker models but adversely affects stronger ones. This finding suggests a strategic approach: only using expansions with weaker models or when the target dataset significantly varies in format from the training corpus. In other situations, it may be more beneficial to refrain from expansions to preserve the clarity of the relevance signal.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Retriever",
      "level": "1",
      "content": "In an IR system, the retriever serves as the first-pass document filter to collect broadly relevant documents for user queries. Given the enormous amounts of documents in an IR system, the retriever's efficiency in locating relevant documents is essential for maintaining search engine performance. Meanwhile, a high recall is also important for the retriever, as the retrieved documents are then fed into the ranker to generate final results for users, which determines the ranking quality of search engines. In recent years, retrieval models have shifted from relying on statistic algorithms~DBLP:conf/trec/RobertsonWJHG94 to neural models~ance,dpr. The latter approaches exhibit superior semantic capability and excel at understanding complicated user intent. The success of neural retrievers relies on two key factors: data and model. From the data perspective, a large amount of high-quality training data is essential. This enables retrievers to acquire comprehensive knowledge and accurate matching patterns. Furthermore, the intrinsic quality of search data, \\ie, issued queries and document corpus, significantly influences retrieval performance. From the model perspective, a strongly representational neural architecture allows retrievers to effectively store and apply knowledge obtained from the training data. Unfortunately, there are some long-term challenges that hinder the advancement of retrieval models. First, user queries are usually short and ambiguous, making it difficult to precisely understand the user's search intents for retrievers. Second, documents typically contain lengthy content and substantial noise, posing challenges in encoding long documents and extracting relevant information for retrieval models. Additionally, the collection of human-annotated relevance labels is time-consuming and costly. It restricts the retrievers' knowledge boundaries and their ability to generalize across different application domains. Moreover, existing model architectures, primarily built on BERT~bert, exhibit inherent limitations, thereby constraining the performance potential of retrievers. Recently, LLMs have exhibited extraordinary abilities in language understanding, text generation, and reasoning. This has motivated researchers to use these abilities to tackle the aforementioned challenges and aid in developing superior retrieval models. Roughly, these studies can be categorized into two groups: (1) leveraging LLMs to generate search data, and (2) employing LLMs to enhance model architecture.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Leveraging LLMs to Generate Search Data",
      "level": "2",
      "content": "In light of the quality and quantity of search data, there are two prevalent perspectives on how to improve retrieval performance via LLMs. The first perspective revolves around search data refinement methods, which concentrate on reformulating input queries to precisely present user intents. The second perspective involves training data augmentation methods, which leverage LLMs' generation ability to enlarge the training data for dense retrieval models, particularly in zero- or few-shot scenarios.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Search Data Refinement",
      "level": "3",
      "content": "Typically, input queries consist of short sentences or keyword-based phrases that may be ambiguous and contain multiple possible user intents. Accurately determining the specific user intent is essential in such cases. Moreover, documents usually contain redundant or noisy information, which poses a challenge for retrievers to extract relevance signals between queries and documents. Leveraging the strong text understanding and generation capabilities of LLMs offers a promising solution to these challenges. As yet, research efforts in this domain primarily concentrate on employing LLMs as query rewriters, aiming to refine input queries for more precise expressions of the user's search intent. Section~sec:qr has provided a comprehensive overview of these studies, so this section refrains from further elaboration. In addition to query rewriter, an intriguing avenue for exploration involves using LLMs to enhance the effectiveness of retrieval by refining lengthy documents. This intriguing area remains open for further investigation and advancement. figure* \\centering \\includegraphics[width=1\\linewidth]{fig/aug-framework2.pdf} Three typical frameworks for LLM-based data augmentation in the retrieval task (right), along with their prompt examples (left). Note that the methods of relevance label generation do not treat questions as inputs but regard their generation probabilities conditioned on the retrieved passages as soft relevance labels. figure* table*[t] \\centering The comparison of existing data augmentation methods powered by LLMs for training retrieval models. \\tabcolsep{1.7mm}{ tabular{lccccc} \\toprule Methods & \\# Examples & Generator & Synthetic Data & Filter Method & LLMs' tuning \\\\ \\midrule InPairs~InPairs & 3 & Curie & {Relevant query} & {Generation probability} & Fixed \\\\ Bottlenecked_Query_Gen & 0-2 & Alpaca-LLaMA \\& tk-Instruct & Relevant query & - & Fixed \\\\ InPairs-v2~InPairs-v2 & 3 & GPT-J & {Relevant query} & Relevance score from\\\\fine-tuned monoT5-3B & Fixed \\\\ PROMPTAGATOR~PROMPTAGATOR & 0-8 & FLAN & Relevant query & Round-trip filtering & Fixed \\\\ TQGen~TQGen & 0 & T0 & Relevant query & Generation probability & Fixed \\\\ UDAPDR~UDAPDR & 0-3 & GPT3 \\& FLAN-T5-XXL & Relevant query & Round-trip filtering & Fixed \\\\ SPTAR~SoftPrompt & 1-2 & LLaMA-7B \\& Vicuna-7B & Relevant query & BM25 filtering & Soft Prompt tuning \\\\ Gecko~Gecko & few-shot & Unkown & Relevant query & - & Fixed \\\\ ART~ART & 0 & T5-XL \\& T5-XXL & Soft relevance labels & - & Fixed \\\\ GENQD & 2 & GPT-4 & Q-PosD-NegD triplet & - & fixed \\\\ \\bottomrule tabular } table*",
      "origin_cites_number": 10
    },
    {
      "section_title": "Training Data Augmentation",
      "level": "3",
      "content": "Due to the expensive economic and time costs of human-annotated labels, a common problem in training neural retrieval models is the lack of training data. Fortunately, the excellent capability of LLMs in text generation offers a potential solution. A key research focus lies in devising strategies to leverage LLMs' capabilities to generate pseudo-relevant signals and augment the training dataset for the retrieval task. Why do we need data augmentation? Previous studies of neural retrieval models focused on supervised learning, namely training retrieval models using labeled data from specific domains. For example, MS MARCO~MSMARCO provides a vast repository, containing a million passages, more than 200,000 documents, and 100,000 queries with human-annotated relevance labels, which has greatly facilitated the development of supervised retrieval models. However, this paradigm inherently constrains the retriever's generalization ability for out-of-distribution data from other domains. The application spectrum of retrieval models varies from natural question-answering to biomedical IR, and it is expensive to annotate relevance labels for data from different domains. As a result, there is an emerging need for zero-shot and few-shot learning models to address this problem~BEIR. A common practice to improve the models' effectiveness in a target domain without adequate label signals is through data augmentation. How to apply LLMs for data augmentation? In the scenario of IR, it is easy to collect numerous documents. However, the challenging and costly task lies in gathering real user queries and labeling the relevant documents accordingly. Considering the strong text generation capability of LLMs, many researchers~InPairs,InPairs-v2 suggest using LLM-driven processes to create pseudo queries or relevance labels based on existing collections. These approaches facilitate the construction of relevant query-document pairs, enlarging the training data for retrieval models. According to the type of generated data, there are three mainstream approaches that complement the LLM-based data augmentation for retrieval models, \\ie, pseudo query generation, relevance label generation, and complete example generation. Their frameworks are visualized in Figure~fig:aug-frame. $\\bullet$ Pseudo query generation. Given the abundance of documents, a straightforward idea is to use LLMs for generating their corresponding pseudo queries. One such illustration is presented by inPairs~InPairs, which leverages the in-context learning capability of GPT-3. This method employs a collection of query-document pairs as demonstrations. These pairs are combined with a document and presented as input to GPT-3, which subsequently generates possible relevant queries for the given document. By combining the same demonstration with various documents, it is easy to create a vast pool of synthetic training samples and support the fine-tuning of retrievers on specific target domains. To enhance the diversity of generated examples, Gecko~Gecko prompts LLMs to first generate a task description, and then generate pseudo queries according to the task. Considering the false negative problems, it further develops an LLM-based positive and negative mining strategy to discover potential relevant and hard negative documents from the corpus for generated queries, which significantly enhance the retrieval performance. Recent studies~Bottlenecked_Query_Gen have also leveraged open-sourced LLMs, such as Alpaca-LLaMA and tk-Instruct, to produce sufficient pseudo queries and applied curriculum learning to pre-train dense retrievers. To enhance the reliability of these synthetic samples, a fine-tuned model (\\eg, a monoT5-3B model fine-tuned on MSMARCO~InPairs-v2) is employed to filter the generated queries. Only the top pairs with the highest estimated relevance scores are kept for training. This ``generating-then-filtering'' paradigm can be conducted iteratively in a round-trip filtering manner, \\ie, by first fine-tuning a retriever on the generated samples and then filtering the generated samples using this retriever. Repeating these EM-like steps until convergence can produce high-quality training sets~PROMPTAGATOR. Furthermore, by adjusting the prompt given to LLMs, they can generate queries of different types. This capability allows for a more accurate simulation of real queries with various patterns~TQGen. In practice, it is costly to generate a substantial number of pseudo queries through LLMs. Balancing the generation costs and the quality of generated samples has become an urgent problem. To tackle this, UDAPDR~UDAPDR is proposed, which first produces a limited set of synthetic queries using LLMs for the target domain. These high-quality examples are subsequently used as prompts for a smaller model to generate a large number of queries, thereby constructing the training set for that specific domain. It is worth noting that the aforementioned studies primarily rely on fixed LLMs with frozen parameters. Empirically, optimizing LLMs' parameters can significantly improve their performance on downstream tasks. Unfortunately, this pursuit is impeded by the prohibitively high demand for computational resources. To overcome this obstacle, SPTAR~SoftPrompt introduces a soft prompt tuning technique that only optimizes the prompts' embedding layer during the training process. This approach allows LLMs to better adapt to the task of generating pseudo-queries, striking a favorable balance between training cost and generation quality. In addition to the above studies, pseudo query generation methods are also introduced in other application scenarios, such as conversational dense retrieval~Converser and multilingual dense retrieval~Multiling_DR. $\\bullet$ Relevance label generation. In some downstream tasks of retrieval, such as question-answering, the collection of questions is also sufficient. However, the relevance labels connecting these questions with the passages of supporting evidence are very limited. In this context, leveraging the capability of LLMs for relevance label generation is a promising approach that can augment the training corpus for retrievers. A recent method, ART~ART, exemplifies this approach. It first retrieves the top-relevant passages for each question. Then, it employs an LLM to produce the generation probabilities of the question conditioned on these top passages. After a normalization process, these probabilities serve as soft relevance labels for the training of the retriever. $\\bullet$ Complete example generation. Recent study~GENQD further investigates approaches to directly utilize LLMs to generate synthetic queries and documents, hence providing tremendous diverse training examples across varied tasks and languages. This work proposes a two-stage generation pipeline, where the first stage prompts the LLM to brainstorm various retrieval tasks, and then the second stage generates corresponding ``(query, positive document, negative document)'' triplets to build synthetic training data. Researchers further control the length, languages, and semantic relationships of queries and documents to produce diverse training samples. The generated triplets, after post-processing, \\eg, de-duplication and JSON-format filtering, are used as training examples for optimizing dense retrievers. Additionally, to highlight the similarities and differences among the corresponding methods, we present a comparative result in Table~tab:retrieve_dataaug_comparison. It compares the aforementioned methods from various perspectives, including the number of examples, the generator employed, the type of synthetic data produced, the method applied to filter synthetic data, and whether LLMs are fine-tuned. This table serves to facilitate a clearer understanding of the landscape of these methods.",
      "origin_cites_number": 15
    },
    {
      "section_title": "Leveraging LLMs as Retrievers' Backbone",
      "level": "2",
      "content": "Thanks to the superior text representation capability, LLMs excel at comprehending the underlying semantics of queries and documents. Therefore, it becomes increasingly popular to apply such large-scale models as the backbone of text retrievers, leading to substantial improvements over the conventional methods based on smaller-sized models~bert.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Dense Retriever",
      "level": "3",
      "content": "The application of LLMs brings about two major impacts on dense retrieval. On one hand, it advances the ongoing progress of the existing methods, making substantial improvements in terms of both in-domain accuracy and out-of-domain generalizability. On the other hand, it extends the boundaries of current methods, introducing new capabilities such as instruction following and in-context learning. Improved Existing Capacities. Dense retrieval needs to fine-tune pre-trained text encoders such that queries and documents can be transformed into semantic-rich embeddings. Therefore, the downstream retrieval performance can benefit from the utilization of stronger foundations. With preliminary progresses achieved by early forms of pre-trained models (\\eg, BERT~bert and T5~t5), people moved on to take advantage of LLMs for dense retrieval. In this direction, a pioneer work is made by OpenAI, where a series of GPT models were fine-tuned towards text and code representation~OpenAI-TextEmb. It is the first time where decoding-only transformers were effectively applied for dense retrieval. Importantly, it empirically validates that the retrieval performance can be consistently improved from the increased model size and embedding dimension. Besides, the LLM-based retrievers also exhibit superior generalizability~Large-T5-Retriever,muennighoff2022sgpt, as notable improvements can be achieved for not only the targeted scenario but also a variety of general tasks beyond the fine-tuned domain. Recently, the development of LLM-based dense retrievers have gotten dramatically promoted as powerful LLMs, \\eg, LLaMA~llama, Vicuna~vicuna, Mistral~mistral, Phi~phi, and Gemma~gemma, are made publicly available. Remarkably, RepLLaMA~rankllama, the first fine-tuned embedder on top of open-source LLM (LLaMA-2-7B), brings forth major improvements on a variety of benchmarks, including MSMARCO passage/doc retrieval~MSMARCO and BEIR~BEIR. Despite extra computation costs due to the expanded model scale, the first-stage retrieval accuracy with RepLLaMA alone already surpasses the multi-stage retrieval accuracy resulted from conventional methods, indicating the its potential value for real-world application. After that, people make exploration of other alternatives for dense retrieval, where additional improvements are continually achieved with adoption of more advanced LLMs~GENQD,SFRAIResearch2024,LinqAIResearch2024. To date, LLM-based embedders have dominated all major text retrieval benchmarks, \\eg, currently, the leading methods on MTEB~muennighoff2022mteb are all back-ended by LLMs. In addition to the above methods which directly fine-tune LLMs, there are also parallel works on adapting generic LLMs as better foundations for dense retrieval. For example, Llama2Vec li2023making performs post pre-training of LLaMA-2 with two new pretext tasks: EBAE (embedding based auto-encoding) and EBAR (embedding based auto-regression). With moderate scale of training on unlabeled corpus, it results in substantial improvements of retrieval performance over the basic Llama-2 model. Besides, NV-Embed modifies LLM's architecture by introducing latent attention layer and bidirectional attention~lee2024nv. Both modifications contribute to the improved performance on MTEB benchmark. Despite the above preliminary progresses, there are still many open challenges about LLM-based embedders, such as efficiency and adaptability, which need to be addressed in the future. Introducing New Capacities. Compared to conventional methods that use small-scale pre-trained models, LLM-based embedders introduce new capabilities that enhance the usability and accuracy of dense retrieval. One notable example is their ability to follow instructions, allowing LLM-based embedders to be trained for various semantic matching tasks based on user demands. For instance, an LLM-based embedder can perform document retrieval when prompted with ``retrieve relevant docs for the input question'', and can be adapted for duplicate question retrieval with the prompt ``retrieve questions with the same meaning as input''. Although BERT-based retrievers are also fine-tuned to follow instructions~su2022one,Task-aware-Instruction, they do not support unseen instructions as effectively as LLM-based embedders~LLM4DR. ChatRetriever~ChatRetriever further leverages dialog-based instruction tuning to build LLM-based conversational embedders, enhancing their conversational retrieval capabilities. In addition to instructions, the LLM-based embedders can also be adapted through in-context learning, where the retrieval function can be updated by demonstration examples of user's interested tasks~jiang2023scaling. Another advantage of LLM-based embedders is their length-generalizable capacity, which allows them to effectively handle much longer texts than those in their training examples~LLM4DR. This makes it possible to manage retrieval applications across various text lengths while maintaining a feasible training cost.",
      "origin_cites_number": 21
    },
    {
      "section_title": "Generative Retriever",
      "level": "3",
      "content": "Traditional IR systems typically follow the ``index-retrieval-rank'' paradigm to locate relevant documents based on user queries. Though, this approach has proven its effective in practice, it usually consist of three separate modules: the index module, the retrieval module, and the reranking module. Optimizing these modules collectively can be challenging, potentially resulting in sub-optimal retrieval outcomes. Additionally, this paradigm demands additional storage space for pre-built indexes, further burdening storage resources. Recently, generative model-based retrieval methods~model-based-ir-1, model-based-ir-2, model-based-ir-3 have emerged to address these challenges. These methods move away from the traditional ``index-retrieval-rank'' paradigm and instead use a unified model to directly generate document identifiers (\\ie, DocIDs) relevant to the queries. In these generative retrieval methods, the knowledge of the document corpus is stored in the model parameters, eliminating the need for additional storage space for a separate index. Existing works have investigated the approaches of generating document identifiers through fine-tuning and prompting of LLMs~DSI,LLMurl Fine-tuning LLMs. Given the vast amount of world knowledge contained in LLMs, it is intuitive to leverage them to build generative retrievers. DSI~DSI is a typical method that fine-tunes the pre-trained T5 models on retrieval datasets. The approach involves encoding queries and decoding document identifiers directly to perform retrieval. They explore multiple techniques for generating document identifiers and find that constructing semantically structured identifiers yields optimal results. In this strategy, DSI applies hierarchical clustering to group documents according to their semantic embeddings and assigns a semantic DocID to each document based on its hierarchical group. To ensure the output DocIDs are valid and do represent actual documents in the corpus, DSI constructs a trie using all DocIDs and utilizes a constraint beam search during the decoding process. Furthermore, this approach observes that the scaling law, which suggests that larger LMs lead to improved performance, is also applied to generative retrievers. Though various generative retrievers have been proposed~DSI,NCI,ARGDI, most of them mainly focus on fine-tuning size-limited LMs on small-size document corpus (usually a subset of MSMARCO~MSMARCO). To analyze how the model size and document-corpus size impact the effectiveness of generative retrievers, GD-analyze conducted a comprehensive analysis by scaling up corpus size from 100k to 8.8M and scaling model size up to 11B (T5-XXL). The primary findings are three-fold: (1) It is still challenging for generative retrievers to cover large-scale document corpus. (2) More model parameters often bring better performance. (3) Introducing synthetic queries generated from documents to expand training samples could significantly enhance the retrieval performance. CorpusLM~CorpusLM further explores combining the retrieval and answering tasks together based on LLMs, making the two mutually reinforcing. Researchers devise various training tasks, \\eg, DocID list generation, closed-book answers generation, and RAG generation, to sufficiently leverage and enhance the world knowledge of LLMs, improving the performance of these generation tasks. Prompting LLMs. In addition to fine-tuning LLMs for retrieval, it has been found that LLMs (\\eg, GPT-series models) can directly generate relevant web URLs for user queries with a few in-context demonstrations~LLMurl. This unique capability of LLMs is believed to arise from their training exposure to various HTML resources. As a result, LLMs can naturally serve as generative retrievers that directly generate document identifiers to retrieve relevant documents for input queries. To achieve this, an LLM-URL~LLMurl model is proposed. It utilizes the GPT-3 text-davinci-003 model to yield candidate URLs. Furthermore, it designs regular expressions to extract valid URLs from these candidates to locate the retrieved documents. To provide a comprehensive understanding of this topic, Table~tab:retrieve_model_comparison summarizes the common and unique characteristics of the LLM-based retrievers discussed above.",
      "origin_cites_number": 17
    },
    {
      "section_title": "Limitations",
      "level": "2",
      "content": "Though some efforts have been made for LLM-augmented retrieval, there are still many areas that require more detailed investigation. For example, a critical requirement for retrievers is fast response, while the main problem of existing LLMs is the huge model parameters and overlong inference time. Addressing this limitation of LLMs to ensure the response time of retrievers is a critical task. Moreover, even when employing LLMs to augment datasets (a context with lower inference time demands), the potential mismatch between LLM-generated texts and real user queries could impact retrieval effectiveness. Furthermore, as LLMs usually lack domain-specific knowledge, they need to be fine-tuned on task-specific datasets before applying them to downstream tasks. Therefore, developing efficient strategies to fine-tune these LLMs with numerous parameters emerges as a key concern.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Reranker",
      "level": "1",
      "content": "Reranker, as the second-pass document filter in IR, aims to rerank a document list retrieved by the retriever (\\eg, BM25) based on the query-document relevance. According to the usage of LLMs, existing LLM-based reranking methods can be divided into four paradigms: utilizing LLMs as supervised rerankers, utilizing LLMs as unsupervised rerankers, utilizing LLMs for training data augmentation and reasoning-intensive rerankers. These paradigms are summarized in Table~tab:reranker and will be introduced in the following sections. Recall that we will use the term document to refer to the text retrieved in general IR scenarios, including instances such as passages (\\eg, passages in MS MARCO passage ranking dataset~MSMARCO).",
      "origin_cites_number": 47
    },
    {
      "section_title": "Utilizing LLMs as Supervised Rerankers",
      "level": "2",
      "content": "Supervised fine-tuning is an important step in applying pre-trained LLMs to a reranking task. Due to the lack of awareness of ranking during pre-training, LLMs cannot appropriately measure the query-document relevance and fully understand the reranking tasks. By fine-tuning LLMs on task-specific ranking datasets, such as the MS MARCO passage ranking dataset~MSMARCO, which includes signals of both relevance and irrelevance, LLMs can adjust their parameters to yield better performance in the reranking tasks. Based on the backbone model structure, we can categorize existing supervised rerankers as: (1) encoder-only, (2) encoder-decoder, and (3) decoder-only.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Encoder-only",
      "level": "3",
      "content": "The encoder-based rerankers represent a significant turning point in applying LLMs to document reranking tasks. They demonstrate how some pre-trained language models~(\\eg, BERT~bert) can be fine-tuned to deliver highly accurate relevance predictions. A representative approach is monoBERT~monobert, which transforms a query-document pair into a sequence ``[CLS] {query} [SEP] {document} [SEP]'' as the model input and calculates the relevance score by feeding the ``[CLS]'' representation into a linear layer. The reranking model is optimized based on the cross-entropy loss.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Encoder-Decoder",
      "level": "3",
      "content": "In this field, existing studies mainly formulate document reranking as a generation task and optimize an encoder-decoder-based reranking model~monot5, JuYW21, Expando-Mono-Duo, RankT5. Specifically, given the query and the document, reranking models are usually fine-tuned to generate a single token, such as ``true'' or ``false''. During inference, the query-document relevance score is determined based on the logit of the generated token. For example, a T5 model can be fine-tuned to generate classification tokens for relevant or irrelevant query-document pairs~monot5. At inference time, a softmax function is applied to the logits of ``true'' and ``false'' tokens, and the relevance score is calculated as the probability of the ``true'' token. The following method~JuYW21 involves a multi-view learning approach based on the T5 model. This approach simultaneously considers two tasks: generating classification tokens for a given query-document pair and generating the corresponding query conditioned on the provided document. DuoT5~Expando-Mono-Duo considers a triple $(q, d_i, d_j)$ as the input of the T5 model and is fine-tuned to generate token ``true'' if document $d_i$ is more relevant to query $q_i$ than document $d_j$, and ``false'' otherwise. During inference, for each document $d_i$, it enumerates all other documents $d_j$ and uses global aggregation functions to generate the relevance score $s_i$ for document $d_i$ (\\eg, $s_i = \\sum_j p_{i,j}$, where $p_{i,j}$ represents the probability of generating ``true'' when taking $(q, d_i, d_j)$ as the model input). Beyond the aforementioned studies, several studies have also explored different training losses and model architectures for reranker training. For example, RankT5~RankT5 is proposed to directly yield a numerical relevance score for each query-document pair during training and optimize the ranking performance with ``pairwise'' or ``listwise'' ranking losses. It differs significantly from the previous studies that optimize rerankers by generating text tokens and using a generation loss, and the use of ranking loss (\\eg, RankNet~RankNet) is more reasonable and aligns better with the inherent nature of the ranking task. Besides, ~listT5 propose ListT5, a listwise reranker based on Fusion-in-decoder architecture. It jointly takes multiple documents as input and directly generates a reranked document list during training and inference.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Decoder-only",
      "level": "3",
      "content": "Recently, there have been some attempts~rankllama, TSARankLLM, Q-PEFT, Rank-without-GPT, PE-Rank to rerank documents by fine-tuning decoder-only models (such as LLaMA). For example, RankLLaMA~rankllama proposes formatting the query-document pair into a prompt ``query: \\{query\\} document: \\{document\\} [EOS]'' and utilizes the last token representation for relevance calculation. Besides, TSARankLLM~TSARankLLM has been proposed to bridge the gap between LLMs' conventional training objectives and the specific needs of document reranking through two-stage training. The first stage involves continuously pretraining LLMs using a large number of relevant text pairs collected from web resources, helping the LLMs to naturally generate queries relevant to the input document. The second stage focuses on improving the model's text ranking performance using high-quality supervised data and well-designed loss functions. ~Q-PEFT propose a query-dependent parameter efficient fine-tuning (Q-PEFT) approach for ranking, which helps the LLM generate true queries based on given documents. Different from these pointwise rerankers~rankllama, TSARankLLM, Q-PEFT, ~Rank-without-GPT and ~PE-Rank proposes to train a listwise reranker that directly outputs a reranked document list. Specifically, ~Rank-without-GPT first demonstrate that existing pointwise datasets (such as MS MARCO~MSMARCO), which only contain binary query-document labels, are insufficient for training efficient listwise rerankers. Then, they propose to use the ranking results of existing ranking systems (such as Cohere rerank API) as gold rankings to train a listwise reranker based on Code-LLaMA-Instruct. ~PE-Rank propose PE-Rank, which compresses each document in the list into a single embedding and then inputs these document embeddings into reranker, which significantly reduces the input length and improves the efficiency of reranker. figure*[t] \\centering \\includegraphics[width=.8\\linewidth]{fig/reranking.pdf} Three types of unsupervised reranking methods: (a) pointwise methods that consist of relevance generation (upper) and query generation (lower), (b) listwise methods, and (c) pairwise methods. The ``Demonstrations'' represents the few-shot demonstrations whose format are same as the current input. figure*",
      "origin_cites_number": 10
    },
    {
      "section_title": "Utilizing LLMs as Unsupervised Rerankers",
      "level": "2",
      "content": "As the size of LLMs scales up (\\eg, exceeding 10 billion parameters), it becomes increasingly difficult to fine-tune the reranking model. Addressing this challenge, recent efforts have attempted to prompt LLMs to directly enhance document reranking in an unsupervised way. In general, these prompting strategies can be divided into three categories: pointwise, listwise, and pairwise methods. A comprehensive exploration of these strategies follows in the subsequent sections.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pointwise methods",
      "level": "3",
      "content": "The pointwise methods measure the relevance between a query and a single document, and can be categorized into two types: relevance generation~DemoRank, relevance_generation, beyond_yes_no, Diverse_Criteria and query generation~query_generation, opensource, Co-Prompt. The upper part in Figure~fig:reranking (a) shows an example of relevance generation based on a given prompt, where LLMs output a binary label (``Yes'' or ``No'') based on whether the document is relevant to the query. Following~monot5, the query-document relevance score $f(q, d)$ can be calculated based on the log-likelihood of token ``Yes'' and ``No'' with a softmax function: equation f(q, d) = \\text{exp(S_{Y})}{exp(S_{Y}) + exp(S_{N})}, equation where $S_{Y}$ and $S_{N}$ represent the LLMs log-likelihood scores of ``Yes'' and ``No'' respectively. In addition to binary labels, beyond_yes_no propose to incorporate fine-grained relevance labels (\\eg, ``highly relevant'', ``somewhat relevant'' and ``not relevant'') into the prompt, which helps LLMs more effectively differentiate among documents with varying levels of relevance to a query. ~Diverse_Criteria discuss the issues of inconsistent and biased relevance assessments of existing pointwise rerankers and introduce MCRanker that generates relevance scores based on a series of criteria from multiple perspectives. As for the query generation shown in the lower part of Figure~fig:reranking (a), the query-document relevance score is determined by the average log-likelihood of generating the actual query tokens based on the document: equation score = 1{|q|} \\sum_i \\log p(q_i | q_{<i}, d, P), equation where $|q|$ denotes the token number of query $q$, $d$ denotes the document, and $P$ represents the provided prompt. The documents are then reranked based on their relevance scores. It has been proven that some LLMs (such as T0) yield significant performance in zero-shot document reranking based on the query generation method~query_generation. Recently, research~opensource has also shown that the LLMs that are pre-trained without any supervised instruction fine-tuning (such as LLaMA) also yield robust zero-shot ranking ability. Although effective, these methods primarily rely on a handcrafted prompt (\\eg, ``Please write a query based on this document''), which may not be optimal. Previous study~Prompt_Variations has shown that prompt has a significant impact on the performance of LLM reranker. Thus, how to design appropriate prompts for ranking task is an important problem. Along this line, a discrete prompt optimization method Co-Prompt~Co-Prompt is proposed for better prompt generation in reranking tasks. Besides, PaRaDe~parade proposes a difficulty-based method to select the most difficult $k$ in-context demonstrations to include in the prompt, proving improvements compared with zero-shot performance. Nevertheless, the experiments in the paper indicate that such difficulty-based selection does not even show a significant advantage compared to random selection, showing that the demonstration selection in ranking task is a very challenging problem. The main challenge lies in the complex nature of query-document relationship, which requires effectively combining multiple demonstrations to help the LLM understand such relationship. Aiming to select more effective demonstrations for ranking task, ~DemoRank propose DemoRank, an effective demonstration selection framework. The core component of DemoRank is a dependency-aware demonstration reranker, which reranks a list of demonstrations (usually obtained by a demonstration retriever) so that the combination of top-ranked demonstrations can yield better performance. An efficient method is proposed to construct the training samples for such demonstration reranker and a novel list-pairwise training loss is designed for optimization. table*[t] \\centering \\small The comparison between different LLM-based reranking methods. $N$ denotes the number of documents to rerank. The ``complexity'', ``logits'', and ``batch'' represent the computational complexity, whether accesses LLM's logits, and whether allows batch inference respectively. $k$ is the constant in sliding windows strategy. As for the performance, we use NDCG@10 as a metric, and the results are calculated by reranking the top-100 documents retrieved by BM25 on TREC-DL2019 and TREC-DL2020. The best model is in bold while the second-best is underlined. The results come from previous study~\\cite{qin-etal-2024-large. *Since the parameters of ChatGPT have not been released, its model parameters are based on public estimates~baktash2023gpt.} \\tabcolsep{1.4mm}{ tabular{llllccccc} \\toprule 2{*}{} & 2{*}{Method} & 2{*}{LLM} & 2{*}{Size} & 3{c}{Properties} & 2{c}{Performance} \\\\ \\cmidrule(lr){5-7} \\cmidrule(lr){8-9} & & & & Complexity & Logits & Batching & TREC-DL19 & -DL20 \\\\ \\midrule Initial Retriever & BM25 & - & - & - & - & - & 50.58 & 47.96 \\\\ \\midrule 3{*}{Supervised} & monoBERT~monobert & BERT & 340M & - & $\\checkmark$ & $\\checkmark$ & 70.50 & 67.28 \\\\ & monoT5~monot5 & T5 & 220M & - & $\\checkmark$ & $\\checkmark$ & 71.48 & 66.99 \\\\ & RankT5~RankT5 & T5 & 3B & - & $\\checkmark$ & $\\checkmark$ & 71.22 & 69.49 \\\\ \\midrule 2{*}{\\makecell[l]{Unsupervised \\\\ Pointwise}} & Query Generation~query_generation & FLAN-UL2 & 20B & $O(N)$ & $\\checkmark$ & $\\checkmark$ & 58.95 & 60.02 \\\\ & Relevance Generation~relevance_generation & FLAN-UL2 & 20B & $O(N)$ & $\\checkmark$ & $\\checkmark$ & 64.61 & 65.39 \\\\ 2{*}{\\makecell[l]{Unsupervised \\\\ Listwise}} & RankGPT$_{3.5}$~sun2023chatgpt & ChatGPT & 154B* & $O(k*N)$ & & & 65.80 & 62.91 \\\\ & RankGPT$_4$~sun2023chatgpt & GPT-4 & 1T* & $O(k*N)$ & & & 75.59 & 70.56 \\\\ 2{*}{\\makecell[l]{Unsupervised \\\\ Pairwise}} & PRP-Allpair~qin-etal-2024-large & FLAN-UL2 & 20B & $O(N^2)$ & $\\checkmark$ & $\\checkmark$ & 72.42 & 70.68 \\\\ & PRP-Heapsort~qin-etal-2024-large & FLAN-UL2 & 20B & $O(N*logN)$ & $\\checkmark$ & & 71.88 & 69.43 \\\\ \\bottomrule tabular} table*",
      "origin_cites_number": 22
    },
    {
      "section_title": "Listwise Methods",
      "level": "3",
      "content": "Listwise methods~sun2023chatgpt, ma2023zero aim to directly rank a list of documents (see Figure~fig:reranking (b)). These methods insert the query and a document list into the prompt and instruct the LLMs to output the reranked document identifiers. Due to the limited input length of LLMs, it is not feasible to insert all candidate documents into the prompt. To alleviate this issue, these methods employ a sliding window strategy to rerank a subset of candidate documents each time. This strategy involves ranking from back to front using a sliding window, re-ranking only the documents within the window at a time. Although listwise methods have yielded promising performance, they still suffer from some weaknesses: (1) The performance of listwise methods is highly sensitive to the document order in the prompt. When the document order is randomly shuffled, listwise methods perform even worse than BM25~sun2023chatgpt, revealing positional bias issues in the listwise ranking of LLMs. (2) The use of sliding windows limits the number of documents that can be ranked each time, and the dependency between adjacent windows prevents parallelization of LLM inference, thereby reducing the efficiency of reranking. Recently, some studies have attempted to mitigate these issues. self_consistency_llm introduce a permutation self-consistency method, which involves shuffling the list in the prompt and aggregating the generated results to achieve a more accurate and a positionally unbiased ranking. ~TourRank introduce a tournament mechanism into listwise ranking and propose TourRank, which parallelizes the reranking process through intelligent grouping and use a tournament-like points system to reduce the impact of the initial document order. ~Top-Down propose a parallelizable partitioning algorithm for listwise ranking, which also aims at mitigating efficiency issues. ~FIRST propose a novel listwise reranking approach which leverages the output logits of the first generated identifier to accelerating reranking process. To optimize the listwise reranking prompt, ~apeer propose a novel automatic prompt engineering algorithm APEER, which generates prompts through feedback and preference optimization. ~fullrank comprehensively discuss the benefits of long-context LLMs for listwise ranking and introduce a novel full reranker which performs better than the sliding window reranker while also being more efficient and having lower API cost. ~coranking propose a collaborative ranking framework CoRanking which combines small and large listwise rerankers for more efficient and effective passage ranking. They also design a novel passage order adjuster to mitigate the sensitivity of listwise reranker to the input document order.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Pairwise Methods",
      "level": "3",
      "content": "In pairwise methods, LLMs are given a prompt that consists of a query and a document pair (see Figure~fig:reranking (c)). Then, they are instructed to generate the identifier of the document with higher relevance. To rerank all candidate documents, aggregation methods like AllPairs are used~qin-etal-2024-large. AllPairs first generates all possible document pairs, yields discrete judgments for each pair (\\eg, Document 1 or Document 2), and aggregates a final relevance score for each document. Efficient sorting algorithms, such as heap sort and bubble sort, are employed to speed up the ranking process. These sorting algorithms utilize efficient data structures to compare document pairs selectively and elevate the most relevant documents to the top of the ranking list, which is particularly useful in top-$k$ ranking. Experimental results show the state-of-the-art performance on the standard benchmarks using moderate-size LLMs (\\eg, Flan-UL2 with 20B parameters), which are much smaller than those typically employed in listwise methods (\\eg, GPT3.5). Building on the pairwise prompting approach, several ranking method variants have been proposed. ~PRP-Graph introduce an innovative scoring unit that leverages the generation probability of judgments instead of discrete judgments, and further design a graph-based aggregation approach to obtain a final relevance score for each document. ~SinhababuPGSM24 and propose to utilize few-shot in-context demonstrations to improve the performance of pairwise ranking. ~Post-Processing utilize the pairwise comparison as a post-processing step to adjust the relevance scores generated by the pointwise LLM reranker. Although effective, pairwise methods still suffer from high time complexity. To alleviate the efficiency problem, a setwise approach~setwise has been proposed to compare a set of documents at a time and select the most relevant one from them. This approach allows the sorting algorithms (such as heap sort) to compare more than two documents at each step, thereby reducing the total number of comparisons and speeding up the sorting process.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Comparison and Discussion",
      "level": "3",
      "content": "We compare different unsupervised methods from various aspects to better illustrate the strengths and weaknesses of each method, which is summarized in Table~tab:reranker_comparison. The representative methods~query_generation, relevance_generation, sun2023chatgpt, qin-etal-2024-large in pointwise, listwise, and pairwise ranking, and some supervised methods~monobert, monot5, RankT5 mentioned in Section~sec:supervised are selected for performance comparison. The pointwise methods (query generation and relevance generation) judge the relevance of each query-document pair independently, thus offering lower time complexity and enabling batch inference. However, compared to other methods, it does not have an advantage in terms of performance. The listwise method yields significant performance especially when calling GPT-4, but suffers from expensive API cost and non-reproducibility~rankvicuna. Compared with the listwise method, the pairwise method shows competitive results based on a much smaller model FLAN-UL2 (20B). Stemming from the necessity to compare an extensive number of document pairs, its primary drawback is low efficiency.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Utilizing LLMs for Training Data Augmentation",
      "level": "2",
      "content": "Furthermore, in the realm of reranking, researchers have explored the integration of LLMs for training data augmentation~ExaRanker, InPars-Light, askari2023generating, rankvicuna, rankzephyr, sun2023instruction. For example, ExaRanker~ExaRanker and ExaRanker-Open~ExaRanker-Open generate explanations for query-passage pairs using GPT-3.5 and open-source LLMs respectively, and subsequently trains a seq2seq ranking model to generate relevance labels along with corresponding explanations. InPars-Light~InPars-Light is proposed as a cost-effective method to synthesize queries for documents by prompting LLMs. ~askari2023generating proposes to generate synthetic documents based on LLMs in response to user queries. Furthermore, ~RL-driven propose to utilize reinforcement learning to improve the quality of synthetic documents generated by LLMs. Recently, many studies~rankvicuna, rankzephyr, sun2023instruction have also attempted to distill the document reranking capability of LLMs into a specialized model. RankVicuna~rankvicuna proposes to use the ranking list of $RankGPT_{3.5}$~sun2023chatgpt as the gold list to train a 7B parameter Vicuna model. RankZephyr~rankzephyr introduces a two-stage training strategy for distillation: initially applying the RankVicuna recipe to train $Zephyr{\\gamma}$ in the first stage, and then further finetuning it in the second stage with the ranking results from $RankGPT_{4}$. These two studies not only demonstrate competitive results but also alleviate the issue of ranking results non-reproducibility of black-box LLMs. Besides, researchers~sun2023instruction have also tried to distill the ranking ability of a pairwise ranker, which is computationally demanding, into a simpler but more efficient pointwise ranker.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Reasoning-intensive Rerankers",
      "level": "2",
      "content": "Recent breakthroughs in Large Reasoning Models (LRMs) like DeepSeek-R1~r1 have demonstrated exceptional capabilities across many NLP tasks. These models significantly improve the answer accuracy in many complex NLP tasks (\\eg, math and coding) through explicit step-by-step reasoning chains during inference. This capability holds particular promise for document reranking, where precise understanding of query intent and cross-document comparison are critical for relevance assessment. Motivated by these advancements, emerging research has explored injecting reasoning ability into document rerankers. For example, ~rank1 and ~rank-k propose to apply DeepSeek-R1 as a teacher model to distill its reasoning process into smaller rerankers. ~rearank and ~rank-r1 propose to use reinforcement learning algorithm to optimize reranker based on rule-based reward. TFRank~tfrank introduces a ``think-free'' pointwise ranker that leverages reasoning during training while eliminating intermediate reasoning steps at inference, significantly improving the reasoning efficiency. While effective, these rerankers are primarily trained on traditional web search data MSMARCO, making them difficult to generalize to many complex and reasoning-intensive ranking benchmarks~bright. To address the scarcity of reasoning-intensive training data, reasonrank propose an automated data synthesis framework and generate 13K high-quality reasoning-intensive training data covering diverse search scenarios. They further propose a two-stage ``SFT+RL'' training framework, to empower LLM with strong reasoning and ranking abilities. Their ReasonRank model has achieved state-of-the-art performance on many reasoning-intensive IR benchmarks such as BRIGHT~bright and R2MED~r2med.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Limitations",
      "level": "2",
      "content": "Although recent research on utilizing LLMs for document reranking has made significant progress, it still has some limitations. First, due to the reliance on API calls and a large number of parameters, the process of LLM ranking could be expensive and inefficient. Therefore, achieving a trade-off between the cost/efficiency and performance of LLMs is a topic worth discussing. Along this line, ~ecorank propose a budget-aware ranking solution which maximizes the LLM's performance within a given budget. Notably, ChenGS25 introduce in-context re-ranking (ICR), an attention-based method that achieves superior efficiency by eliminating generative overhead through O(1) forward passes. Besides, ~MengAAAR24 systematically discuss the improvements in ranking efficiency and effectiveness brought by the rank list truncation technique. Second, while existing studies mainly focus on applying LLMs to open-domain datasets (such as MSMARCO~MSMARCO) or relevance-based text ranking tasks, their adaptability to in-domain datasets~BEIR, non-standard ranking datasets~WachsmuthSS18 and reasoning-intensive datasets~bright remains an area that demands more comprehensive exploration.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Reader",
      "level": "1",
      "content": "table*[t] \\centering The comparison of existing representative methods that have a passive reader module. REALM and RAG do not use LLMs, but their frameworks have been widely applied in many following approaches. \\tabcolsep{1.6mm}{ tabular{lcccc} \\toprule Methods & Backbone models & Where to incorporate retrieval & When to retrieve & How to use LLMs \\\\ \\midrule REALM~REALM & BERT & Input layer & In the beginning & Fine-tuning \\\\ RAG~RAG & BART & Input layer & In the beginning & Fine-tuning \\\\ REPLUG~REPLUG & GPT & Input layer & In the beginning & Fine-tuning\\\\ Atlas~Atlas & T5 & Input layer & In the beginning & Fine-tuning \\\\ RAG1 & Gopher & Input layer & In the beginning & Prompting \\\\ RAG2 & GPT & Input layer & In the beginning & Prompting \\\\ Chain-of-Note~chain-of-note & LLaMA & Input layer & In the beginning & Fine-tuning \\\\ RALM~RALM & LLaMA \\& OPT \\& GPT & Input layer & During generation (every $n$ tokens) & Prompting \\\\ RETRO~RETRO & Transformer & Attention layer & During generation (every $n$ tokens) & Training from scratch \\\\ ITERGEN~itergen2 & GPT & Input layer & During generation (every answer) & Prompting\\\\ IRCoT~IRCOT & Flan-T5 \\& GPT & Input layer & During generation (every sentence) & Prompting \\\\ FLARE~ARG & GPT & Input layer & During generation (aperiodic) & Prompting \\\\ Self-RAG~self-RAG & LLaMA & Input layer & During generation (aperiodic) & Fine-tuning \\\\ \\bottomrule tabular } table* With the impressive capabilities of LLMs in understanding, extracting, and processing textual data, researchers explore expanding the scope of IR systems beyond content ranking to answer generation. In this evolution, a reader module has been introduced to generate answers based on the document corpus in IR systems. By integrating a reader module, IR systems can directly present conclusive passages to users. Compared with providing a list of documents, users can simply comprehend the answering passages instead of analyzing the ranking list in this new paradigm. Furthermore, by repeatedly providing documents to LLMs based on their generating texts, the final generated answers can potentially be more accurate and information-rich than the original retrieved lists. A naive strategy for implementing this function is to heuristically provide LLMs with documents relevant to the user queries or the previously generated texts to support the following generation. However, this passive approach limits LLMs to merely collecting documents from IR systems without active engagement. An alternative solution is to train LLMs to interact proactively with search engines. For example, LLMs can formulate their own queries instead of relying solely on user queries or generated texts for references. According to the way LLMs utilize IR systems in the reader module, we can categorize them into passive readers and active readers. Each approach has its advantages and challenges for implementing LLM-powered answer generation in IR systems. Furthermore, since the documents provided by upstream IR systems are sometimes too long to directly feed as input for LLMs, some compression modules are proposed to extractively or abstractively compress the retrieved contexts for LLMs to understand and generate answers for queries. We will present these reader and compressor modules in the following parts and briefly introduce the existing analysis work on retrieval-augmented generation (RAG) strategy and their applications.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Passive Reader",
      "level": "2",
      "content": "To generate answers for users, a straightforward strategy is to supply the retrieved documents according to the queries or previously generated texts from IR systems as inputs to LLMs for creating passages~retallm, REALM, REPLUG, Atlas, RAG1, RAG2, RAG3, importancerag, IRCOT, RAG, RALM, RETRO, ARG, richrag, RoleRAG. By this means, these approaches use the LLMs and IR systems separately, with LLMs functioning as passive recipients of documents from the IR systems. The strategies for utilizing LLMs within IR systems' reader modules can be categorized into the following three groups according to the frequency of retrieving documents for LLMs.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Once-Retrieval Reader",
      "level": "3",
      "content": "To obtain useful references for LLMs to generate responses for user queries, an intuitive way is to retrieve the top documents based on the queries themselves in the beginning. For example, REALM~REALM adopts this strategy by directly attending the document contents to the original queries to predict the final answers based on masked language modeling. RAG~RAG follows this strategy but applies the generative language modeling paradigm. However, these two approaches only use language models with limited parameters, such as BERT and BART. Recent approaches such as REPLUG~REPLUG and Atlas~Atlas have improved them by leveraging LLMs such as GPTs, T5s, and LLaMAs for response generation. To yield better answer generation performances, these models usually fine-tune LLMs on QA tasks. However, due to the limited computing resources, many methods~RAG1,RAG2, RAG3, longrag choose to prompt LLMs for generation as they could use larger LMs in this way. Furthermore, to improve the quality of the generated answers, several approaches~ALCE, chain-of-note also try to train or prompt the LLMs to generate contexts such as citations or notes in addition to the answers to force LLMs to understand and assess the relevance of retrieved passages to the user queries. ActiveRAG~activerag and PG-RAG~PG-RAG improve them by using knowledge construction during the answer generation process. Some approaches~importancerag, REAR evaluate the importance of each retrieved reference using policy gradients to indicate which reference is more useful for generating. Specifically, ~REAR utilize LLMs themselves to provide importance for different references which also supply additional training signals. Besides, researchers explore instruction tuning LLMs such LLaMAs to improve their abilities to generate conclusive passages relying on retrieved knowledge~SAIL,RADIT,instructrag. During the training of LLM-based readers, some approaches~contras_reader explore the strategy of contrastive learning by augmenting training data by removing and replacing retrieved passages to improve the generating performances. Additionally, SPRING~springrag inserts several trainable tokens between the retrieved documents and issued questions for better optimization of the reader. R$^2$AG~R2AG extracts features from retrieval models and attaches them to the reference contents to overcome the semantic gaps between LLMs and retrievers. ~irr_rag also propose to generate noisy training data to help LLMs generate correct answers while irrelevant contents are included in the retrieved contexts. RAAT~noisy_rag and ATM~ATM further solve the noisy problem by introducing the adversarial training strategy.",
      "origin_cites_number": 17
    },
    {
      "section_title": "Periodic-Retrieval Reader",
      "level": "3",
      "content": "However, while generating long conclusive answers, it is shown~RETRO, RALM that only using the references retrieved by the original user intents as in once-retrieval readers may be inadequate. For example, when providing a passage about ``Barack Obama'', language models may need additional knowledge about his university, which may not be included in the results of simply searching the initial query. In conclusion, language models may need extra references to support the following generation during the generating process, where multiple retrieval processes may be required. To address this, solutions such as RETRO~RETRO and RALM~RALM have emerged, emphasizing the periodic collection of documents based on both the original queries and the concurrently generated texts (triggering a retrieval every $n$ generated tokens). In this manner, when generating the text about the university career of Barack Obama, the LLM can receive additional documents as supplementary materials. This need for additional references highlights the necessity for multiple retrieval iterations to ensure robustness in subsequent answer generation. Notably, RETRO~RETRO introduces a novel approach incorporating cross-attention between the generating texts and the references within the Transformer attention calculation, as opposed to directly embedding references into the input texts of LLMs. Since it involves additional cross-attention modules in the Transformer's structure, RETRO trains this model from scratch. However, these two approaches mainly rely on the successive $n$ tokens to separate generation and retrieve documents, which may not be semantically continuous and may cause the collected references noisy and useless. To solve this problem, some approaches such as IRCoT~IRCOT also explore retrieving documents for every generated sentence, which is a more complete semantic structure. Furthermore, researchers find that the whole generated passages can be considered as conclusive contexts for current queries and can be used to find more relevant knowledge to generate more thorough answers. Consequently, many recent approaches~itergen1, itergen2, itergen3, IM-RAG, GRG have also tried to extend this periodic-retrieval paradigm to iteratively using the whole generated passages to retrieve references to re-generate the answers, until the iterations reach a pre-defined limitation. Particularly, these methods can be regarded as special periodic-retrieval readers that retrieve passages when every answer is (re)-generated. Since the LLMs can receive more comprehensive and relevant references with the iterations increase, these methods that combine RAG and generation-augmented retrieval strategies can generate more accurate answers but consume more computation costs.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Aperiodic-Retrieval Reader",
      "level": "3",
      "content": "In the above strategy, the retrieval systems supply documents to LLMs in a periodic manner. However, retrieving documents in a mandatory frequency may mismatch the retrieval timing and can be costly. Recently, FLARE~ARG has addressed this problem by automatically determining the timing of retrieval according to the probability of generating texts. Since the probability can serve as an indicator of LLMs' confidence during text generation~DBLP:journals/corr/abs-2207-05221, DBLP:journals/tacl/JiangADN21, a low probability for a generated term could suggest that LLMs require additional knowledge. Specifically, when the probability of a term falls below a predefined threshold, FLARE employs IR systems to retrieve references in accordance with the ongoing generated sentences, while removing these low-probability terms. FLARE adopts this strategy of prompting LLMs for answer generation solely based on the probabilities of generating terms, avoiding the need for fine-tuning while still maintaining effectiveness. Besides, self-RAG~self-RAG tends to solve this problem by training LLMs such as LlaMA to generate specific tokens when they need additional knowledge to support following generations. Another critical model is introduced to judge whether the retrieved references are beneficial for generating. We summarize representative passive reader approaches in Table~tab:reader, considering various aspects such as the backbone language models, the insertion point for retrieved references, the timing of using retrieval models, and the tuning strategy employed for LLMs.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Active Reader",
      "level": "2",
      "content": "However, the passive reader-based approaches separate IR systems and generative language models. This signifies that LLMs can only submissively utilize references provided by IR systems and are unable to interactively engage with the IR systems in a manner akin to human interaction such as issuing queries to seek information. To allow LLMs to actively use search engines, Self-Ask~Self-Ask, DSP~DSP, and PlanRAG~PlanRAG try to employ few-shot prompts for LLMs, triggering them to search queries when they believe it is required. For example, in a scenario where the query is ``When was the existing tallest wooden lattice tower built?'', these prompted LLMs can decide to search a query ``What is the existing tallest wooden lattice tower'' to gather necessary references as they find the query cannot be directly answered. Once acquired information about the tower, they can iteratively query IR systems for more details until they determine to generate the final answers instead of asking questions. To alleviate the problem of insufficient manually annotated data for fine-tuning, LPKG~LPKG constructs high-quality active retrieval-augmented reasoning paths from existing knowledge graphs. Notably, these methods involve IR systems to construct a single reasoning chain for LLMs. MRC~MRC further improves these methods by prompting LLMs to explore multiple reasoning chains and subsequently combining all generated answers using LLMs.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Compressor",
      "level": "2",
      "content": "Existing LLMs, especially open-sourced ones, such as LLaMA and Flan-T5, have limited input lengths (usually 4,096 or 8,192 tokens). However, the documents or web pages retrieved by upstream IR systems are usually long. Therefore, it is difficult to concatenate all the retrieved documents and feed them into LLMs to generate answers. Though some approaches manage to solve these problems by aggregating the answers supported by each reference as the final answers, this strategy neglects the potential relations between retrieved passages. A more straightforward way is to directly compress the retrieved documents into short input tokens or even dense vectors~LeanContext, RECOMP, FILCO, TCRA, PRCA, xRAG, bider. To compress the retrieved references, an intuitive idea is to extract the most useful $K$ sentences from the retrieved documents. LeanContext~LeanContext applies this method and trains a small model by reinforcement learning (RL) to select the top $K$ similar sentences to the queries. The researchers also augment this strategy by using a free open-sourced text reduction method for the rest sentences as a supplement. Instead of using RL-based methods, RECOMP~RECOMP directly uses the probability or the match ratio of the generated answers to the golden answers as signals to build training datasets and tune the compressor model. For example, the sentence corresponding to the highest generating probability is the positive one while others are negative ones. Furthermore, FILCO~FILCO applies the ``hindsight'' methods, which directly align the prior distribution (the predicted importance probability distribution of sentences without knowing the gold answer) to the posterior distribution (the same distribution of sentences within knowing the gold answer) to tune language models to select sentences. However, these extractive methods may lose potential intent among all references. Therefore, abstractive methods are proposed to summarize retrieved documents into short but concise summaries for downstream generation. These methods~TCRA, RECOMP usually distill the summarizing abilities of LLMs to small models. For example, TCRA~TCRA leverages GPT-3.5-turbo to build abstractive compression datasets for MT5 model. Recently, xRAG~xRAG proposes to use a freeze sentence encoder and tunes a projector to comprise retrieved passage into a dense vector.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Analysis",
      "level": "2",
      "content": "With the rapid development of the above reader approaches, many researchers have begun to analyze the characteristics of retrieval-augmented LLMs: $\\bullet$ lost_in_middle find that the position of the relevant/golden reference has significant influences on the final generation performance. The performance is always better when the relevant reference is at the beginning or the end, which indicates the necessity of introducing a ranking module to order the retrieved knowledge. $\\bullet$ LLMbond observe that by applying retrieval augmentation generation strategy, LLMs can have a better awareness of their knowledge boundaries. $\\bullet$ integration_str analyze different strategies of integrating retrieval systems and LLMs such as concatenate (\\ie, concatenating all references for answer generation) and post fusion (\\ie, aggregating the answers corresponding to each reference). They also explore several ways of combining these two strategies. $\\bullet$ rag_tradeoff demonstrate that there exists an attribution and fluency tradeoff for retrieval-augmented LLMs: with more received references, the attribution of generated answers increases while the fluency decreases. $\\bullet$ if_rag argue that always retrieving references to support LLMs to generate answers hurts the question-answering performance. The reason is that LLMs themselves may have adequate knowledge while answering questions about popular entities and the retrieved noisy passages may interfere and bias the answering process. To overcome this challenge, they devise a simple strategy that only retrieves references while the popularity of entities in the query is quite low. By this means, the efficacy and efficiency of RAG both improve. ifrag2 pay attention to the same phenomenon and propose to paraphrase several perturbed questions for LLMs to answer according to their internal knowledge and perform a consistency check to decide whether to retrieve external information. ~ifrag3,ifrag4,ifrag5 also focus on this problem using triplets extracted from the knowledge graph and the confidence of LLMs. ~ifrag6, ifrag7 solve this problem by training LLMs or small language models to judge whether the questions are known by LLMs. $\\bullet$ knowledge-conflict analyze the impacts of knowledge conflict among retrieved references and LLM's internal knowledge. and find that LLMs follow the majority rule while facing this phenomenon. $\\bullet$ rag_attack, rag_attack2, and~phantom explore the attacking technique towards LLM-based retrieval augmented generation by poisoning retrieved passages. They find that even introducing some typos in the references may also affect the answer generation. $\\bullet$ domainrag construct an in-domain reader evaluation dataset. They deeply analyze the effectiveness of the retrieval augmented generation paradigm under the long-tail and in-domain situations. $\\bullet$ reader_compare compare the performances between readers based on base LLMs and ``instructed'' LLMs. Different from previous popular belief, They find base models outperform their corresponding instruction-tuned versions.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Applications",
      "level": "2",
      "content": "Recently, researchers~readerapp1, readerapp2, readerapp3, readerapp4, readerapp5, readerapp6, M-RAG have applied the RAG strategy to areas such as clinical QA, medical QA, and financial QA to enhance LLMs with external knowledge and to develop domain-specific applications. For example, ATLANTIC~readerapp2 adapts Atlas to the scientific domain to derive a science QA system. Besides, some approaches~PRAG also apply techniques in federated learning such as multi-party computation to perform personal RAG with privacy protection. Furthermore, to better facilitate the deployment of these RAG systems, some tools or frameworks are proposed~retallm, readertool1, readertool2. For example, RETA-LLM~retallm breaks down the whole complex generation task into several simple modules in the reader pipeline. These modules include a query rewriter module for refining query intents, a passage extraction module for aligning reference lengths with LLM limitations, and a fact verification module for confirming the absence of fabricated information in the generated answers. flashrag~release the FlashRAG toolkit for the reproduction and development of RAG research, which includes 32 pre-processed benchmark datasets and 14 state-of-the-art algorithms.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Limitations",
      "level": "2",
      "content": "Several IR systems applying the RAG strategy, such as {New Bing} and {Langchain}, have already entered commercial use. However, there are also some challenges in this novel retrieval-augmented content generation system. These include challenges such as effective query reformulation, optimal retrieval frequency, correct document comprehension, accurate passage extraction, and effective content summarization. It is crucial to address these challenges to effectively realize the potential of LLMs in this paradigm.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Search Agent",
      "level": "1",
      "content": "The emergence of large reasoning models (LRMs) has ushered in a new era for IR systems, with a growing focus on developing LRM-based intelligent agents. This paradigm shift seeks to replicate human-like reasoning and retrieval processes, thereby augmenting the capacity of LLM-powered IR models to tackle complex, real-world problems. Leveraging their advanced natural language understanding, reasoning, and generation capabilities, these agents can autonomously search, interpret, and synthesize information from diverse sources. Initial research in this domain focused on static pipeline-based architectures, where an information-seeking task is broken down into a series of modules, each with a pre-defined role~agent/static/lamda,agent/static/seeker,agent/static/webglm,agent/static/webagent,agent/static/gophercite,agent/static/knowwheretogo,CoSearchAgent. While these systems demonstrate a foundational approach, their fixed workflows limit their ability to adapt to the dynamic and complex interactions inherent in real-world scenarios. This inflexibility constrains their overall performance and hinders their effectiveness in advanced reasoning and problem-solving. Recently, the development of LRMs has enabled the development of a new class of autonomous search agents. These agents move beyond static pipelines by allowing the LLM to actively and dynamically explore the web. This is achieved by enabling the model to decide its next action based on real-time feedback from the environment or humans. This shift towards flexible, self-guided behavior makes these agents more adaptable and more closely aligned with human-like problem-solving. In this section, we will comprehensively introduce the studies about search agents from the following four aspects: (1) architecture of search agents, (2) information seeking module, (3) optimization of search agents, and (4) benchmarks and resources.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Architecture of Search Agent",
      "level": "2",
      "content": "The design of a search agent's architecture is a foundational step that establishes its core operational mechanism. Existing approaches can be broadly categorized into two main paradigms: single-agent frameworks and multi-agent frameworks. A single-agent framework utilizes a single LLM to handle all aspects of the task, including reasoning, interaction, and answer generation. In contrast, a multi-agent framework distributes these responsibilities among multiple LLMs, which act as collaborative agents to achieve the target.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Single-Agent Frameworks",
      "level": "3",
      "content": "In a single-agent framework, a single LLM with strong reasoning capabilities serves as the central decision-maker and task executor. This LLM dynamically determines the next action based on the current information state and environmental context, managing the entire reasoning and interaction process. For example, Search-R1~Search-R1 and ReSearch~ReSearch adopt a ReAct-style mechanism, enabling the LLM policy to automatically generate actions such as ``think'', ``search'', and ``answer''. This allows the agent to iteratively interact with search tools to resolve complex multi-hop questions. To optimize the performance of these single-agent systems, researchers have employed various reinforcement learning (RL) techniques. The GRPO algorithm~deepseek-math has been commonly utilized to enhance performance. In a more refined approach, R1-Searcher~R1-Searcher introduces a two-stage GRPO-based RL optimization. The first stage assigns rewards based on retrieval frequency and output format correctness, while the second stage rewards answer accuracy and the final output format. To further improve reasoning and exploration, START propose START, which introduces a hint-infer mechanism that manually inserts hint strings during inference. This encourages the LLM to self-reflect and make better use of external tools. They also design Hint-RFT, a method that performs rejection sampling and revises reasoning trajectories to support the supervised fine-tuning of search agents. More recently, Atom-Searcher~atom-searcher has been proposed to decompose the holistic ``thinking'' process into several finer-grained ``atom-thinking'' actions. These actions are guided by reasoning reward models, which provide precise feedback on reasoning trajectories, going beyond simple outcome-based rewards. The main advantage of the single-agent framework lies in its simplicity, as it can be trained end-to-end via RL. This allows researchers to explore the reasoning limits of a single model using carefully designed optimization algorithms. However, a single agent often struggles with highly complex queries that require extensive tool use and long-context reasoning. To address this limitation, multi-agent frameworks have been explored, where multiple agents collaborate to complete complex search and reasoning tasks.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Multi-Agent Frameworks",
      "level": "3",
      "content": "Multi-agent frameworks utilize multiple, specialized LLMs that collaborate to complete complex search and reasoning tasks. This approach enables a division of labor, where each agent focuses on a distinct function, leading to improved system effectiveness and efficiency. For example, KwaiAgents~KwaiAgents separates reasoning and summarization into two distinct agents. The reasoning agent is equipped with capabilities such as query understanding, external documents referencing, memory management, and task execution via a hybrid searchbrowse toolkit. Similarly, AI-SearchPlanner propose decoupling the search agent into a search planner and a generator, with optimization efforts primarily on the planner. Building on this, MindSearch~MindSearch introduces a multi-agent framework inspired by human cognitive processes for web retrieval. Its architecture consists of a WebPlanner and multiple WebSearchers. The WebPlanner acts as a high-level controller, decomposing user queries into atomic sub-questions and designing the reasoning process. The WebSearchers then perform hierarchical retrieval over the web, guided by these sub-questions. By employing a coarse-to-fine selection strategy, they efficiently filter valuable information from a large pool of web pages, thereby alleviating the information overload that often hinders LLMs. Alita~Alita advances this idea by incorporating self-evolving capabilities through a dynamic MCP box. Its manager agent handles central task planning and MCP brainstorming, deciding whether to generate new MCP tools for emerging tasks. The web agent, in turn, is responsible for browsing and retrieving external information. Similarly, OWL~OWL further decouples central planning and task execution to improve generalization across different domains. OWL consists of a domain-agnostic planner agent for high-level task decomposition, a coordinator agent for managing task assignments and dependencies, and a set of specialized agents with domain-specific toolkits that execute subtasks and report results. This modular design allows researchers to focus optimization efforts on the planner, enhancing its adaptability while minimizing training complexity for other components. The primary advantage of the multi-agent framework is that it allows individual agents to specialize in distinct tasks, enhancing overall system effectiveness and efficiency. However, it introduces challenges in jointly optimizing multiple agents through RL. Current approaches often limit optimization to the core planner agent, which plays the most critical role in coordinating the framework. Therefore, developing more stable and efficient RL strategies for multi-agent search systems remains a promising direction for future research.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Information Seeking Module",
      "level": "2",
      "content": "To handle atomic queries or complete sub-tasks from an upstream planner, a search agent typically relies on an information-seeking module that locates, collects, and synthesizes relevant information. These approaches can be roughly categorized into two types: API-based and browsing-based methods. API-based methods use search engine APIs to retrieve information, while browsing-based methods construct sandboxes or virtual environments that enable agents to simulate human-like web interactions.",
      "origin_cites_number": 0
    },
    {
      "section_title": "API-based Information Seeking",
      "level": "3",
      "content": "The most straightforward information-seeking strategy is to leverage search engine and scientific database APIs as external tools. Many commercial applications, such as Gemini DeepResearch and Grok DeepSearch,Gemini DeepResearch: \\url{https://gemini.google/overview/deep-research/, Grok DeepSearch: https://x.ai/news/grok-3} rely on APIs like Google Search, Bing Search, and X Search to access external knowledge. In the research community, Cognitive Kernel-Pro~Cognitive-Kernel-Pro uses the free DuckDuckGo search interface to create a fully open-source pipeline, while CoSearch-Agent~co-search-agent integrates SerpApi for real-time search within Slack-based environments. Beyond basic search APIs, other systems incorporate specialized APIs to refine the retrieval process. For example, Search-o1~search-o1 and Agent Laboratory~Agent-Laboratory use Jina Reader API to extract and refine web passages for downstream reasoning, and the arXiv API to obtain academic metadata. Similarly, AI Scientist~AI-scientist employs the Semantic Scholar API to verify citation relationships. While simple and accessible, API-based approaches often struggle with complex, dynamic content rendered by JavaScript, interactive components, or information gated behind authentication.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Browsing-based Information Seeking",
      "level": "3",
      "content": "In contrast to API-based methods, browsing-based information-seeking approaches provide search agents with interactive environments that simulate human-web interactions. For example, Manus AI's browsing agent creates a sandboxed Chromium instance for each subtask,Manus AI: \\url{https://manus.im/} while AutoGLM~AutoGLM: \\url{https://autoglm-research.zhipuai.cn/} sequentially opens web pages, reads content, and generates refined reports. In research, AutoAgent~autoagent uses the BrowserGym environment to perform scrolling and interaction with webpage components. SimpleDeepSearcher~simpledeepsearcher and Tool-star~toolstar further compress retrieved content from both browsing and API-based extraction to generate condensed references for answer generation. While browsing-based approaches are better suited for retrieving real-time and deeply nested content, they typically incur higher latency and resource costs. To address the high costs associated with real search APIs, some recent works explore alternative approaches. For example, ZeroSearch~zerosearch trains LLMs to simulate search engine behavior without making actual API calls, thereby significantly reducing training costs. Additionally, some methods, such as Alita~Alita, propose to dynamically create new MCP tools during the agent's reasoning process, enabling a self-evolving capability that reduces reliance on pre-defined toolkits and further optimizes computation costs.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Optimization",
      "level": "2",
      "content": "To transform general-purpose LLMs into specialized search agents, researchers have explored various optimization and fine-tuning methods. These approaches aim to internalize advanced search skills, such as planning, reasoning, and tool usage into the model's parametric knowledge. The ultimate goal is to enable agents to perform exploratory information acquisition. Based on the progressive levels of search capabilities, this section categorizes mainstream agent tuning methods into three paradigms: strategic retrieval optimization, iterative search tuning, and autonomous open-web search.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Strategic Retrieval Optimization",
      "level": "3",
      "content": "In basic search scenarios, agents must first learn when to retrieve information and how to formulate high-quality queries. Early RAG methods typically follow a fixed and passive pipeline: given a question, the system directly performs a search and then generates an answer based on the results. This approach is often inefficient, as it can lead to redundant searches and struggles to handle irrelevant information. Recent studies have introduced strategic retrieval optimization techniques that enable agents to explicitly model retrieval decisions, thereby balancing search costs against the potential benefit of new information. For example, Open-RAG~Open-RAG proposes a ``hybrid adaptive retrieval'' mechanism that learns to generate specific tokens to control its retrieval behavior. This work also introduces a constructive learning paradigm, which actively injects distracting information into training data to enhance the model's robustness and discrimination capabilities against noisy or irrelevant search results. Similarly, DeepRAG~DeepRAG models retrieval decisions as a Markov decision process, using imitation learning to train models to weigh the benefits of relying on internal knowledge versus performing an external search at each reasoning step. This provide the model with the dynamic ability to decide when to retrieve. ATLAS~ATLAS-Agent takes a different approach by applying gradient backpropagation only on ``critical steps'' within expert trajectories. In search tasks, these steps correspond to key decisions such as initiating a search or formulating a core query. By focusing the training signal on these strategic points, this method improves the agent's core decision-making and generalization ability.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Iterative Retrieval Tuning",
      "level": "3",
      "content": "For complex tasks that cannot be solved with a single retrieval, search agents require multi-step reasoning and iterative information acquisition. These capabilities are crucial for applications such as multi-hop question answering and open-domain problem solving, where agents must converge on an answer through a dynamic cycle of ``think-search-integrate-rethink''. Research in this area has leveraged both supervised and reinforcement learning paradigms. Supervised learning trains models by providing expert trajectories that include all intermediate reasoning and retrieval steps. CoRAG~CoRAG exemplifies this approach by automatically generating retrieval chains with intermediate sub-queries and sub-answers for existing datasets. Through rejection sampling, it enables models to explicitly learn multi-step retrieval patterns. Similarly, Auto-RAG~Auto-RAG focuses on synthesizing instruction data that contains retrieval decision processes, allowing models to master autonomous multi-round retrieval logic through SFT. Reinforcement learning allows agents to autonomously explore and learn optimal dynamic search strategies through interaction with an environment. Works such as ReSearch~ReSearch, R1-Searcher~R1-Searcher, and Search-R1~Search-R1 all adopt RL frameworks, defining search as a learnable action within the reasoning process. Agents learn when and how to intersperse search queries by maximizing rewards from task success. Among these, R1-Searcher designs a two-stage RL training pipeline that effectively decouples the objectives of learning to use tools from learning to solve problems with tools. At the algorithmic level, ARPO~ARPO optimizes training efficiency for iterative agents by proposing an entropy-based adaptive exploration mechanism. This method increases exploration intensity at critical decision points where models show high uncertainty, significantly reducing training costs.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Autonomous Open-Web Search",
      "level": "3",
      "content": "At the highest level, search agents must be able to operate autonomously within open and dynamic web environments. This capability requires models to handle complex challenges such as noisy data, conflicting information from multiple sources, and a lack of explicit supervision. To succeed, they must possess advanced skills in information planning, cross-validation, and multimodal understanding. Recent studies show that end-to-end RL is an effective path for endowing models with autonomous research capabilities. WebAgent-R1~WebAgent-R1 and DeepResearcher~DeepResearcher use sparse reward training in real browser environments, enabling agents to autonomously plan search paths, verify information, and integrate knowledge from various sources. WebThinker~WebThinker proposes an ``Think-Search-and-Draft'' strategy, using iterative online direct preference optimization (DPO) to enable agents to seamlessly switch between information collection, reasoning, and content generation. Subsequent research has focused on building more systematic training methodologies. Works such as WebDancer~WebDancer, WebSailor~WebSailor, and WebShaper~WebShaper demonstrate that a combination of high-quality data synthesis and hybrid training strategies is an effective path for training advanced agents. These works have made important innovations at the data level. For example, WebShaper proposes a ``formalization-driven'' data synthesis framework that generates logically consistent data from a task's reasoning structure. WebWatcher~WebWatcher further advances this filed by incorporating visual information during training, enabling models to understand and utilize both images and texts on web pages, thereby moving toward human-like research capabilities. Through these methods, search agents are gradually evolving into research-oriented agents capable of autonomous exploration and information integration on the open web.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Benchmarks and Resources",
      "level": "2",
      "content": "To effectively evaluate and advance search agents, a diverse set of benchmarks and resources is essential. Current evaluation methodologies can be broadly categorized into two main paradigms: QA-style benchmarks, which assess the agent's ability to answer complex questions, and task-oriented benchmarks, which measure its capacity for planning, tool use, and environmental interaction. Additionally, a growing number of agent platforms and datasets serve as valuable resources for both evaluation and model training.",
      "origin_cites_number": 0
    },
    {
      "section_title": "QA Benchmarks",
      "level": "3",
      "content": "QA benchmarks are designed to evaluate problem-solving and reasoning capabilities of search agents. They range from simple factual recall to multi-hop reasoning and expert-level challenges. Single-hop QA. It involves questions that can be answered by retrieving information from a single document or source. These benchmarks, such as TriviaQA~TriviaQA, SimpleQA~SimpleQA, PopQA~PopQA, and Natural Questions (NQ)~nq, primarily test a model's ability to perform open-domain factual retrieval and reading comprehension. For example, NQ provides anonymized Google search queries paired with human-annotated answers and Wikipedia evidence. Multi-hop QA. It requires models to reason over and combine information from multiple sources to find the answer. A typical multi-hop QA dataset is HotpotQA~HotpotQA. It focuses on multi-hop reasoning by providing questions that require chaining evidence across multiple Wikipedia pages. 2WikiMultiHopQA~2WikiMultiHopQA extends this by mixing structured knowledge and unstructured text and providing explicit reasoning paths for a more fine-grained evaluation of multi-step inference. Expert-level Challenges. They are designed to be extremely difficult, often requiring deep domain knowledge and complex reasoning to push the limits of advanced models. Humanity's Last Exam (HLE)~HLE assembles thousands of hard, expert-crafted questions to stress test models across broad domains. Benchmarks like BrowseComp~BrowseComp attempt to force genuine web-based retrieval by filtering out items solvable from parametric memory. However, even with these efforts, top-performing systems can still exploit internal knowledge, which may overstate their true research capability.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Task-oriented Benchmarks",
      "level": "3",
      "content": "Task-oriented benchmarks assess an agent's practical skills in planning, tool use, and interaction with various environments. General Assistant Workflows. GAIA~GAIA, AssistantBench~AssistantBench, and Magnetic-One~Magentic_One cover broad assistant tasks that require planning across dialogue, retrieval, and simple tool calls. GAIA, for instance, measures an agent's end-to-end task management, while Magnetic-One emphasizes robustness across diverse domains and chained subtasks. Code and Research. SWE-bench~SWE-bench, HumanEvalFix~HumanEvalFix, MLE-bench~MLE-bench, and MLAgentBench~MLAgentBench probe pipelines centered on software engineering and research. They require agents to perform tasks like code implementation, debugging, experiment setup, and hyperparameter tuning. Multi-Agent Coordination. RE-Bench~RE-Bench and RESEARCHTOWN~RESEARCHTOWN stress multi-agent collaboration, role assignment, and iterative refinement on shared research goals. GUI Control. WebArena~agent/dynamic/webarena and SpaBench~SpaBench extend evaluation to include direct interface manipulation, measuring an agent's ability to control web UIs or simulated devices and handle noisy, stateful environments.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Agent Datasets and Platforms",
      "level": "3",
      "content": "Beyond static benchmarks, several projects offer comprehensive resources that bundle evaluation suites with data-generation pipelines and agent toolkits. These resources serve as both benchmarks and valuable training corpora for agent research. The Alibaba-NLP WebAgent repository is a notable example, packaging a web-traversal benchmark (WebWalkerQA~WebWalker) with agent models and data tools (WebDancer~WebDancer, WebShaper~WebShaper, and WebSailor~WebSailor). Specifically, WebWalkerQA~WebWalker probes an agent's ability to traverse sites and extract evidence across multiple subpages, emphasizing structured navigation over single-turn retrieval. WebDancer~WebDancer implements a four-stage training paradigm and releases both models and browsing trajectories, enabling reproducible, end-to-end evaluation. WebShaper~WebShaper provides a ``formalization-driven'' data synthesis pipeline that systematically generates information-seeking instances, making it valuable for cold-starting agents and for studying data-centric training strategies. The recent model releases from WebSailor~WebSailor demonstrate how post-training and specialized agent tuning can yield stronger navigation and planning behaviors on these benchmarks.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Future Direction",
      "level": "1",
      "content": "In this survey, we comprehensively reviewed recent advancements in LLM-enhanced IR systems and discussed their limitations. Since the integration of LLMs into IR systems is still in its early stages, there are still many opportunities and challenges. In this section, we summarize the potential future directions in terms of the four modules in an IR system we just discussed, namely query rewriter, retriever, reranker, and reader. In addition, as evaluation has also emerged as an important aspect, we will also introduce the corresponding research problems that need to be addressed in the future. Another discussion about important research topics on applying LLMs to IR can be found in a recent perspective paper~ir_perspective.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Query Rewriter",
      "level": "2",
      "content": "LLMs have enhanced query rewriter for both ad-hoc and conversational search scenarios. Most of the existing methods rely on prompting LLMs to generate new queries. While yielding remarkable results, the refinement of rewriting quality and the exploration of potential application scenarios require further investigation. $\\bullet$ Rewriting queries according to ranking performance. A typical paradigm of prompting-based methods is providing LLMs with several ground-truth rewriting cases (optional) and the task description of query rewriter. Despite LLMs being capable of identifying potential user intents of the query~intent5, they lack awareness of the resulting retrieval quality of the rewritten query. The absence of this connection can result in rewritten queries that seem correct yet produce unsatisfactory ranking results. Although some existing studies have used reinforcement learning to adjust the query rewriter process according to generation results~ma2023query, a substantial realm of research remains unexplored concerning the integration of ranking results. $\\bullet$ Improving query rewriter in conversational search. As yet, primary efforts have been made to improve query rewriter in ad-hoc search. In contrast, conversational search presents a more developed landscape with a broader scope for LLMs to contribute to query understanding. By incorporating historical interactive information, LLMs can adapt system responses based on user preferences, providing a more effective conversational experience. However, this potential has not been explored in depth. In addition, LLMs could also be used to simulate user behavior in conversational search scenarios, providing more training data, which are urgently needed in current research. $\\bullet$ Achieving personalized query rewriter. LLMs offer valuable contributions to personalized search through their capacity to analyze user-specific data. In terms of query rewriter, with the excellent language comprehension ability of LLMs, it is possible to leverage them to build user profiles based on users' search histories (\\eg, issued queries, click-through behaviors, and dwell time). This empowers the achievement of personalized query rewriter for enhanced IR and finally benefits personalized search or personalized recommendation.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Retriever",
      "level": "2",
      "content": "Leveraging LLMs to improve retrieval models has received considerable attention, promising an enhanced understanding of queries and documents for improved ranking performance. However, despite strides in this field, several challenges and limitations still need to be investigated in the future: $\\bullet$ Reducing the latency of LLM-based retrievers. LLMs, with their massive parameters and world knowledge, often entail high latency during the inferring process. This delay poses a significant challenge for practical applications of LLM-based retrievers, as search engines require in-time responses. To address this issue, promising research directions include transferring the capabilities of LLMs to smaller models, exploring quantization techniques for LLMs in IR tasks, and so on. $\\bullet$ Simulating realistic queries for data augmentation. Since the high latency of LLMs usually blocks their online application for retrieval tasks, many existing studies have leveraged LLMs to augment training data, which is insensitive to inference latency. Existing methods that leverage LLMs for data augmentation often generate queries without aligning them with real user queries, leading to noise in the training data and limiting the effectiveness of retrievers. As a consequence, exploring techniques such as reinforcement learning to enable LLMs to simulate the way that real queries are issued holds the potential for improving retrieval tasks. $\\bullet$ Incremental indexing for generative retrieval. As elaborated in Section~gen_ir, the emergence of LLMs has paved the way for generative retrievers to generate document identifiers for retrieval tasks. This approach encodes document indexes and knowledge into the LLM parameters. However, the static nature of LLM parameters, coupled with the expensive fine-tuning costs, poses challenges for updating document indexes in generative retrievers when new documents are added. Therefore, it is crucial to explore methods for constructing an incremental index that allows for efficient updates in LLM-based generative retrievers. $\\bullet$ Supporting multi-modal search. Web pages usually contain multi-modal information, including texts, images, audios, and videos. However, existing LLM-enhanced IR systems mainly support retrieval for text-based content. A straightforward solution is to replace the backbone with multi-modal large models, such as GPT-4~gpt-4. However, this undoubtedly increases the cost of deployment. A promising yet challenging direction is to combine the language understanding capability of LLMs with existing multi-modal retrieval models. By this means, LLMs can contribute their language skills in handling different types of content.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Reranker",
      "level": "2",
      "content": "In Section~sec:rank, we have discussed the recent advanced techniques of utilizing LLMs for the reranking task. Some potential future directions in reranking are discussed as follows. $\\bullet$ Enhancing the online availability of LLMs. Though effective, many LLMs have a massive number of parameters, making it challenging to deploy them in online applications. Besides, many reranking methods~sun2023chatgpt, ma2023zero rely on calling LLM APIs, incurring considerable costs. Consequently, devising effective approaches (such as distilling to small models) to enhance the online applicability of LLMs emerges as a research direction worth exploring. $\\bullet$ Improving personalized search. Many existing LLM-based reranking methods mainly focus on the ad-hoc reranking task. However, by incorporating user-specific information, LLMs can also improve the effectiveness of the personalized reranking task. For example, by analyzing users' search history, LLMs can construct accurate user profiles and rerank the search results accordingly, providing personalized results with higher user satisfaction. $\\bullet$ Adapting to diverse ranking tasks. In addition to document reranking, there are also other ranking tasks, such as response ranking, evidence ranking, entity ranking and etc., which also belong to the universal information access system. Navigating LLMs towards adeptness in these diverse ranking tasks can be achieved through specialized methodologies, such as instruction tuning. Exploring this avenue holds promise as an intriguing and valuable research trajectory.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Reader",
      "level": "2",
      "content": "With the increasing capabilities of LLMs, the future interaction between users and IR systems will be significantly changed. Due to the powerful natural language processing and understanding capabilities of LLMs, the traditional search paradigm of providing ranking results is expected to be progressively replaced by synthesizing conclusive answering passages for user queries using the reader module. Although such strategies have already been investigated by academia and facilitated by industry as we stated in Section~sec:reader, there still exists much room for exploration. $\\bullet$ Improving the reference quality for LLMs. To support answer generation, existing approaches usually directly feed the retrieved documents to the LLMs as references. However, since a document usually covers many topics, some passages in it may be irrelevant to the user queries and can introduce noise during LLMs' generation. Therefore, it is necessary to explore techniques for extracting relevant snippets from retrieved documents, enhancing the performance of retrieval-augmented generation. $\\bullet$ Improving the answer reliability of LLMs. Incorporating the retrieved references has significantly alleviated the ``hallucination'' problem of LLMs. However, it remains uncertain whether the LLMs refer to these supported materials during answering queries. Some studies~LLMbond have revealed that LLMs can still provide unfaithful answers even with additional references. Therefore, the reliability of the conclusive answers might be lower compared to the ranking results provided by traditional IR systems. It is essential to investigate the influence of these references on the generation process, thereby improving the credibility of reader-based novel IR systems.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Search Agent",
      "level": "2",
      "content": "With the outstanding performance of LLMs, the patterns of searching may completely change from traditional IR systems to autonomous search agents. In Section~sec:agent, we have discussed many existing works that utilize a static or dynamic pipeline to autonomously browse the web. These works are believed to be the pioneering works of the new searching paradigm. However, there is still plenty of room for further improvements. $\\bullet$ Enhancing the Trustworthiness of LLMs. When LLMs are enabled to browse the web, it is important to ensure the validity of retrieved documents. Otherwise, the unfaithful information may increase the LLMs' hallucination problem. Besides, even if the gathered information has high quality, it remains unclear whether they are really used for synthesizing responses. A potential strategy to address this issue is enabling LLMs to autonomously validate the documents they scrape. This self-validation process could incorporate mechanisms for assessing the credibility and accuracy of the information within these documents. $\\bullet$ Mitigating Bias and Offensive Content in LLMs. The presence of biases and offensive content within LLM outputs is a pressing concern. This issue primarily stems from biases inherent in the training data and will be amplified by the low-quality information gathered from the web. Achieving this requires a multi-faceted approach, including improvements in training data, algorithmic adjustments, and continuous monitoring for bias and inappropriate content that LLMs collect and generate.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Evaluation",
      "level": "2",
      "content": "LLMs have attracted significant attention in the field of IR due to their strong ability in context understanding and text generation. To validate the effectiveness of LLM-enhanced IR approaches, it is crucial to develop appropriate evaluation metrics. Given the growing significance of readers as integral components of IR systems, the evaluation should consider two aspects: assessing ranking performance and evaluating generation performance. $\\bullet$ Generation-oriented ranking evaluation. Traditional evaluation metrics for ranking primarily focus on comparing the retrieval results of IR models with ground-truth (relevance) labels. Typical metrics include precision, recall, mean reciprocal rank (MRR)~MRR, mean average precision (MAP), and normalized discounted cumulative gain (nDCG)~NDCG. These metrics measure the alignment between ranking results and human preference on using these results. Nevertheless, these metrics may fall short in capturing a document's role in the generation of passages or answers, as their relevance to the query alone might not adequately reflect this aspect. This effect could be leveraged as a means to evaluate the usefulness of documents more comprehensively. A formal and rigorous evaluation metric for ranking that centers on generation quality has yet to be defined. $\\bullet$ Text generation evaluation. The wide application of LLMs in IR has led to a notable enhancement in their generation capability. Consequently, there is an imperative demand for novel evaluation strategies to effectively evaluate the performance of passage or answer generation. Previous evaluation metrics for text generation have several limitations, including: (1)~Dependency on lexical matching: methods such as BLEU~bleu or ROUGE~rouge primarily evaluate the quality of generated outputs based on $n$-gram matching. This approach cannot account for lexical diversity and contextual semantics. As a result, models may favor generating common phrases or sentence structures rather than producing creative and novel content. (2)~Insensitivity to subtle differences: existing evaluation methods may be insensitive to subtle differences in generated outputs. For example, if a generated output has minor semantic differences from the reference answer but is otherwise similar, traditional methods might overlook these nuanced distinctions. (3)~Lack of ability to evaluate factuality: LLMs are prone to generating ``hallucination'' problems~DBLP:journals/corr/abs-2303-08896,webbrain,DBLP:journals/corr/abs-2305-11747,DBLP:conf/emnlp/ChenDBQWCW23. The hallucinated texts can closely resemble the oracle texts in terms of vocabulary usage, sentence structures, and patterns, while having non-factual content. Existing methods are hard to identify such problems, while the incorporation of additional knowledge sources such as knowledge bases or reference texts could potentially aid in addressing this challenge.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Bias",
      "level": "2",
      "content": "Since ChatGPT was released, LLMs have drawn much attention from both academia and industry. The wide applications of LLMs have led to a notable increase in content on the Internet that is not authored by humans but rather generated by these language models. However, as LLMs may hallucinate and generate non-factual texts, the increasing number of LLM-generated contents also brings worries that these contents may provide fictitious information for users across IR systems. More severely, researchers~sourcebias1,sourcebias2 show that some modules in IR systems such as retriever and reranker, especially those based on neural models, may prefer LLM-generated documents, since their topics are more consistent and the perplexity of them are lower compared with human-written documents. The authors refer to this phenomenon as the ``source bias'' towards LLM-generated text. It is challenging but necessary to consider how to build IR systems free from this category of bias.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "In this survey, we have conducted a thorough exploration of the transformative impact of LLMs on IR across various dimensions. We have organized existing approaches into distinct categories based on their functions: query rewriter, retrieval, reranking, and reader modules. In the domain of query rewriter, LLMs have demonstrated their effectiveness in understanding ambiguous or multi-faceted queries, enhancing the accuracy of intent identification. In the context of retrieval, LLMs have improved retrieval accuracy by enabling more nuanced matching between queries and documents, considering context as well. Within the reranking realm, LLM-enhanced models consider more fine-grained linguistic nuances when re-ordering results. The incorporation of reader modules in IR systems represents a significant step towards generating comprehensive responses instead of mere document lists. The integration of LLMs into IR systems has brought about a fundamental change in how users engage with information and knowledge. From query rewriter to retrieval, reranking, and reader modules, LLMs have enriched each aspect of the IR process with advanced linguistic comprehension, semantic representation, and context-sensitive handling. As this field continues to progress, the journey of LLMs in IR portends a future characterized by more personalized, precise, and user-centric search encounters. This survey focuses on reviewing recent studies of applying LLMs to different IR components and using LLMs as search agents. Beyond this, a more significant problem brought by the appearance of LLMs is: is the conventional IR framework necessary in the era of LLMs? For example, traditional IR aims to return a ranking list of documents that are relevant to issued queries. However, the development of generative language models has introduced a novel paradigm: the direct generation of answers to input questions. Furthermore, according to a recent perspective paper~ir_perspective, IR might evolve into a fundamental service for diverse systems. For example, in a multi-agent simulation system~DBLP:journals/corr/abs-2304-03442, an IR component can be used for memory recall. This implies that there will be many new challenges in future IR. \\balance document",
      "origin_cites_number": 2
    }
  ],
  "literature_review_id": 260887838,
  "meta_info": {
    "cite_counts": 350,
    "Conference_journal_name": "ACM Transactions on Information Systems",
    "influentialcitationcount": 15,
    "Author_info": {
      "Publicationsh": 76,
      "h_index": 25,
      "Citations": 3360,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots",
      "From eliza to xiaoice: challenges and opportunities with social chatbots",
      "Dense passage retrieval for open-domain question answering",
      "Image retrieval: Ideas, influences, and trends of the new age",
      "Multi-hop selector network for multiturn response selection in retrieval-based chatbots",
      "Content selection network for document-grounded retrievalbased chatbots",
      "Proactive retrieval-based chatbots based on relevant knowledge and goals",
      "Learning implicit user profiles for personalized retrieval-based chatbot",
      "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
      "Query reformulation for dynamic information integration",
      "Analyzing and evaluating query reformulation strategies in web search logs",
      "Multistage document ranking with BERT",
      "Document ranking with a pretrained sequence-to-sequence model",
      "Contrastive learning of user behavior sequence for context-aware document ranking",
      "Personalizing search via automated analysis of interests and activities",
      "Modeling the impact of short-and long-term behavior on search personalization",
      "Personalizing search results using hierarchical RNN with query-aware attention",
      "PSSL: selfsupervised learning for personalized search with contrastive sampling",
      "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
      "Diversifying search results",
      "DVGAN: A minimax game for search result diversification combining explicit and implicit features",
      "Modeling intent graph for search result diversification",
      "Improving language models by retrieving from trillions of tokens",
      "Webgpt: Browser-assisted question-answering with human feedback",
      "Introduction to Modern Information Retrieval",
      "A vector space model for automatic indexing",
      "A general language model for information retrieval",
      "Delta TFIDF: an improved feature space for sentiment analysis",
      "Okapi at TREC-3",
      "A deep relevance matching model for ad-hoc retrieval",
      "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
      "Pretrained Transformers for Text Ranking: BERT and Beyond, ser. Synthesis Lectures on Human Language Technologies",
      "Language models are unsupervised multitask learners",
      "Language models are few-shot learners",
      "Llama: Open and efficient foundation language models",
      "Recommendation as instruction following: A large language model empowered recommendation approach",
      "Large language models are zeroshot rankers for recommender systems",
      "Towards open-world recommendation with knowledge augmentation from large language models",
      "Recommender systems in the era of large language models (llms)",
      "Bloomberggpt: A large language model for finance",
      "Empowering molecule discovery for moleculecaption translation with large language models: A chatgpt perspective",
      "Emergent abilities of large language models",
      "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "Pre-trained models for natural language processing: A survey",
      "A comprehensive survey of ai-generated content (AIGC): A history of generative AI from GAN to chatgpt",
      "Pretrained language model for text generation: A survey",
      "A survey for in-context learning",
      "Towards reasoning in large language models: A survey",
      "A survey of large language models",
      "Information retrieval meets large language models: A strategic report from chinese IR community",
      "Statistical language modeling for information retrieval",
      "Neural models for information retrieval",
      "Dense text retrieval based on pretrained language models: A survey",
      "Deep contextualized word representations",
      "BERT: pre-training of deep bidirectional transformers for language understanding",
      "Attention is all you need",
      "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "Exploring the limits of transfer learning with a unified text-totext transformer",
      "Scaling laws for neural language mod-els",
      "Unified scaling laws for routed language models",
      "Lora: Low-rank adaptation of large language models",
      "Prefix-tuning: Optimizing continuous prompts for generation",
      "The power of scale for parameter-efficient prompt tuning",
      "Qlora: Efficient finetuning of quantized llms",
      "Query2doc: Query expansion with large language models",
      "Query expansion techniques for information retrieval: A survey",
      "The limitations of term co-occurrence data for query expansion in document retrieval systems",
      "Wordnet: An electronic lexical database",
      "Lexical relations: Enhancing effectiveness of information retrieval systems",
      "Automatic thesaurus construction for cross generation corpus",
      "A corpus analysis approach for automatic query expansion and its extension to multiple databases",
      "Improving weak ad-hoc queries using wikipedia asexternal corpus",
      "Query expansion with freebase",
      "A new fuzzy logic-based query expansion model for efficient information retrieval using relevance feedback approach",
      "Precise zero-shot dense retrieval without relevance labels",
      "Query expansion by prompting large language models",
      "Crafting the path: Robust query rewriting for information retrieval",
      "Can generative llms create query variants for test collections? an exploratory study",
      "Can query expansion improve generalization of strong cross-encoder rankers",
      "Query rewriting for retrieval-augmented large language models",
      "Large language model based long-tail query rewriting in taobao search",
      "Generative and pseudo-relevant feedback for sparse, dense and learned sparse retrieval",
      "GRM: generative relevance modeling using relevance-aware sample estimation for document retrieval",
      "Knowledge refinement via interaction between search engines and large language models",
      "Retrieval-augmented retrieval: Large language models are strong zero-shot retriever",
      "Corpussteered query expansion with large language models",
      "Context aware query rewriting for text rankers using LLM",
      "Rafe: Ranking feedback improves query rewriting for RAG",
      "Large language models know your contextual search intent: A prompting framework for conversational search",
      "CON-VERSER: few-shot conversational dense retrieval with synthetic data generation",
      "Enhancing conversational search: Large language model-aided informative query rewriting",
      "Convtrans: Transforming web search ses-sions for conversational dense retrieval",
      "Dialog inpainting: Turning documents into dialogs",
      "Re-invoke: Tool invocation rewriting for zero-shot tool retrieval",
      "Rrnorm: A novel framework for chinese disease diagnoses normalization via llm-driven terminology component recognition and reconstruction",
      "An interactive query generation assistant using llm-based prompt modification and user feedback",
      "Contextualizing search queries in-context learning for conversational rewriting with llms",
      "Learning interpretable legal case retrieval via knowledge-guided case reformulation",
      "Generate, filter, and fuse: Query expansion via multi-step keyword generation for zero-shot neural rankers",
      "Chain-ofthought prompting elicits reasoning in large language models",
      "Generate rather than retrieve: Large language models are strong context generators",
      "MS MARCO: A human generated machine reading comprehension dataset",
      "Natural questions: a benchmark for question answering research",
      "Cotkr: Chain-of-thought enhanced knowledge rewriting for complex knowledge graph question answering",
      "Crafting the path: Robust query rewriting for information retrieval",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning",
      "When do generative query and document expansions fail? A comprehensive study across methods, retrievers, and datasets",
      "Inpars: Data augmentation for information retrieval using large language models",
      "Pretraining with large language model-based document expansion for dense passage retrieval",
      "Inpars-v2: Large language models as efficient dataset generators for information retrieval",
      "Promptagator: Few-shot dense retrieval from 8 examples",
      "Augtriever: Unsupervised dense retrieval by scalable data augmentation",
      "UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers",
      "Soft prompt tuning for augmenting dense retrieval with large language models",
      "Gecko: Versatile text embeddings distilled from large language models",
      "Questions are all you need to train a dense passage retriever",
      "Improving text embeddings with large language models",
      "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
      "Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval",
      "Text and code embeddings by contrastive pre-training",
      "Large dual encoders are generalizable retrievers",
      "SGPT: GPT sentence embeddings for semantic search",
      "Instruction tuning with GPT-4",
      "A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam- ford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, \"Mistral 7b,\" CoRR, vol. abs/2310.06825, 2023.",
      "Textbooks are all you need",
      "Gemma: Open models based on gemini research and technology",
      "Finetuning llama for multi-stage text retrieval",
      "Sfr-embedding-mistral: Enhance text retrieval with transfer learning",
      "Linq-embed-mistral: Elevating text retrieval with improved gpt data through taskspecific control and quality refinement",
      "MTEB: massive text embedding benchmark",
      "Making large language models A better foundation for dense retrieval",
      "Nv-embed: Improved techniques for training llms as generalist embedding models",
      "One embedder, any task: Instruction-finetuned text embeddings",
      "Task-aware retrieval with instructions",
      "Large language models as foundations for next-gen dense retrieval: A comprehensive empirical assessment",
      "Chatretriever: Adapting large language models for generalized and robust conversational dense retrieval",
      "Scaling sentence embeddings with large language models",
      "Rethinking search: making domain experts out of dilettantes",
      "Dynamicretriever: A pre-trained model-based IR system without an explicit index",
      "Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks",
      "Transformer memory as a differentiable search index",
      "Large language models are built-in autoregressive search engines",
      "A neural corpus indexer for document retrieval",
      "Autoregressive search engines: Generating substrings as document identifiers",
      "How does generative retrieval scale to millions of passages?",
      "Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks",
      "Text-to-text multi-view learning for passage re-ranking",
      "The expandomono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
      "Rankt5: Finetuning T5 for text ranking with ranking losses",
      "Listt5: Listwise reranking with fusion-in-decoder improves zero-shot retrieval",
      "A two-stage adaptation of large language models for text ranking",
      "Q-PEFT: query-dependent parameter efficient finetuning for text reranking with large language models",
      "Rank-without-gpt: Building gpt-independent listwise rerankers on open-source large language models",
      "Leveraging passage embeddings for efficient listwise reranking with large language models",
      "Holistic evaluation of language models",
      "Beyond yes and no: Improving zeroshot LLM rankers via scoring fine-grained relevance labels",
      "Generating diverse criteria on-thefly to improve point-wise LLM rankers",
      "Improving passage retrieval with zero-shot question generation",
      "Open-source large language models are strong zeroshot query likelihood models for document ranking",
      "An investigation of prompt variations for zero-shot llmbased rankers",
      "Discrete prompt optimization via constrained generation for zero-shot re-ranker",
      "Demorank: Selecting effective demonstrations for large language models in ranking task",
      "Parade: Passage ranking using demonstrations with llms",
      "Is chatgpt good at search? investigating large language models as re-ranking agents",
      "Sliding windows are not the end: Exploring full ranking with long-context large language models",
      "Coranking: Collaborative ranking with small and large ranking agents",
      "Zero-shot listwise document reranking with a large language model",
      "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
      "Tourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy",
      "Top-down partitioning for efficient list-wise ranking",
      "APEER : Automatic prompt engineering enhances large language model reranking",
      "Large language models are effective text rankers with pairwise ranking prompting",
      "A setwise approach for effective and highly efficient zero-shot ranking with large language models",
      "Prp-graph: Pairwise ranking prompting to llms with graph aggregation for effective text re-ranking",
      "Consolidating ranking and relevance predictions of large language models through post-processing",
      "Exaranker: Synthetic explanations improve neural rankers",
      "Exaranker-open: Synthetic explanation for IR using open-source llms",
      "Inpars-light: Costeffective unsupervised training of efficient rankers",
      "Generating synthetic documents for crossencoder re-rankers: A comparative study of chatgpt and human experts",
      "Expand, highlight, generate: Rldriven document generation for passage reranking",
      "Rankvicuna: Zero-shot listwise document reranking with open-source large language models",
      "Rankzephyr: Effective and robust zeroshot listwise reranking is a breeze",
      "Instruction distillation makes large language models efficient zero-shot rankers",
      "Reasonrank: Empowering passage ranking with strong reasoning ability",
      "Rank1: Test-time compute for reranking in information retrieval",
      "Rank-k: Test-time reasoning for listwise reranking",
      "REARANK: reasoning re-ranking agent via reinforcement learning",
      "Rank-r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning",
      "Tfrank: Think-free reasoning enables practical pointwise LLM ranking",
      "Learning to rank using gradient descent",
      "Gpt-4: A review on advancements and opportunities in natural language processing",
      "FIRST: faster improved listwise reranking with single token decoding",
      "Few-shot prompting for pairwise ranking: An effective non-parametric retrieval model",
      "BRIGHT: A realistic and challenging benchmark for reasoningintensive retrieval",
      "R2MED: A benchmark for reasoning-driven medical retrieval",
      "Eco-Rank: Budget-constrained text re-ranking using large language models",
      "Attention in large language models yields efficient zero-shot rerankers",
      "Ranked list truncation for large language model-based re-ranking",
      "Retrieval of the best counterargument without prior topic knowledge",
      "Retrieval augmented language model pre-training",
      "Retrievalaugmented generation for knowledge-intensive NLP tasks",
      "REPLUG: retrieval-augmented black-box language models",
      "Atlas: Few-shot learning with retrieval augmented language models",
      "Internet-augmented language models through few-shot prompting for open-domain question answering",
      "Rethinking with retrieval: Faithful large language model inference",
      "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
      "Incontext retrieval-augmented language models",
      "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
      "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
      "Active retrieval augmented generation",
      "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "RETA-LLM: A retrieval-augmented large language model toolkit",
      "Freshllms: Refreshing large language models with search engine augmentation",
      "Improving retrievalaugmented large language models via data importance learning",
      "Richrag: Crafting rich responses for multifaceted queries in retrieval-augmented generation",
      "Single llm, multiple roles: A unified retrieval-augmented generation framework using role-specific token optimization",
      "Longrag: Enhancing retrieval-augmented generation with long-context llms",
      "Enabling large language models to generate text with citations",
      "Activerag: Revealing the treasures of knowledge via active learning",
      "Empowering large language models to set up a knowledge retrieval indexer via self-learning",
      "REAR: A relevance-aware retrievalaugmented framework for open-domain question answering",
      "Search augmented instruction learning",
      "RA-DIT: retrievalaugmented dual instruction tuning",
      "Instructrag: Instruct-ing retrieval-augmented generation with explicit denoising",
      "Unsupervised information refinement training of large language models for retrieval-augmented generation",
      "One token can help! learning scalable and pluggable virtual tokens for retrieval-augmented large language models",
      "R2ag: Incorporating retrieval information into retrieval augmented generation",
      "Making retrieval-augmented language models robust to irrelevant context",
      "Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training",
      "ATM: adversarial tuning multi-agent system makes a robust retrieval-augmented generator",
      "Improving language models via plug-and-play retrieval feedback",
      "Retrieval-generation synergy augmented large language models",
      "IM-RAG: multi-round retrievalaugmented generation through learning inner monologues",
      "Generate-then-ground in retrievalaugmented generation for multi-hop question answering",
      "Language models (mostly) know what they know",
      "How can we know When language models know? on the calibration of language models for question answering",
      "Measuring and narrowing the compositionality gap in language models",
      "Demonstratesearch-predict: Composing retrieval and language models for knowledge-intensive NLP",
      "Planrag: A plan-thenretrieval augmented generation for generative large language models as decision makers",
      "Learning to plan for retrievalaugmented large language models from knowledge graphs",
      "Answering questions by meta-reasoning over multiple chains of thought",
      "Leancontext: Cost-efficient domain-specific question answering using llms",
      "RECOMP: improving retrieval-augmented lms with context compression and selective augmentation",
      "Learning to filter context for retrievalaugmented generation",
      "TCRA-LLM: token compression retrieval augmented large language model for inference cost reduction",
      "PRCA: fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter",
      "xrag: Extreme context compression for retrieval-augmented generation with one token",
      "BIDER: bridging knowledge inconsistency for efficient retrievalaugmented llms via key supporting evidence",
      "Lost in the middle: How language models use long contexts",
      "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
      "Exploring the integration strategies of retriever and large language models",
      "Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models",
      "When not to trust language models: Investigating effectiveness of parametric and nonparametric memories",
      "Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models",
      "Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions",
      "Retrieval helps or hurts? A deeper dive into the efficacy of retrieval augmentation to language models",
      "When do llms need retrieval augmentation? mitigating llms' overconfidence helps retrieval augmentation",
      "Self-knowledge guided retrieval augmentation for large language models",
      "Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms",
      "Tug-of-war between knowl-edge: Exploring and resolving knowledge conflicts in retrieval-augmented language models",
      "Typos that broke the rag's back: Genetic attack on RAG pipeline by simulating documents in the wild via lowlevel perturbations",
      "Badrag: Identifying vulnerabilities in retrieval augmented generation of large language models",
      "Phantom: General trigger attacks on retrieval augmented language generation",
      "Domainrag: A chinese benchmark for evaluating domain-specific retrievalaugmented generation",
      "A tale of trust and accuracy: Base vs. instruct llms in RAG systems",
      "Augmenting blackbox llms with medical textbooks for clinical question answering",
      "ATLANTIC: structureaware retrieval-augmented language model for interdisciplinary science",
      "Crosslingual retrieval augmented in-context learning for bangla",
      "Clinfo.ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature",
      "Enhancing financial sentiment analysis via retrieval augmented large language models",
      "Interpretable long-form legal question answering with retrievalaugmented large language models",
      "M-RAG: reinforcing large language model performance through retrieval-augmented generation with multiple partitions",
      "Don't forget private retrieval: distributed private similarity search for large language models",
      "Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models",
      "Ralle: A framework for developing and evaluating retrieval-augmented large language models",
      "Flashrag: A modular toolkit for efficient retrieval-augmented generation research",
      "Lamda: Language models for dialog applications",
      "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
      "Webglm: Towards an efficient web-enhanced question answering system with human preferences",
      "A real-world webagent with planning, long context understanding, and program synthesis",
      "Teaching language models to support answers with verified quotes",
      "Know where to go: Make LLM a relevant, responsible, and trustworthy searchers",
      "Cosearchagent: A lightweight collaborative search agent with large language models",
      "Search-r1: Training llms to reason and leverage search engines with reinforcement learning",
      "Research: Learning to reason with search for llms via reinforcement learning",
      "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "R1-searcher: Incentivizing the search capability in llms via reinforcement learning",
      "START: self-taught reasoner with tools",
      "Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward",
      "Kwaiagents: Generalized informationseeking agent system with large language models",
      "Ai-searchplanner: Modular agentic search via pareto-optimal multiobjective reinforcement learning",
      "Mindsearch: Mimicking human minds elicits deep AI searcher",
      "Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal selfevolution",
      "OWL: optimized workforce learning for general multi-agent assistance in real-world task automation",
      "A cognitive writing perspective for constrained longform text generation",
      "Cosearchagent: A lightweight collaborative search agent with large language models",
      "Search-o1: Agentic searchenhanced large reasoning models",
      "Agent laboratory: Using LLM agents as research assistants",
      "The AI scientist: Towards fully automated open-ended scientific discovery",
      "Auto-rag: Autonomous retrieval-augmented generation for large language models",
      "Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis",
      "Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning",
      "Zerosearch: Incentivize the search capability of llms without searching",
      "Open-rag: Enhanced retrieval augmented reasoning with opensource large language models",
      "Deeprag: Thinking to retrieve step by step for large language models",
      "ATLAS: agent tuning via learning critical steps",
      "Chain-of-retrieval augmented generation",
      "Auto-rag: Autonomous retrieval-augmented generation for large language models",
      "Agentic reinforced policy optimization",
      "Webagentr1: Training web agents via end-to-end multi-turn reinforcement learning",
      "Deepresearcher: Scaling deep research via reinforcement learning in real-world environments",
      "Webthinker: Empowering large reasoning models with deep research capability",
      "Webdancer: Towards autonomous information seeking agency",
      "Websailor: Navigating super-human reasoning for web agent",
      "Webshaper: Agentically data synthesizing via information-seeking formalization",
      "Webwatcher: Breaking new frontiers of vision-language deep research agent",
      "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "Measuring short-form factuality in large language models",
      "When not to trust language models: Investigating effectiveness of parametric and nonparametric memories",
      "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps",
      "L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, S. Shi, M. Choi, A. Agrawal, A. Chopra, A. Khoja, R. Kim, J. Hausenloy, O. Zhang, M. Mazeika, D. An- derson, T. Nguyen, M. Mahmood, F. Feng, S. Y. Feng, H. Zhao, M. Yu, V. Gangal, C. Zou, Z. Wang, J. P. Wang, P. Kumar, O. Pokutnyi, R. Gerbicz, S. Popov, J. Levin, M. Kazakov, J. Schmitt, G. Galgon, A. Sanchez, Y. Lee, W. Yeadon, S. Sauers, M. Roth, C. Agu, S. Riis, F. Giska, S. Utpala, Z. Giboney, G. M. Goshu, J. of Arc Xavier, S. Crowson, M. M. Naiya, N. Burns, L. Finke, Z. Cheng, H. Park, F. Fournier- Facio, J. Wydallis, M. Nandor, A. Singh, T. Gehrunger, J. Cai, B. McCarty, D. Duclosel, J. Nam, J. Zam- pese, R. G. Hoerr, A. Bacho, G. A. Loume, A. Galal, H. Cao, A. C. Garretson, D. Sileo, Q. Ren, D. Cojoc, P. Arkhipov, U. Qazi, L. Li, S. Motwani, C. S. de Witt, E. Taylor, J. Veith, E. Singer, T. D. Hartman, P. Ris- sone, J. Jin, J. W. L. Shi, C. G. Willcocks, J. Robin- son, A. Mikov, A. Prabhu, L. Tang, X. Alapont, J. L. Uro, K. Zhou, E. de Oliveira Santos, A. P. Maksimov, E. Vendrow, K. Zenitani, J. Guillod, Y. Li, J. Vendrow, V. Kuchkin, and N. Ze-An, \"Humanity's last exam,\" CoRR, vol. abs/2501.14249, 2025.",
      "Browsecomp: A simple yet challenging benchmark for browsing agents",
      "GAIA: a benchmark for general AI assistants",
      "Assistantbench: Can web agents solve realistic and time-consuming tasks",
      "Magentic-one: A generalist multi-agent system for solving complex tasks",
      "Swe-bench: Can language models resolve real-world github issues?",
      "Octopack: Instruction tuning code large language models",
      "Mle-bench: Evaluating machine learning agents on machine learning engineering",
      "Mlagentbench: Evaluating language agents on machine learning experimentation",
      "Re-bench: Evaluating frontier AI r&d capabilities of language model agents against human experts",
      "Researchtown: Simulator of human research community",
      "Webarena: A realistic web environment for building autonomous agents",
      "Spabench: a comprehensive benchmark for smartphone agent evaluation",
      "Webwalker: Benchmarking llms in web traversal",
      "Intent5: Search result diversification using causal language models",
      "GPT-4 technical report",
      "Mean reciprocal rank",
      "Cumulated gain-based evaluation of IR techniques",
      "Bleu: a method for automatic evaluation of machine translation",
      "ROUGE: A package for automatic evaluation of summaries",
      "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus",
      "Halueval: A large-scale hallucination evaluation benchmark for large language models",
      "Beyond factuality: A comprehensive evaluation of large language models as knowledge generators",
      "Ai-generated images introduce invisible relevance bias to text-image retrieval",
      "Llms may dominate information access: Neural retrievers are biased towards llmgenerated texts",
      "Generative agents: Interactive simulacra of human behavior"
    ]
  }
}