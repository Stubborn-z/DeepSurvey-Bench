{
  "survey": "Large Language Models (LLMs) are revolutionizing the field of Information Retrieval (IR) by enhancing semantic search and AI-driven retrieval systems. This survey provides a comprehensive exploration of LLMs' applications in IR, emphasizing their transformative potential in improving search accuracy and relevance. LLMs leverage advanced natural language processing techniques to overcome traditional retrieval limitations, enabling more nuanced and context-aware search experiences. By integrating generative retrieval and semantic matching, LLMs enhance performance in complex queries, such as those encountered in e-commerce and legal case retrieval. Despite their advancements, challenges persist, including computational constraints, data privacy concerns, and issues of accuracy and bias. The survey identifies key areas for future research, including optimizing model efficiency, enhancing adaptability for domain-specific applications, and addressing ethical considerations. Innovative frameworks and benchmarks are crucial for evaluating LLM performance and guiding future advancements. Overall, the survey underscores the potential of LLMs to transform IR, paving the way for more accurate, contextually relevant, and efficient search experiences, while highlighting the need for ongoing research to address existing challenges and harness the full potential of LLMs in IR systems.\n\nIntroduction Significance of Large Language Models in Information Retrieval Large language models (LLMs) have revolutionized information retrieval (IR) by significantly enhancing search capabilities and addressing the limitations of traditional methods. Their sophisticated natural language processing (NLP) techniques facilitate nuanced, context-aware search experiences, particularly in optimizing semantic matching for complex queries, such as those prevalent in e-commerce, where LLMs excel in handling long-tail queries [1,2]. A key advancement is instruction tuning, which employs machine-generated data to improve zero-shot capabilities across diverse tasks, showcasing LLM adaptability to various IR challenges, including specialized applications like financial sentiment analysis, where conventional NLP models often fall short [3,4]. The emergence of lightweight, state-of-the-art open models, such as the Gemini models, further illustrates the ongoing evolution and significance of LLMs in the AI landscape [5]. LLMs have also transformed dialog applications, as seen with the LaMDA family, which addresses critical issues in safety and factual grounding, enhancing user engagement in conversational systems [6]. This enhancement is crucial for social chatbots, where LLMs contribute to more engaging user experiences [7]. Moreover, LLMs improve legal case retrieval by integrating expert knowledge, vital for ensuring judicial fairness [8]. The impact of LLMs extends to complex reasoning tasks, filling knowledge gaps in reasoning capabilities [9]. This ability is essential for developing fully automated agents capable of scientific research and knowledge discovery, surpassing traditional methods that merely assist human scientists [10]. Additionally, LLMs mitigate issues of hallucination and factual inaccuracies by actively retrieving information from external resources during text generation, thus enhancing reliability and accuracy in information retrieval [11]. The evolving field of IR underscores the importance of LLMs in enhancing text understanding, generation, and knowledge inference, which collectively improve search result accuracy and relevance. LLMs facilitate generative retrieval, contributing to user understanding and model evaluation, establishing a synergistic relationship with IR models and human evaluators. This new paradigm not only optimizes information seeking but also addresses challenges like computational costs and credibility concerns. Recent studies indicate that LLMs can generate query variants and text with citations, enhancing factual correctness and verifiability, although improvements are still needed in citation support and synthesizing information from multiple sources. In specialized domains such as legal case retrieval, LLMs, guided by expert knowledge, can distill complex cases into concise sub-facts, thereby improving retrieval performance and interpretability. These developments highlight the transformative potential of LLMs in advancing the IR field [12,13,8,14]. Objectives of the Survey This survey systematically explores the diverse applications of large language models (LLMs) in information retrieval (IR), emphasizing their role in enhancing semantic search and AI-driven retrieval systems. It aims to bridge the gap between traditional retrieval models and contemporary data-driven approaches, addressing inefficiencies and hallucination issues prevalent in conventional systems. A critical focus is on examining pre-training, adaptation tuning, and utilization techniques employed by LLMs, evaluating their transformative potential in IR processes [15]. The survey seeks to uncover the mechanisms behind the impressive performance of AI-generated content techniques, such as ChatGPT, DALL-E-2, and Codex, providing insights into their applications in IR [16]. It also highlights the need for advancements in sophisticated reasoning patterns, currently lacking in open-source models, which limits their effectiveness in navigating vast information landscapes [17]. This includes leveraging dense retrieval approaches that utilize pre-trained language models (PLMs) to enhance retrieval performance in knowledge-intensive tasks [18]. Additionally, the survey examines innovative frameworks like the multi-task learning paradigm, incorporating knowledge prediction and goal selection to improve the dialogue process [19]. It investigates comprehensive training paradigms aimed at developing end-to-end agentic information-seeking agents, exemplified by frameworks such as WebDancer [20]. The role of deep relevance matching models (DRMM), designed to meet the needs of ad-hoc retrieval through joint deep architectures, is also assessed [21]. Moreover, the survey includes a comparative analysis of advanced language models like GPT-4, focusing on their features and applications in IR [22]. By developing robust benchmarks and fostering innovation, as demonstrated by initiatives like MindSearch, the survey aims to provide researchers with accessible, replicable models that enhance understanding of user-system interactions [23]. Through these objectives, the survey aspires to offer a comprehensive understanding of the current landscape and future directions of LLMs in enhancing information retrieval. Importance of Improving Search Accuracy and Relevance Enhancing search accuracy and relevance in information retrieval (IR) systems through large language models (LLMs) is essential due to inherent challenges and limitations in existing methodologies. Traditional IR systems often inadequately leverage the sequential order and contextual relatedness of user queries, which are vital for delivering personalized and context-aware search results [24]. Furthermore, the static nature of the internal knowledge bases used by many LLMs can result in outdated or inaccurate information, particularly for time-sensitive or knowledge-intensive queries. The inefficiencies and high costs associated with conventional IR methods necessitate significant improvements to facilitate scientific advancement and efficient information dissemination [25]. These systems often depend on expensive computational resources and require substantial amounts of hand-labeled data, limiting scalability and efficiency [26]. Additionally, complex information requests that require synthesizing data from multiple web sources pose further challenges, highlighting the need for improved retrieval accuracy and relevance [23]. Efforts to tackle these challenges are exemplified by systems like CoSearchAgent, which utilize LLMs to better understand user queries and contextual nuances, thereby enhancing search outcomes [27]. However, reasoning-intensive ranking models, crucial for assessing the relevance of retrieved items, face practical limitations due to high computational costs and latency associated with large-scale LLMs. Effective reranking strategies are essential for evaluating the relevance of items within a list, thereby improving the overall quality of IR tasks [28]. The pursuit of enhanced search accuracy and relevance is further complicated by current limitations of AI models, which, while capable of assisting in various research aspects, cannot autonomously execute the entire scientific process, thus constraining their potential in driving scientific discovery [10]. Nevertheless, advancements like the LaMDA model represent significant progress towards creating safer and more reliable dialog systems, integral to the evolution of IR technologies [6]. Continuous enhancement of search accuracy and relevance in IR systems is imperative for overcoming existing limitations and ensuring users receive the most pertinent and up-to-date information available. Structure of the Survey This survey is meticulously designed to provide an in-depth examination of the application of large language models (LLMs) in information retrieval (IR), emphasizing their transformative role in advancing semantic search and AI-driven retrieval systems. It explores the synergistic integration of IR models, LLMs, and human evaluators, presenting a novel technical paradigm that enhances generative retrieval, user understanding, and interaction. Despite challenges such as computational costs and ethical considerations, the survey draws insights from strategic workshops and empirical studies, demonstrating how LLMs serve as versatile backbone encoders, improving in-domain accuracy, zero-shot generalization, and multi-task learning across various retrieval tasks. It also discusses the retrieval-augmented framework for specific domains like financial sentiment analysis, where LLMs significantly outperform traditional models by leveraging extensive pre-training and retrieval augmentation for context enhancement [29,12,4]. The paper begins with an introduction outlining the significance of LLMs in IR, detailing their role in overcoming traditional retrieval methods' limitations and improving search accuracy and relevance. The introduction also sets the survey's objectives, focusing on exploring LLM applications in IR and the necessity for enhanced search accuracy. Following the introduction, the background section provides a historical overview of IR systems, discussing traditional methods and the integration of natural language processing (NLP) technologies in IR. It introduces LLMs and their capabilities in understanding and generating human language, setting the stage for subsequent sections focusing on specific applications and challenges. The third section delves into the diverse applications of LLMs in IR, including enhancing semantic search, query rewriting, and retrieval-augmented generation. It discusses frameworks and benchmarks, providing examples of systems that utilize LLMs for improved retrieval performance while highlighting innovative approaches employed to refine user queries and facilitate personalized search experiences. The fourth section examines LLM capabilities, focusing on strengths in understanding context, generating relevant responses, and improving retrieval accuracy. It discusses dynamic retrieval strategies and innovative training techniques that enhance LLM performance across various IR tasks. The fifth section identifies challenges faced when integrating LLMs into IR systems, such as computational constraints, data privacy concerns, and issues of accuracy and bias. It addresses difficulties in evaluating and benchmarking LLM performance, as well as integration limitations with existing IR systems. Finally, the survey concludes by highlighting future research and development directions for LLMs in IR, including optimizing model efficiency, enhancing adaptability, and addressing ethical considerations. The conclusion emphasizes the transformative impact of LLMs on IR, underscoring their potential to redefine the field by improving generative retrieval, user understanding, and user-system interactions. It stresses the importance of ongoing advancements and innovation, noting the synergistic relationship between IR models, LLMs, and humans in creating a more powerful information-seeking paradigm. Despite challenges such as computational costs and ethical concerns, LLMs are pivotal in addressing diverse user needs, enhancing factual correctness, and generating query variants, underscoring the necessity for continuous research and development in this domain [1,12,13,8,14].The following sections are organized as shown in . Background Evolution of Information Retrieval Systems Information retrieval (IR) systems have evolved significantly with the integration of Large Language Models (LLMs) and neural ranking models, enhancing text comprehension, generation, and knowledge inference. Neural models bridge the vocabulary gap between queries and documents, while benchmarks like BEIR evaluate IR models across domains, highlighting strengths and limitations of lexical, sparse, dense, and re-ranking architectures. Despite challenges such as computational costs and credibility concerns, these innovations foster a new paradigm in information seeking, emphasizing interaction between IR models, LLMs, and users [30,12,31]. Initially, IR systems relied on keyword matching techniques, limiting context comprehension and often yielding irrelevant results. Statistical models like the vector space model and probabilistic models introduced term frequency and inverse document frequency (TF-IDF) to enhance document relevance [23]. The digital information surge necessitated sophisticated algorithms and indexing techniques for improved retrieval performance. Natural language processing (NLP) technologies marked a pivotal advancement, enabling IR systems to grasp syntactic and semantic structures of queries and documents. Machine learning and deep learning techniques further accelerated this evolution, allowing models to learn from extensive datasets and adapt to new IR challenges [24,25]. Recently, LLMs revolutionized IR with their ability to process and generate human-like text, utilizing pre-trained models like BERT and T5 for semantic search and query expansion, significantly improving accuracy and relevance in dense and instruction-based retrieval. In applications like financial sentiment analysis, LLMs augmented with retrieval mechanisms achieve notable accuracy improvements [29,4]. AI and machine learning advancements continue to propel IR systems' evolution, paving the way for intelligent information retrieval solutions. Traditional Methods in Information Retrieval Traditional information retrieval (IR) methods primarily follow the index-retrieve paradigm, relying on pre-constructed indices, challenging optimization for specific targets [32]. Sparse vector space models, such as TF-IDF and BM25, dominate passage retrieval in open-domain question answering [33], representing documents and queries as high-dimensional vectors, with relevance determined by proximity. Traditional methods face inefficiencies in list-wise ranking, like the sliding window technique, which often fails to rank multiple documents effectively [34]. Pseudo-Relevance Feedback (PRF) techniques aim to enhance recall by expanding queries with top-ranked document terms but struggle to improve overall effectiveness [35]. Query expansion techniques widely adopted for first-stage retrievers remain under-explored for second-stage rankers, revealing optimization gaps in retrieval pipelines [36]. Traditional IR approaches often assume fixed user behavior sequences, neglecting inherent variability in interactions [37], hindering effective personalization and adaptation to preferences. Attempts to extract user behavior data often fall short of achieving true personalization [24]. Inefficiencies are further underscored by the lack of coherence between retrieved information and applied reasoning, noted in critiques of current web data collection and question generation practices [38]. Benchmarks like comparisons between Generative Relevance Feedback (GRF) and PRF illustrate ongoing challenges in advancing traditional IR methodologies [39]. Role of Natural Language Processing in IR Natural Language Processing (NLP) significantly advances information retrieval (IR) by enabling systems to understand and generate human language, transitioning IR from keyword-based methods to sophisticated approaches considering semantic and syntactic structures of queries and documents. This shift enhances relevance and precision, addressing limitations of traditional methods that struggled with polysemy and synonymy [23]. NLP contributes to semantic search capabilities, allowing retrieval systems to interpret user query intent rather than relying solely on exact matches. Techniques like word embeddings and transformer-based models capture nuanced meanings in context, improving understanding of relationships and enhancing retrieval of relevant documents despite vocabulary differences [15]. NLP facilitates query expansion and rewriting strategies, broadening search scopes, generating additional terms, or reformulating queries for better alignment with document language, crucial in overcoming vocabulary mismatch challenges [21]. NLP technologies have led to retrieval-augmented generation (RAG) frameworks, combining generative capabilities with traditional retrieval methods for more informative responses [22]. By integrating retrieval with language generation, RAG frameworks enhance IR systems' ability to provide comprehensive responses, particularly for complex queries requiring synthesis from multiple sources [28]. NLP's impact on IR processes is profound, driving innovations that improve accuracy, relevance, and efficiency, enabling IR systems to understand and respond to queries more intuitively [6]. Introduction to Large Language Models Large Language Models (LLMs) have emerged as a transformative force in natural language processing (NLP) and information retrieval (IR), offering unparalleled capabilities in understanding and generating human language with high contextual accuracy. Built on advanced transformer architectures, LLMs redefine the IR landscape by processing complex language patterns and generating coherent responses [40], particularly in semantic search and sentence embeddings, addressing traditional limitations [40]. A key advancement in LLMs is generative retrieval, reframing classic IR as a sequence-to-sequence modeling task [41]. This approach allows LLMs to generate answers directly from input sequences, integrating generation and retrieval into a cohesive framework. Datasets like CommitPack exemplify instruction tuning's role in refining LLM capabilities for domain-specific tasks like code generation and understanding [42]. Instruction tuning expands LLM utility across diverse IR scenarios. Techniques like QLoRA enable efficient finetuning by reducing memory usage through innovative quantization methods, making models more accessible for various applications [43], crucial for deploying LLMs in environments with limited computational resources. LLMs demonstrate remarkable adaptability in real-world applications, as illustrated by systems like WebAgent, which learn from self-experience to automate tasks on websites, showcasing LLMs' potential to plan, summarize HTML, and generate executable code autonomously [44]. These capabilities highlight LLMs' transformative impact on automating complex tasks and enhancing IR system efficiency. LLMs advance NLP and IR by improving human language comprehension and generation, enabling more intuitive search experiences across domains. They contribute significantly to the evolving IR paradigm by integrating internal knowledge with real-time information from IR models, while users evaluate information reliability. Despite challenges like computational costs, credibility concerns, and ethical considerations, LLMs revolutionize AI development, addressing diverse NLP tasks and exhibiting unique capabilities at larger scales, evidenced by advancements like ChatGPT [45,12]. As these models evolve, they promise to transform IR further by adapting to new challenges and delivering contextually-aware solutions. Semantic Search and AI-driven Retrieval Semantic search and AI-driven retrieval represent a significant shift in information retrieval (IR), leveraging large language models (LLMs) to interpret user query intent and deliver more relevant, contextually accurate results. Unlike traditional keyword-based methods, semantic search utilizes LLMs' deep understanding of language semantics to comprehend meanings and relationships, facilitating sophisticated matching processes [2], advantageous for handling complex and long-tail queries [2]. AI-driven retrieval systems, enhanced by LLMs, incorporate advanced techniques like retrieval-augmented generation (RAG), combining retrieval and generation processes for more informative responses. The CoRAG model exemplifies improvements in traditional RAG methods by enabling iterative query refinement, addressing limitations of static retrieval methods [46]. This iterative approach ensures a dynamically guided retrieval process by evolving query understanding, enhancing information quality. LLM integration with retrieval systems facilitates frameworks like IRCoT, interleaving retrieval with chains of thought (CoT) reasoning for multi-step question answering. This method improves attribution of generated outputs, reducing model hallucination and enhancing factual accuracy, demonstrated by substantial improvements in performance across datasets [47,48,49,50]. Such frameworks illustrate LLMs' potential to augment retrieval processes with advanced reasoning capabilities, leading to accurate and reliable search outcomes. Models like CorpusBrain further illustrate LLMs' transformative impact on semantic search and AI-driven retrieval, encoding all information within parameters to eliminate additional indexing, improving retrieval efficiency [51], streamlining processes, and enhancing scalability [41]. Benchmarks like Gecko emphasize data quality in training compact models for superior performance [52], setting new standards for evaluating and enhancing AI-driven retrieval systems, underscoring high-quality training data's critical role in achieving optimal results. In recent years, large language models (LLMs) have significantly transformed the landscape of information retrieval, leading to a myriad of advancements that enhance the efficacy of search systems. As illustrated in , the hierarchical categorization of applications of LLMs in this domain highlights key advancements, including semantic search, query rewriting, retrieval-augmented generation, conversational search, and evaluation frameworks. Each category is meticulously broken down into specific innovations and challenges, which underscores the transformative impact of LLMs in improving retrieval accuracy, user engagement, and overall system performance. This comprehensive overview not only elucidates the diverse applications of LLMs but also sets the stage for a deeper exploration of their implications in the field of information retrieval. Applications of Large Language Models in Information Retrieval Enhancing Semantic Search Large language models (LLMs) have revolutionized semantic search in information retrieval (IR) systems by leveraging advanced natural language processing techniques to improve accuracy and relevance. The SGPT method, which utilizes decoders for generating effective sentence embeddings, exemplifies this advancement by capturing nuanced meanings in queries and documents, thus enhancing semantic relationships beyond traditional keyword-based methods [40]. Benchmarks like Rank1 further illustrate the integration of LLMs in IR systems, improving smaller models' performance through reasoning during the reranking process, thereby ensuring retrieval outcomes are informed by sophisticated reasoning capabilities [53]. As depicted in , this figure illustrates the advancements in semantic search through the integration of LLMs, domain-specific applications, and the challenges faced along with future directions. Key methods and benchmarks such as SGPT, Rank1, and generative retrieval highlight the role of LLMs in enhancing information retrieval systems. Generative retrieval techniques merge generation and retrieval into a unified framework, allowing LLMs to excel in smaller corpora and addressing scaling challenges in larger datasets [41]. In legal information retrieval, frameworks like KELLER demonstrate LLMs' ability to distill complex legal cases into concise sub-facts, enhancing retrieval accuracy in specialized domains [8]. Innovations such as WebAgent showcase LLMs' adaptability in dynamic environments, refining semantic search processes in web applications [44]. Comprehensive benchmarks for language understanding, reasoning, and safety further enhance semantic search accuracy by providing structured evaluation frameworks [5]. LLMs have transformed IR systems by enhancing text understanding, generation, and knowledge inference, offering intuitive search experiences. Retrieval-augmented frameworks address domain-specific limitations and credibility concerns, improving accuracy significantly in fields like financial sentiment analysis. Despite challenges such as computational costs and ethical considerations, approaches like SlimPLM and ALCE demonstrate notable performance improvements, paving the way for a new technical paradigm in information seeking [54,12,13,4]. Through dynamic query adjustments, efficient parameter usage, and deep contextual understanding, LLMs continue to redefine semantic search capabilities, driving future advancements. Query Rewriting and Expansion Query rewriting and expansion are crucial for optimizing information retrieval (IR) systems, refining user queries for more accurate search outcomes. Large language models (LLMs) offer innovative techniques for generating contextually rich query rewrites, surpassing traditional methods reliant on static annotations and predefined rewards, particularly in Retrieval Augmentation Generation (RAG) systems [55]. The BEQUE method exemplifies LLMs' potential in refining long-tail queries, improving search results on platforms like e-commerce [2]. Interactive tools like the Query Generation Assistant enhance LLM-generated queries, aligning them closely with user intent for more effective retrieval [56]. Prompt-based strategies guide LLMs in generating context-independent queries for conversational search, emphasizing adaptability in diverse retrieval contexts [57]. LLMs optimize user queries by transforming context-dependent queries into informative standalone forms, enhancing retrieval performance. This approach facilitates generative retrieval and fosters a synergistic relationship among IR models, LLMs, and human evaluators. Despite challenges such as computational costs and credibility concerns, LLMs signify a transformative shift in IR, supported by insights from the Chinese IR community and experimental evaluations demonstrating substantial retrieval performance improvements, particularly in conversational search scenarios [54,12,58]. Through innovative methods and frameworks, LLMs continue to enhance retrieval accuracy and relevance, advancing search experiences. Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) represents a pivotal advancement in integrating large language models (LLMs) within information retrieval (IR) systems, enhancing response depth and precision through dynamic interaction with external data sources. Incorporating retrieval processes into LLM generative workflows allows for contextually rich and accurate outputs, addressing limitations of static models [1]. Experiments demonstrate that retrieval augmentation significantly enhances LLMs' question-answering accuracy [1]. Innovations like Pseudo-Graph Retrieval-Augmented Generation (PG-RAG) enable LLMs to generate concise mental indices from raw data, facilitating knowledge retrieval and enhancing generative capabilities [59]. Methods like FLARE guide document retrieval to regenerate sentences with low-confidence tokens, boosting predictive accuracy [11]. Instruction tuning further enhances zero-shot performance in diverse retrieval scenarios [3]. Novel reranking models, such as Rank1, introduce test-time computation strategies to improve retrieval performance, showcasing the intersection of reranking techniques and retrieval augmentation [53]. Benchmarks assessing ranking capabilities of LLMs provide insights into optimizing retrieval strategies for improved accuracy [60]. The integration of retrieval processes with LLMs through RAG frameworks signifies substantial innovation in IR. Self-Reflective Retrieval-Augmented Generation (Self-RAG) enhances contextual reliability and accuracy, outperforming models like ChatGPT and Llama2-chat in tasks like open-domain QA, reasoning, and fact verification. This approach reduces factual inaccuracies by selectively retrieving relevant information, improving factuality and citation accuracy in long-form content. Recent studies challenge the conventional preference for instructed LLMs in RAG systems, revealing that base models can outperform them, prompting a reevaluation of strategies in contemporary IR solutions [61,62]. Conversational Search and Personalization Conversational search and personalization are critical components of modern information retrieval (IR) systems, where large language models (LLMs) enhance user engagement and tailor search experiences. LLM integration into conversational search frameworks significantly improves retrieval performance, particularly when LLM-generated query rewrites are utilized with sparse retrieval systems, offering contextually rich and relevant outcomes compared to traditional queries [58]. A proactive approach to conversational search emphasizes leading goal-oriented conversations rather than merely reacting, enhancing response relevance and user satisfaction [19]. Innovations like ChatRetriever adapt LLMs for conversational dense retrieval through dual-learning approaches, refining retrieval by leveraging both user queries and system responses [63]. Analyses of social chatbots reveal strategies for engaging users emotionally and responsively, showcasing LLMs' role in creating personalized interactions [7]. LLMs in conversational search and personalization signify a major advancement in IR systems, offering intuitive and responsive experiences. Through ongoing innovations and sophisticated frameworks, LLMs redefine IR capabilities, enhancing text understanding, generation, and knowledge inference. These advancements facilitate generative retrieval and improve user understanding, model evaluation, and user-system interactions, creating a new technical paradigm in information seeking. LLMs contribute internal knowledge while IR models provide real-time information, enabling personalized interactions. Challenges such as computational costs, credibility concerns, and ethical considerations persist. Insights from the Chinese IR community highlight LLMs' transformative impact, emphasizing mutual enhancement of LLMs and IR systems and the potential for further innovation in personalized experiences [64,12,65]. Frameworks and Benchmarks Frameworks and benchmarks are integral to advancing large language models (LLMs) in information retrieval (IR), providing structured environments for evaluating and optimizing retrieval performance. The Modular Text Embedding Benchmark (MTEB) exemplifies a comprehensive evaluation framework for refining text embeddings, crucial for enhancing semantic search capabilities [66]. DeepRAG models retrieval-augmented reasoning as a Markov Decision Process, integrating reasoning with retrieval to generate contextually relevant responses [67]. The Chameleon framework introduces a disaggregated architecture for independent scaling of LLM and vector search accelerators, improving scalability and efficiency [68]. FlashRAG offers a customizable modular framework with pre-implemented RAG methods, providing flexibility and comprehensiveness for evaluating RAG configurations [69]. LongRAG showcases strong retrieval performance across datasets, indicating its potential as a viable alternative to existing RAG methods [70]. The phi-1 introduction in the Textbooks benchmark sets a new standard for evaluating smaller language models, highlighting their practical potential [71]. HELM adopts a multi-metric approach, evaluating models across diverse scenarios for nuanced performance understanding [72]. Experiments using the BEIR benchmark emphasize efficiency and ranking performance of models like FIRST [31]. Search-R1 experiments reveal significant improvements over RAG baselines, showcasing advancements in retrieval performance for models like Qwen2.5-7B [73]. The SGPT method tested on the BEIR search benchmark illustrates competitive sentence embedding performance [40]. Table provides a detailed overview of representative benchmarks integral to evaluating and optimizing large language models in information retrieval systems. These frameworks and benchmarks represent significant advancements in applying LLMs in IR, providing structured environments for evaluating and enhancing retrieval performance. By integrating innovative approaches and offering comprehensive evaluation tools, they advance IR systems by enabling exploration of LLMs for improved text understanding, generative retrieval, and user-system interactions. Robust benchmarks like BEIR assess zero-shot performance of diverse IR models, while frameworks like RaLLe optimize retrieval-augmented LLMs, collectively driving IR systems' evolution, addressing challenges like computational costs and domain-specific limitations, and paving the way for more effective, efficient, and generalizable search experiences [31,12,74]. Capabilities of Large Language Models Contextual Understanding and Adaptability Large language models (LLMs) have significantly advanced contextual understanding and adaptability, enhancing information retrieval (IR) beyond traditional methods. Demonstration rerankers like DemoRank utilize contextual dependencies to refine ranking strategies, improving search outcomes [75]. The BEQUE method addresses semantic gaps in long-tail queries, showcasing LLMs' ability to enhance user experience through query expansion and improved semantic matching [2]. Techniques like PG-RAG enable LLMs to autonomously organize information into pseudo-graph databases, enhancing knowledge retrieval [59]. Systems such as ARPO mimic human research dynamics, optimizing multi-turn reasoning tasks and search operations, demonstrating LLMs' versatility in refining reasoning strategies [76,12]. Table provides a comprehensive comparison of different methods utilized in large language models, illustrating their effectiveness in contextual understanding, semantic matching, and reasoning capabilities. Innovative training methodologies and the development of large reasoning models have enhanced LLM reasoning capabilities [9]. The SGPT method exemplifies LLM adaptability in semantic search, particularly in evolving web contexts [40]. Frameworks like QLoRA employ memory-saving techniques to enhance LLM adaptability while minimizing computational overhead [43]. The integration of LLMs into IR systems revolutionizes text understanding, generation, and knowledge inference, enabling real-time, relevant information delivery. Despite challenges such as computational costs and ethical considerations, LLMs' adaptability in low-resource settings and conversational query rewriting underscores their transformative potential in advancing IR systems [57,12]. Through dynamic retrieval processes and enhanced reasoning capabilities, LLMs redefine IR systems, fostering more accurate and contextually relevant search experiences. Generative Capabilities and Response Quality LLMs exhibit strong generative capabilities that significantly enhance response quality in IR systems. The integration of retrieval processes with generative models, as seen in frameworks like Rethinking with Retrieval (RR), leads to substantial improvements in response accuracy and efficiency without additional training [48]. Larger models consistently outperform smaller ones in fluency and attribution, as demonstrated by BloombergGPT, which excels in financial tasks while maintaining strong general performance [77,78]. Advanced training techniques, such as those used in RocketQA, enhance the retrieval capabilities of dense passage models, contributing to superior performance in IR tasks compared to traditional supervised methods [79,60]. The integration of reasoning capabilities, demonstrated by models trained on the Rank1 benchmark, further refines response quality through advanced reasoning strategies [53]. Metrics for evaluating models, like those used in WebGLM, emphasize the importance of both accuracy and efficiency in response generation [80]. The Co-Prompt method exemplifies LLM adaptability in enhancing relevance without parameter tuning [81]. LLMs' generative capabilities play a pivotal role in advancing IR systems, facilitating more accurate, contextually relevant, and reliable search experiences. The synergy among LLMs, IR models, and human evaluators creates a powerful new paradigm for information seeking, despite challenges such as computational costs and ethical considerations. LLMs enhance factual correctness through internal knowledge and citation generation, redefining information retrieval and promising future advancements in retrievers and long-context capabilities [12,13]. Scalability and Efficiency The scalability and efficiency of LLMs are crucial for their effectiveness in IR systems. Larger models capture complex data patterns, enhancing retrieval accuracy, as demonstrated by the GTR framework, which leverages LLM scale for improved performance across diverse domains [82]. The Chameleon framework optimizes resource allocation and processing efficiency through independent scaling, allowing tailored adjustments of LLM and vector search components [68]. Innovative methodologies, such as top-down parallelization of ranking processes, minimize computational overhead and enhance efficiency, enabling timely and accurate retrieval outcomes [34]. Routing Networks evaluated across various scales highlight LLM scalability in accommodating diverse retrieval tasks [83]. Additionally, Prompt Tuning allows the reuse of a single large model across multiple tasks, reducing computational costs associated with full model tuning [84]. Ongoing innovations in resource optimization and processing methodologies are pivotal for advancing IR systems, enabling them to handle diverse tasks with improved accuracy and reduced costs. LLMs enhance text understanding, generation, and knowledge inference, leading to more efficient search experiences. Despite challenges like computational costs and credibility concerns, advancements such as retrieval-augmented LLMs and knowledge-guided case reformulation are addressing these issues, with tools like RETA-LLM and SlimPLM refining citation generation and optimizing knowledge acquisition [12,85,13,54,8]. Dynamic Retrieval Strategies Dynamic retrieval strategies employed by LLMs represent a significant advancement in IR, enhancing adaptability and efficiency. The Generative Retrieval Model (GRM) estimates the relevance of generated documents by comparing them to real documents identified through a neural re-ranker, allowing for precise retrieval outcomes by dynamically adjusting relevance assessments [86]. The Privacy-Aware Retrieval Augmented Generation (PRAG) framework implements secure computations to ensure user privacy, preventing any single server from accessing complete queries or databases [87]. The Agentic Retrieval with Proactive Optimization (ARPO) framework utilizes an entropy-based adaptive rollout mechanism, enabling LLMs to dynamically adjust retrieval strategies based on evolving contexts, thereby optimizing exploration and improving overall retrieval performance [88]. These dynamic retrieval strategies showcase LLMs' transformative potential in advancing IR systems through innovative methodologies. By continually refining these strategies, LLMs enhance accuracy, contextual relevance, and efficiency in search experiences, fundamentally transforming the field of information retrieval. LLMs excel in text understanding, generation, and knowledge inference, forming a synergistic relationship with IR models and human evaluators, despite challenges like computational costs and credibility concerns. They advance towards generating text with citations, improving factual correctness and paving the way for future developments in better retrievers and multi-source information synthesis [12,13]. Innovative Training and Optimization Techniques Innovative training and optimization techniques are crucial for enhancing LLM capabilities, enabling them to perform complex IR tasks with increased efficiency. The AttendOut method addresses challenges faced by self-attention models through a tailored dropout approach, improving robustness and generalization [89]. Generating high-quality training data from live web search environments, as demonstrated by SimpleDeep, optimizes input and output diversity, enhancing LLM adaptability to dynamic IR contexts [90]. The introduction of Atomic Thought and Atomic Thought Rewards (ATR) in the Atom-Search framework provides fine-grained guidance, enhancing training efficiency and reasoning effectiveness [91]. These training and optimization techniques are integral to enhancing LLM capabilities in complex IR tasks, improving efficiency and accuracy while addressing challenges like hallucination and context limitations. By integrating retrieval-augmented frameworks and leveraging external knowledge, LLMs achieve superior performance in diverse applications, fostering a synergistic relationship between LLMs, IR systems, and human evaluators [12,4,85,13,8]. Through ongoing advancements, LLMs continue to redefine the information retrieval landscape, paving the way for more effective and reliable search experiences. Challenges in Using Large Language Models for Information Retrieval The deployment of large language models (LLMs) in information retrieval (IR) systems faces challenges such as computational limitations, data privacy concerns, accuracy issues, and bias. Addressing these challenges is crucial for effective LLM integration in IR applications. The following subsections delve into computational and resource constraints affecting LLM scalability and effectiveness, emphasizing the need for innovative solutions. Computational and Resource Constraints LLMs in IR systems are constrained by computational and resource limitations, affecting scalability. High memory usage during LLM fine-tuning limits access to advanced models, especially with outdated or unavailable large labeled datasets due to evolving domains [43,4]. Prompting and fine-tuning quality is crucial for LLM performance, requiring robust optimization strategies [40]. Existing benchmarks inadequately assess test-time computation's impact on reranking, leading to suboptimal evaluations [53]. Iterative retrieval methods like FLARE add computational overhead, while limitations in context length and inference latency challenge IR system efficiency [11,44]. Listwise reranking methods struggle with multiple candidates, increasing computational complexity [75]. Innovative approaches like Promptagator offer efficient retrieval model creation with minimal supervision, reducing reliance on extensive training data [9]. However, models like BART have limited generalization across diverse text types due to specific noising functions [28]. To overcome these constraints, developing efficient models and refined resource management strategies is essential. Advanced methodologies and optimized training processes can enhance LLMs' ability to meet dynamic IR demands, improving user understanding, model evaluation, and interactions. The collaboration among IR models, LLMs, and users fosters a robust technical paradigm for information seeking, enabling query variant generation and achieving state-of-the-art performance across retrieval tasks, including zero-shot generalization and multi-task learning. Retrieval-augmented LLM frameworks show significant performance improvements in specialized areas like financial sentiment analysis, highlighting their potential as effective backbone encoders in dense retrieval [29,12,4,14]. Data Privacy and Security Concerns Data privacy and security are critical in deploying LLMs for IR systems due to the sensitive nature of processed data. Real-world data sourced through IR systems may inadvertently expose sensitive information [85]. Web-retrieved data quality and accuracy complicate matters, affecting model performance and reliability. Training LLMs in variable web environments introduces noise and uncertainty, compromising data integrity and privacy [92]. This variability challenges ensuring models do not propagate erroneous or sensitive information, risking data security breaches. Current studies often overlook ethical implications and domain-specific limitations of LLMs in IR, particularly regarding data privacy [12]. Existing benchmarks inadequately assess language models' safety and responsibility, raising issues about user data protection and ethical deployment [5]. Mitigating privacy and security challenges requires robust methodologies prioritizing data protection while ensuring system efficiency. Techniques like Private Retrieval Augmented Generation (PRAG) for secure multi-party computation, enhanced text understanding, and dense retrieval methods using pretrained language models for semantic representation are vital. Domain-specific data augmentation and diverse IR model benchmarking can improve system performance and generalization capabilities, fostering a secure IR ecosystem [18,12,87,31,93]. Implementing stringent data handling protocols and enhancing evaluation frameworks with privacy and security metrics can safeguard user information and uphold ethical standards in IR. Accuracy and Bias in Information Generation LLMs in IR systems face challenges related to accuracy and bias in generated information. LLMs tend to exhibit overconfidence, often providing misleading answers when information is insufficient [94]. Traditional methods inadequately address language complexities like ambiguity and context dependence, hindering natural language processing applications [95]. Synthetic queries generated by methods like HyDE may contain inaccuracies or hallucinations, impacting retrieval quality if not properly filtered [96]. Existing benchmarks often fail to measure the interaction between LLMs' internal knowledge and external retrieval systems, leading to a lack of understanding of LLMs' confidence and knowledge limitations [1]. Experiments indicate models lack complete citation support 50\\ Evaluation and Benchmarking Challenges Evaluating and benchmarking LLM performance in IR systems presents challenges due to the complexity and variability of real-world tasks. The R2MED benchmark testing 15 retrieval systems highlighted difficulties in achieving consistent performance across models, as measured by nDCG@10 [97]. This underscores the need for comprehensive evaluation frameworks capable of accurately assessing retrieval quality in diverse scenarios. Pairwise few-shot rankers showed notable performance improvements over zero-shot and supervised models, indicating few-shot learning's potential to enhance retrieval effectiveness [98]. However, these models struggle to maintain ranking consistency across benchmarks, as evidenced by evaluations using TourRank on TREC DL datasets and the BEIR benchmark [99]. This inconsistency reveals the need for robust evaluation methodologies adapting to dynamic IR tasks. The Task-aware benchmark provides a framework for evaluating retrieval systems in user instruction contexts, enhancing practical applicability [100]. Despite advancements, benchmarks like FlashRAG have not fully addressed datasets' limitations, indicating areas for further investigation [69]. Additionally, the Assistant benchmark may not capture all aspects of real-world task execution, potentially affecting result interpretation and generalization [101]. Assessing methods like Self-RAG, measuring factuality and citation accuracy in long-form generations, emphasizes evaluating LLM outputs' reliability and precision against baseline models [61]. Evaluating ListT5 using average NDCG@10 scores provides insights into retrieved passages' ranking quality, reinforcing the need for consistent metrics to measure retrieval effectiveness [102]. Addressing evaluation and benchmarking challenges requires developing comprehensive frameworks accurately assessing retrieval quality across diverse IR tasks. These frameworks should incorporate the synergistic relationship between IR models, LLMs, and human evaluators to enhance information seeking. Integrating domain-specific knowledge, like legal or financial expertise, can improve retrieval accuracy and interpretability in specialized fields. Tools like RaLLe can facilitate retrieval-augmented LLM development and optimization for knowledge-intensive tasks, ensuring transparent evaluation and performance measurement [12,4,8,74]. By refining evaluation methodologies and incorporating robust metrics, LLMs can better adapt to IR complexities, paving the way for effective and reliable search experiences. Integration and Interaction Limitations Integrating LLMs with existing IR systems and user interactions presents limitations impacting effectiveness and usability. A key challenge is reliance on traditional retrieval architectures, which may not fully accommodate LLMs' advanced capabilities, resulting in suboptimal integration outcomes [32]. These architectures often fail to leverage LLMs' potential due to their static nature and inability to adapt dynamically to evolving user queries and contexts. Moreover, interaction between LLMs and IR systems is constrained by benchmarks inadequately assessing real-world applicability of LLM-generated responses [101]. This gap highlights the need for comprehensive evaluation frameworks accurately measuring interaction quality and relevance in diverse IR scenarios. Traditional IR systems' static nature also presents challenges in adapting to dynamic user interactions, as evidenced by inefficiencies in managing complex conversational dynamics [58]. This limitation underscores the importance of developing adaptive and responsive IR frameworks seamlessly integrating with LLMs for personalized and contextually relevant search experiences. Existing listwise ranking methods often struggle with managing multiple candidates effectively, leading to increased computational complexity and hindering LLM integration into IR systems [75]. Innovative approaches like Promptagator offer efficient methods for creating retrieval models with minimal supervision, reducing the need for extensive training data [9]. To tackle integration and interaction limitations in deploying LLMs for IR, developing adaptive models and refined interaction strategies is essential. Leveraging the synergistic relationship between IR models, LLMs, and humans creates a new technical paradigm enhancing information seeking. Incorporating retrieval-augmented LLMs, mitigating hallucination issues, and improving factual accuracy through external knowledge integration, alongside using slim proxy models to optimize knowledge retrieval while reducing computational costs, can significantly enhance LLMs' end-to-end performance in IR tasks. These strategies improve user understanding and system interactions, addressing challenges related to computational efficiency, credibility, and domain-specific limitations [54,12,85]. By leveraging innovative methodologies and optimizing integration processes, LLMs can better manage the diverse demands of information retrieval, paving the way for practical and scalable applications. Future Directions Optimizing Model Efficiency and Architecture Enhancing the efficiency and architecture of large language models (LLMs) is crucial for improving their performance and scalability in information retrieval (IR) systems. Future research should focus on advancements in HTML understanding and the integration of diverse datasets to increase the robustness of systems like WebAgent [44]. Optimizing SGPT for various language tasks and integrating it with additional models can expand its utility across different IR contexts [40]. Quantization techniques, such as those utilized by QLoRA, present significant opportunities for optimizing LLMs by reducing memory usage without compromising performance. Refining these techniques and developing robust evaluation benchmarks are essential for assessing chatbot performance and other IR applications [43]. Moreover, enhancing reasoning capabilities and applying the Rank1 benchmark to diverse retrieval contexts can improve LLM adaptability and effectiveness [53]. Future exploration should focus on optimizing retrieval efficiency and extending methods like active retrieval across various domains and text generation types to create more efficient, context-aware retrieval processes [11]. By leveraging innovative methodologies and refining architectural strategies, research can significantly enhance LLM capabilities in diverse IR scenarios, resulting in scalable and reliable solutions that meet dynamic information retrieval demands. Enhancing Adaptability and Domain-Specific Applications Improving the adaptability of LLMs for domain-specific applications in IR is essential for advancing retrieval capabilities. Future research should refine query generation processes and explore additional metrics to evaluate the quality of generated queries, thereby enhancing the accuracy and relevance of retrieval outcomes [23]. Enhancing query rewriting accuracy and expanding method applicability to specialized domains will further bolster LLM adaptability [2]. Emerging trends indicate a need for research aimed at improving the adaptability of pre-trained language models (PLMs) for domain-specific applications, leading to more effective retrieval strategies [27]. Enhancements in module interactions and the scalability of frameworks like RoleRAG across diverse tasks can significantly improve adaptability [63]. Future work should focus on enhancing framework capabilities and integrating user feedback, highlighting potential areas for further research [103]. Expanding datasets to encompass more complex user instructions and investigating user feedback integration into retrieval systems can further enhance adaptability [104]. Improving the diversity and quality of synthetic data, along with exploring the method's applicability across various tasks and languages, can enhance adaptability for specialized IR tasks [105]. Additionally, expanding benchmarks to include diverse question types and user behaviors can provide valuable insights into adaptability for domain-specific applications [106]. Refining data augmentation strategies, such as CL-UBS, will enhance adaptability for domain-specific applications [27]. By focusing on these areas, LLMs can better meet the unique challenges and requirements of domain-specific IR applications, paving the way for more effective and tailored retrieval solutions. Improving Interpretability and User Interaction Enhancing the interpretability of LLMs and improving user interactions are critical for advancing IR systems. Future research should refine model interpretability and coherence to facilitate effective user interactions [107]. Improving the transparency of model decisions and outputs can help users understand how LLMs generate responses, fostering trust and usability in IR applications. Advancements in the n-gram generation process can enhance scalability and effectiveness across larger datasets [108]. Investigating these methodologies can lead to more coherent and contextually relevant outputs, improving user interactions. Future directions may include developing frameworks that integrate user feedback into the model refinement process, enabling LLMs to adapt dynamically to user needs and preferences. By leveraging innovative techniques and optimizing interaction strategies, LLMs can significantly enhance personalized and context-aware search experiences. These models excel in text understanding, generation, and knowledge inference, facilitating generative retrieval and improved user-system interactions. Integrating LLMs with search engines fosters a synergistic relationship among IR models, LLMs, and users, forming a powerful paradigm for information seeking. Despite challenges such as computational costs and credibility concerns, LLMs offer promising solutions for creating query variants, detecting missing knowledge, and generating text with citations, paving the way for more intuitive and effective user interactions in complex search tasks. The development of interactive query generation assistants further supports users in refining queries and providing feedback, enhancing retrieval and ranking model effectiveness [12,56,13,54,14]. Addressing Ethical Considerations and Data Privacy Addressing ethical considerations and data privacy concerns associated with LLM deployment in IR systems is crucial for ensuring responsible and secure usage. Current studies highlight significant gaps in addressing the ethical implications and deployment challenges of models like GPT-4, underscoring the necessity for comprehensive frameworks prioritizing ethical standards and data protection [22]. Future research should focus on developing robust methodologies that enhance the ethical deployment of LLMs, ensuring adherence to established ethical guidelines while maintaining user data integrity and confidentiality [12]. This includes exploring advanced data anonymization techniques and implementing privacy-preserving protocols that safeguard user information during retrieval processes. Additionally, enhancing domain-specific applications of LLMs can contribute to more responsible and contextually aware IR systems, minimizing ethical breach risks and data misuse. Exploring new trends in user-system interactions within the IR framework can provide valuable insights into addressing ethical concerns and facilitating the development of more transparent and accountable LLMs. This exploration leverages the synergistic relationship between IR models, LLMs, and humans, forming a powerful technical paradigm for information seeking. By incorporating professional knowledge and user feedback, as demonstrated in approaches like KELLER for legal case retrieval and interactive query generation assistants, these interactions can enhance the accuracy, interpretability, and ethical accountability of LLMs in diverse domains, addressing challenges such as computational costs, credibility concerns, and domain-specific limitations [12,56,8]. By prioritizing ethical considerations and data privacy in future research, LLMs can be deployed in a manner that upholds user trust and ensures responsible AI technology use in information retrieval. Emerging Trends and Evaluation Methodologies Emerging trends in LLMs for information retrieval are driving advancements by integrating diverse datasets and refining evaluation methodologies to enhance retrieval systems. Future research should explore the long-term effects of retrieval augmentation on LLMs' learning processes, examining how these models adapt to diverse datasets and retrieval scenarios [1]. This exploration is crucial for understanding the dynamic interactions between retrieval augmentation and model learning, paving the way for more robust and contextually aware retrieval systems. Enhancing benchmarks to include diverse tasks and safety evaluations is essential for addressing emerging trends in LLM evaluation methodologies [5]. By broadening the scope of evaluation frameworks, researchers can better assess LLM performance across various IR contexts, ensuring comprehensive evaluation in terms of accuracy, relevance, and safety. This expansion is vital for capturing the complexities of real-world IR scenarios and facilitating the development of more reliable and effective LLM applications. The integration of diverse data sources and refinement of evaluation metrics are critical for advancing LLM capabilities in IR. By concentrating on these emerging trends and refining evaluation frameworks, future research can significantly enhance the effectiveness of LLMs in information retrieval. This advancement will facilitate the development of more reliable, contextually aware search experiences by leveraging LLMs' capabilities in text understanding, generation, and knowledge inference. Furthermore, integrating LLMs with retrieval-augmentation frameworks and citation generation benchmarks can address challenges related to credibility, domain-specific limitations, and ethical considerations. This integration will also improve user understanding, model evaluation, and user-system interactions, ultimately fostering a synergistic relationship between IR models, LLMs, and human evaluators. Such efforts are crucial for overcoming existing challenges, including computational costs and the reliability of information services, paving the way for a novel technical paradigm in information seeking [12,13,4,14]. Conclusion The survey highlights the significant impact of large language models (LLMs) in reshaping information retrieval (IR) by enhancing semantic search, query rewriting, and retrieval-augmented generation. Systems like DeepRetrieval have demonstrated exceptional capabilities in literature search and evidence-seeking tasks, surpassing traditional methods. The integration of LLMs into IR frameworks has led to notable improvements in search accuracy and relevance, as seen with models like PG-RAG that advance knowledge retrieval processes. However, achieving human-level performance remains a challenge, as evidenced by benchmarks such as TriviaQA, which continue to guide research in reading comprehension. The potential of AI agents in machine learning engineering tasks has been affirmed through various experiments, indicating promising future applications. Moreover, instruction tuning techniques like CommitPack have shown to enhance LLM performance in coding tasks, suggesting substantial implications for the field. This survey underscores the transformative potential of LLMs in revolutionizing IR, fostering more accurate and contextually aware search experiences. Through ongoing innovation and the adoption of advanced methodologies, LLMs are set to redefine IR systems' capabilities, offering exciting prospects for future research and development.",
  "reference": {
    "1": "2307.11019v3",
    "2": "2311.03758v3",
    "3": "2304.03277v1",
    "4": "2310.04027v2",
    "5": "2403.08295v4",
    "6": "2201.08239v3",
    "7": "1801.01957v2",
    "8": "2406.19760v1",
    "9": "2212.10403v2",
    "10": "2408.06292v3",
    "11": "2305.06983v2",
    "12": "2307.09751v2",
    "13": "2305.14627v2",
    "14": "2501.17981v1",
    "15": "2107.13586v1",
    "16": "2303.04226v1",
    "17": "2507.02592v1",
    "18": "2211.14876v1",
    "19": "2107.08329v1",
    "20": "2505.22648v3",
    "21": "1711.08611v1",
    "22": "2305.03195v1",
    "23": "2407.20183v2",
    "24": "1908.07600v1",
    "25": "2501.04227v2",
    "26": "2503.00223v3",
    "27": "2402.06360v1",
    "28": "2505.20046v1",
    "29": "2408.12194v2",
    "30": "1705.01509v1",
    "31": "2104.08663v4",
    "32": "2206.02743v3",
    "33": "2004.04906v3",
    "34": "2405.14589v1",
    "35": "2305.03653v1",
    "36": "2311.09175v2",
    "37": "2108.10510v1",
    "38": "2507.15061v1",
    "39": "2305.07477v1",
    "40": "2202.08904v5",
    "41": "2305.11841v1",
    "42": "2308.07124v2",
    "43": "2305.14314v1",
    "44": "2307.12856v4",
    "45": "2303.18223v16",
    "46": "2501.14342v3",
    "47": "2402.13492v3",
    "48": "2301.00303v1",
    "49": "2409.19753v4",
    "50": "2212.10509v2",
    "51": "2208.07652v1",
    "52": "2403.20327v1",
    "53": "2502.18418v2",
    "54": "2402.12052v3",
    "55": "2405.14431v1",
    "56": "2311.11226v1",
    "57": "2502.15009v1",
    "58": "2310.09716v2",
    "59": "2405.16933v1",
    "60": "2304.09542v3",
    "61": "2310.11511v1",
    "62": "2406.14972v1",
    "63": "2404.13556v1",
    "64": "2307.02046v6",
    "65": "2303.06573v2",
    "66": "2210.07316v3",
    "67": "2502.01142v2",
    "68": "2310.09949v4",
    "69": "2405.13576v2",
    "70": "2406.15319v3",
    "71": "2306.11644v2",
    "72": "2211.09110v2",
    "73": "2503.09516v5",
    "74": "2308.10633v2",
    "75": "2406.16332v2",
    "76": "2503.19470v3",
    "77": "2302.05578v2",
    "78": "2303.17564v3",
    "79": "2010.08191v2",
    "80": "2306.07906v1",
    "81": "2305.13729v1",
    "82": "2112.07899v1",
    "83": "2202.01169v2",
    "84": "2104.08691v2",
    "85": "2306.05212v1",
    "86": "2306.09938v1",
    "87": "2311.12955v1",
    "88": "2507.19849v1",
    "89": "1706.03762v7",
    "90": "2505.16834v3",
    "91": "2508.12800v3",
    "92": "2504.03160v4",
    "93": "2202.05144v1",
    "94": "2402.11457v2",
    "95": "2003.08271v4",
    "96": "2212.10496v1",
    "97": "2305.02156v1",
    "98": "2505.14558v1",
    "99": "2409.17745v3",
    "100": "2406.11678v2",
    "101": "2211.09260v2",
    "102": "2407.15711v2",
    "103": "2402.15838v3",
    "104": "2206.10658v4",
    "105": "1809.09600v1",
    "106": "2011.01060v2",
    "107": "2410.07095v6",
    "108": "2501.12948v1",
    "109": "2204.10628v1"
  },
  "chooseref": {
    "1": "2303.04226v1",
    "2": "1711.08611v1",
    "3": "2206.02743v3",
    "4": "2307.12856v4",
    "5": "2310.09497v2",
    "6": "2303.18223v16",
    "7": "2406.14972v1",
    "8": "2311.16720v3",
    "9": "2406.14449v2",
    "10": "2503.02197v2",
    "11": "2305.06983v2",
    "12": "2501.04227v2",
    "13": "2507.19849v1",
    "14": "2311.11226v1",
    "15": "2007.00808v2",
    "16": "2407.15711v2",
    "17": "2208.03299v3",
    "18": "2508.12800v3",
    "19": "1706.03762v7",
    "20": "2411.19443v1",
    "21": "2204.10628v1",
    "22": "1910.13461v1",
    "23": "2104.08663v4",
    "24": "1810.04805v2",
    "25": "2406.00083v2",
    "26": "2303.17564v3",
    "27": "2504.12516v1",
    "28": "2501.17981v1",
    "29": "2311.09175v2",
    "30": "2311.09210v2",
    "31": "2501.14342v3",
    "32": "2310.09949v4",
    "33": "2302.05578v2",
    "34": "2404.13556v1",
    "35": "2310.16146v1",
    "36": "2404.11791v1",
    "37": "2011.01060v2",
    "38": "2308.16753v1",
    "39": "2502.15009v1",
    "40": "2108.10510v1",
    "41": "2503.23427v3",
    "42": "2208.07652v1",
    "43": "2402.01176v2",
    "44": "2402.06360v1",
    "45": "2409.19753v4",
    "46": "2407.12529v2",
    "47": "2311.00587v2",
    "48": "1802.05365v2",
    "49": "2502.01142v2",
    "50": "2504.03160v4",
    "51": "2503.00223v3",
    "52": "2501.12948v1",
    "53": "2402.03300v3",
    "54": "2406.16332v2",
    "55": "2004.04906v3",
    "56": "2211.14876v1",
    "57": "2205.09073v2",
    "58": "2305.18290v3",
    "59": "2305.13729v1",
    "60": "2003.06713v1",
    "61": "2311.12955v1",
    "62": "2206.07682v2",
    "63": "2405.16933v1",
    "64": "2305.14627v2",
    "65": "2310.09716v2",
    "66": "2310.04027v2",
    "67": "2305.15294v2",
    "68": "2402.06334v1",
    "69": "2406.15657v1",
    "70": "2409.17745v3",
    "71": "2405.13576v2",
    "72": "2310.07712v2",
    "73": "2310.03214v2",
    "74": "1801.01957v2",
    "75": "2306.09938v1",
    "76": "2403.20327v1",
    "77": "2403.08295v4",
    "78": "2209.10063v3",
    "79": "2305.07477v1",
    "80": "2305.03195v1",
    "81": "2211.09110v2",
    "82": "1809.09600v1",
    "83": "2305.11841v1",
    "84": "2112.04426v3",
    "85": "2204.07496v4",
    "86": "2401.00368v3",
    "87": "2307.09751v2",
    "88": "2301.01820v4",
    "89": "2202.05144v1",
    "90": "2311.01555v1",
    "91": "2304.03277v1",
    "92": "2212.10509v2",
    "93": "2203.05115v2",
    "94": "2307.11019v3",
    "95": "2304.09542v3",
    "96": "2201.08239v3",
    "97": "2005.14165v4",
    "98": "2203.13224v2",
    "99": "2112.07899v1",
    "100": "2311.03758v3",
    "101": "2305.09612v1",
    "102": "2306.17563v2",
    "103": "2408.12194v2",
    "104": "2303.06573v2",
    "105": "2108.07935v1",
    "106": "2406.19760v1",
    "107": "2406.14848v2",
    "108": "2402.15838v3",
    "109": "2302.13971v1",
    "110": "2406.15319v3",
    "111": "2106.09685v2",
    "112": "2307.03172v3",
    "113": "2405.16420v1",
    "114": "2210.07316v3",
    "115": "2411.04468v1",
    "116": "2411.04368v1",
    "117": "2407.20183v2",
    "118": "2410.07095v6",
    "119": "1705.01509v1",
    "120": "2405.17428v3",
    "121": "2505.23885v2",
    "122": "2308.07124v2",
    "123": "2212.09741v3",
    "124": "1908.07600v1",
    "125": "2107.13586v1",
    "126": "2003.08271v4",
    "127": "2212.10496v1",
    "128": "2101.00190v1",
    "129": "2201.05273v4",
    "130": "2107.08329v1",
    "131": "2209.11755v1",
    "132": "2305.14314v1",
    "133": "2305.03653v1",
    "134": "2303.07678v2",
    "135": "2206.10658v4",
    "136": "2503.05592v2",
    "137": "2505.14558v1",
    "138": "2505.20046v1",
    "139": "2301.12652v4",
    "140": "2306.05212v1",
    "141": "2405.14431v1",
    "142": "2308.10633v2",
    "143": "2505.14432v1",
    "144": "2503.06034v1",
    "145": "2312.02969v1",
    "146": "2502.18418v2",
    "147": "2404.18185v1",
    "148": "2309.15088v1",
    "149": "2408.01875v2",
    "150": "2508.07050v2",
    "151": "2305.07001v1",
    "152": "2307.02046v6",
    "153": "2503.19470v3",
    "154": "2301.00303v1",
    "155": "2002.08909v1",
    "156": "2402.13492v3",
    "157": "2010.08191v2",
    "158": "2202.08904v5",
    "159": "2503.04625v2",
    "160": "2307.16645v1",
    "161": "2503.09516v5",
    "162": "2310.05002v1",
    "163": "2310.11511v1",
    "164": "1612.01627v2",
    "165": "2505.16834v3",
    "166": "2505.15444v1",
    "167": "2412.14574v1",
    "168": "2402.12052v3",
    "169": "2307.08303v5",
    "170": "2310.06770v3",
    "171": "2211.09260v2",
    "172": "2203.11147v1",
    "173": "2201.10005v1",
    "174": "2104.14133v2",
    "175": "2306.11644v2",
    "176": "2508.09539v3",
    "177": "2408.06292v3",
    "178": "2104.08691v2",
    "179": "2505.16410v1",
    "180": "2405.14589v1",
    "181": "2406.11678v2",
    "182": "2306.10933v4",
    "183": "2212.10403v2",
    "184": "2202.06991v3",
    "185": "1705.03551v2",
    "186": "2303.00807v3",
    "187": "2202.01169v2",
    "188": "2505.22648v3",
    "189": "2306.07906v1",
    "190": "2112.09332v3",
    "191": "2507.02592v1",
    "192": "2507.15061v1",
    "193": "2504.21776v2",
    "194": "2508.05748v3",
    "195": "2402.11457v2",
    "196": "2305.02156v1",
    "197": "2505.04588v2"
  }
}