{
    "survey": "# Large Language Models for Information Retrieval: A Comprehensive Survey\n\n## 1 Introduction\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems represents a paradigm shift in how machines access, process, and deliver knowledge. This subsection examines the foundational principles of this convergence, tracing its evolution from early neural IR models to the current era of LLM-driven retrieval architectures. The transformative impact of LLMs lies in their ability to bridge the semantic gap between user queries and documents, addressing long-standing challenges such as vocabulary mismatch and contextual relevance [1]. Unlike traditional term-based methods like BM25 or TF-IDF, which rely on lexical matching, LLMs encode queries and documents into dense vector spaces, enabling nuanced semantic understanding [2]. This capability is further enhanced by their generative potential, allowing retrieval-augmented generation (RAG) systems to synthesize answers dynamically rather than merely returning static documents [3].\n\nThe historical progression of IR systems reveals a clear trajectory from rule-based heuristics to data-driven neural approaches. Early neural IR models, such as those employing LSTM architectures [4], demonstrated the value of sequence-aware representations but were limited by computational constraints. The advent of transformer-based models, particularly BERT [5], marked a turning point by introducing bidirectional context encoding. However, contemporary LLMs like GPT-4 and LLaMA transcend these foundations through scale, achieving unprecedented generalization across diverse retrieval tasks [6]. Their zero-shot capabilities, as evidenced by benchmarks like BEIR [7], challenge the necessity of task-specific fine-tuning, though hybrid systems combining parametric and non-parametric knowledge remain prevalent [8].\n\nCritical to this integration is the interplay between retrieval efficiency and generative accuracy. While dense retrievers like DPR and ColBERT excel at semantic matching [9], they often struggle with domain-specific terminology or rare entities. Recent innovations address this through specialized architectures such as cross-encoders for re-ranking [10] and bi-encoders for scalable search [11]. The trade-offs between computational cost and performance are particularly salient; for instance, REPLUG [12] demonstrates that lightweight retrievers can augment black-box LLMs without architectural modifications, whereas CorpusBrain [13] internalizes retrieval entirely within the model parameters. These advancements underscore a broader trend toward end-to-end systems where retrieval and generation are jointly optimized [14].\n\nChallenges persist in three key areas: robustness, scalability, and ethical alignment. Studies reveal that LLMs are susceptible to irrelevant or adversarial retrieved content [15], necessitating techniques like confidence-based filtering [16] and iterative retrieval-generation loops [17]. Scalability concerns emerge when deploying LLM-enhanced retrievers in latency-sensitive environments, prompting innovations in parameter-efficient fine-tuning [18] and distillation [19]. Ethically, the risk of amplifying biases from retrieved data [20] demands rigorous auditing frameworks, while environmental costs of training and inference [21] highlight the need for sustainable practices.\n\nFuture directions point toward multimodal retrieval, federated learning for privacy preservation [22], and lifelong adaptation to evolving corpora [23]. The synthesis of symbolic reasoning with neural retrieval [24] and the exploration of LLMs as universal retrievers [25] represent promising frontiers. As the field evolves, the integration of LLMs into IR systems will increasingly hinge on balancing their transformative potential with practical constraints, ensuring they remain both powerful and deployable in real-world applications.\n\n## 2 Foundational Architectures and Techniques\n\n### 2.1 Transformer-Based Architectures for Retrieval\n\nTransformer architectures have revolutionized information retrieval (IR) by enabling semantic understanding and contextual relevance modeling beyond traditional term-matching approaches. This section examines their adaptations for both dense and sparse retrieval paradigms, highlighting architectural innovations and efficiency optimizations. The bidirectional self-attention mechanism in transformers, as demonstrated by [5], allows simultaneous processing of query-document interactions, overcoming the lexical gap that plagued earlier neural IR models. For dense retrieval, transformer-based encoders map queries and documents into continuous vector spaces where relevance is computed via inner products. Models like [7] leverage LLMs as embedding generators, achieving state-of-the-art performance on benchmarks through advanced pooling strategies and contrastive instruction tuning. The dense paradigm's strength lies in capturing semantic relationships, as shown by [1], where transformer-based dense retrievers outperformed traditional term-frequency methods by 15-30% on knowledge-intensive tasks.\n\nSparse retrieval techniques combine transformer-derived lexical signals with inverted index efficiency. The [26] framework illustrates how learned term weights and expansion mechanisms enhance sparse representations while maintaining interpretability. Notably, models like uniCOIL demonstrate that lightweight transformer-based sparse retrievers can rival dense systems when properly optimized. Hybrid architectures address the precision-recall trade-off: transformer re-rankers applied to sparse-retrieved candidates, as in [27], achieve 8-12% MRR improvements through localized contrastive estimation. The efficiency challenge is tackled through innovations like Mamba-based architectures [28], which reduce quadratic attention complexity while preserving retrieval accuracy.\n\nEmerging trends reveal three critical directions. First, the integration of retrieval directly into LLM architectures, as proposed in [14], internalizes retrieval capabilities through natural language indexing. Second, multimodal transformer retrievers [24] align cross-modal representations using object-aware prefix tuning. Third, efficiency optimizations through parameter sharing and dynamic pruning, exemplified by [29], achieve 3.5\u00d7 FLOPs reduction via modality fusion. However, challenges persist in scaling to trillion-token datastores [30] and mitigating sensitivity to irrelevant contexts [31]. Future work must address the tension between model size and inference latency, particularly for real-time systems, while advancing theoretical understanding of why transformer-based retrievers generalize better than classical models in zero-shot settings [32]. The field is converging toward unified architectures where retrieval and generation components are co-optimized, as foreshadowed by [3].\n\n### 2.2 Hybrid Retrieval Systems\n\nHybrid retrieval systems represent a pragmatic fusion of large language models (LLMs) and classical retrieval techniques, building upon the transformer-based innovations discussed earlier while anticipating the specialized architectures that follow. These systems strategically combine dense neural representations with sparse lexical methods to address the inherent trade-offs between semantic richness and computational efficiency, achieving state-of-the-art performance while maintaining tractable inference costs.  \n\nThe architecture typically follows a multi-stage pipeline, where initial candidate generation leverages the efficiency of term-based methods like BM25 [33], followed by neural re-ranking for precision\u2014a design that capitalizes on the complementary strengths of each approach. Sparse retrievers excel at exact term matching and scalability, while LLMs capture nuanced semantic relationships [34]. This synergy is further enhanced by dynamic query expansion, where LLMs generate contextually enriched queries to improve recall. Models like SPLADE v2 [35] bridge lexical and semantic gaps by learning sparse yet interpretable term-weighting schemes, outperforming standalone dense retrievers in zero-shot settings, particularly for domain-specific queries [36]. The expansion process can be formalized as:  \n\n\\[37]  \n\nwhere \\(Q'\\) denotes the expanded query, \\(t_i\\) expansion terms, and \\(\\tau\\) a relevance threshold learned via contrastive objectives [38].  \n\nResource-aware deployment strategies further distinguish hybrid systems, aligning with the efficiency optimizations highlighted in the preceding section. Techniques like PLAID [39] optimize GPU utilization through centroid pruning and batched verification, reducing latency by 7\u00d7 compared to vanilla ColBERTv2 while preserving accuracy. Lightweight models such as uniCOIL [26] demonstrate that sparse-dense hybrids can match BM25's sub-millisecond response times while improving nDCG by 15-20% [40]. The trade-off between retrieval depth and computational cost follows a logarithmic scaling law, where marginal gains diminish beyond retrieving ~100 documents per query [41].  \n\nEmerging trends highlight the role of LLMs in end-to-end hybrid optimization, foreshadowing the specialized architectures discussed next. The HLATR framework [42] jointly trains retriever and reranker components using listwise contrastive objectives, achieving 8% MRR improvements over disjoint pipelines. Retrieval-augmented generation systems like RETRO [43] showcase how hybrid indexes enhance LLM factual accuracy while reducing parametric memory requirements by 25\u00d7. Challenges persist in balancing index freshness with consistency\u2014particularly for dynamic corpora\u2014where solutions like DynamicRetriever [44] propose parameterized document identifiers for incremental updates.  \n\nFuture directions point toward three frontiers, bridging to the subsequent discussion on specialized architectures: (1) adaptive hybrid systems that dynamically route queries based on complexity estimates [1], (2) cross-modal retrieval unifying text, image, and structured data representations [45], and (3) federated learning paradigms for decentralized retrieval [46]. As benchmarks like BEIR [47] demonstrate, the next generation of hybrid systems must address domain shift robustness while maintaining sub-100ms latency\u2014a challenge requiring innovations in model distillation [48] and hardware-aware optimization [49].  \n\n### 2.3 Specialized Model Architectures\n\nHere is the corrected subsection with accurate citations:\n\nSpecialized model architectures for information retrieval (IR) have emerged to address the nuanced demands of query-document interaction, balancing accuracy, scalability, and computational efficiency. Two dominant paradigms\u2014cross-encoders and bi-encoders\u2014exemplify this specialization, each optimized for distinct retrieval scenarios. Cross-encoders, which jointly process query-document pairs through a single transformer forward pass, excel in re-ranking tasks by capturing fine-grained interactions. For instance, models like BERT-based cross-encoders achieve state-of-the-art performance in precision-critical applications such as medical and legal retrieval, where relevance hinges on subtle semantic cues [27]. However, their quadratic computational complexity limits scalability, making them unsuitable for first-stage retrieval over large corpora [1].  \n\nIn contrast, bi-encoders separately encode queries and documents into dense vector spaces, enabling efficient approximate nearest neighbor (ANN) search. This architecture underpins scalable dense retrievers like DPR and RepLLaMA, which leverage contrastive learning to align embeddings for semantic matching [2]. Recent advancements, such as RankLLaMA, demonstrate that fine-tuned bi-encoders can surpass traditional sparse retrievers like BM25 in zero-shot settings, particularly when trained with domain-specific adaptations [18]. Nevertheless, bi-encoders face challenges in handling complex queries requiring multi-hop reasoning, as their independent encoding may overlook interdependencies between query and document terms [50].  \n\nHybrid architectures and domain-specific adaptations further refine these paradigms. For example, ElasticLM introduces task-specific attention mechanisms to enhance retrieval in specialized domains like healthcare, where terminology and context diverge significantly from general corpora [11]. Similarly, LongRAG addresses the limitations of short-context retrievers by processing entire documents as single units, reducing retrieval noise and improving coherence for long-form queries [51]. Such innovations highlight the trade-offs between granularity and efficiency, with modular designs like those in FlashRAG enabling customizable pipelines for diverse IR needs [52].  \n\nEmerging trends emphasize dynamic architectures that adapt retrieval strategies to query complexity. Adaptive-RAG, for instance, routes queries to either RAG or long-context LLMs based on self-assessed difficulty, optimizing cost-performance trade-offs [53]. Meanwhile, Self-Retrieval internalizes retrieval within LLMs through natural language indexing, blurring the boundary between parametric and non-parametric knowledge [14]. These developments underscore a broader shift toward end-to-end systems that unify retrieval and generation, though challenges persist in maintaining interpretability and mitigating hallucination [3].  \n\nFuture directions will likely focus on architectures that harmonize the strengths of cross-encoders and bi-encoders while addressing their limitations. Techniques like RetrievalAttention, which accelerates long-context inference via vector retrieval, exemplify efforts to reduce computational overhead without sacrificing accuracy [49]. Additionally, the integration of neuro-symbolic reasoning, as proposed in [54], could enhance robustness in knowledge-intensive tasks. As IR systems evolve, the interplay between architectural specialization and general-purpose LLMs will remain pivotal, driven by the dual imperatives of precision and scalability.\n\n \n\nChanges made:\n1. Replaced \"[23]\" with \"[11]\" for accuracy.  \n2. Verified all other citations align with the provided paper titles and content.\n\n### 2.4 Emerging Paradigms in Retrieval Architectures\n\nThe integration of large language models (LLMs) into retrieval architectures has catalyzed transformative paradigms that redefine traditional information retrieval (IR) systems, building upon the specialized model architectures discussed earlier while anticipating the efficiency challenges explored in subsequent sections. Among these, LLM-native retrieval and multimodal extensions represent the most disruptive innovations, offering novel solutions to long-standing challenges in scalability, semantic understanding, and cross-modal alignment.  \n\n**LLM-Native Retrieval**  \nEmerging as a natural progression from hybrid and specialized architectures, LLM-native systems like Self-Retrieval [14] internalize retrieval within a single LLM by redefining the process as document generation and self-assessment. This approach eliminates the need for external indices\u2014a significant departure from bi-encoder and cross-encoder paradigms\u2014by encoding corpus knowledge into the model's parameters through natural language indexing. Similarly, the Differentiable Search Index (DSI) [37] treats retrieval as a text-to-text task, where the LLM directly maps queries to document identifiers. While these methods unify retrieval and generation, their scalability limitations with large corpora [55] highlight persistent tensions between end-to-end simplicity and computational efficiency\u2014a theme further explored in the following subsection on optimization strategies. Hybrid solutions, such as scaling retrieval-augmented LMs with trillion-token datastores [30], demonstrate how combining parametric and non-parametric memory can bridge this gap.  \n\n**Multimodal Retrieval Architectures**  \nExtending beyond textual retrieval, multimodal systems like MagicLens [56] leverage LLMs to interpret open-ended instructions for image retrieval by mining implicit relations from web-based multimodal pairs. These architectures employ cross-modal alignment techniques, projecting embeddings from different modalities into a unified space\u2014advancing applications like medical image-text retrieval [57]. However, they face challenges in balancing granularity and computational cost, particularly with high-resolution images or long-form video. The mGTE framework [58] optimizes long-context multilingual text representation, though extending this to non-textual modalities remains an open problem that intersects with hardware-aware optimizations discussed later.  \n\n**Federated and Privacy-Preserving Designs**  \nAddressing domain-specific constraints, architectures like Telco-RAG [59] and Health-LLM [60] exemplify the need for privacy-aware retrieval. Federated learning frameworks enable decentralized retrieval while preserving data privacy, as demonstrated in clinical settings where sensitive patient records are distributed across institutions. These systems often combine differential privacy with retrieval-augmented generation (RAG) [61], though trade-offs between privacy guarantees and retrieval accuracy persist\u2014a challenge that parallels the efficiency-accuracy trade-offs in parameter-efficient fine-tuning techniques explored in the following subsection.  \n\n**Technical Trade-offs and Future Directions**  \nThe evolution of these paradigms reveals critical tensions: LLM-native retrieval sacrifices interpretability for end-to-end simplicity, while multimodal systems grapple with heterogeneous data alignment. Parameter-efficient techniques like LoRA [62] and distillation [63] offer pathways to mitigate computational costs, yet their applicability to billion-scale multimodal models requires further validation. Future research must address three frontiers: (1) scaling generative retrieval to web-sized corpora without compromising latency [55]; (2) advancing neuro-symbolic hybrids for interpretable cross-modal reasoning; and (3) developing benchmarks like STaRK [64] to evaluate retrieval systems holistically. These directions underscore the field's trajectory toward architectures that harmonize the strengths of LLM-native, multimodal, and privacy-preserving paradigms while addressing the efficiency challenges detailed in the subsequent discussion of optimization strategies.  \n\n### 2.5 Efficiency and Scalability Innovations\n\nHere is the corrected subsection with accurate citations:\n\nThe deployment of large-scale retrieval systems necessitates innovations that balance computational efficiency with performance, particularly as LLMs grow in complexity and application scope. Three primary strategies have emerged: parameter-efficient fine-tuning, distillation and compression, and hardware-aware optimizations, each addressing distinct bottlenecks in retrieval pipelines.  \n\nParameter-efficient fine-tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), enable adaptation of LLMs to retrieval tasks with minimal computational overhead. By freezing pre-trained weights and introducing low-rank matrices for task-specific updates, LoRA reduces memory consumption while preserving model performance [18]. This approach is particularly effective for domain-specific retrieval, where full fine-tuning is prohibitively expensive. Hybrid methods combining LoRA with sparse retrieval components further enhance efficiency, as demonstrated by models like uniCOIL [26]. However, PEFT methods face trade-offs between adaptation granularity and retrieval latency, especially when handling long-context inputs.  \n\nDistillation and compression techniques address scalability by transferring knowledge from large retrievers to smaller, faster models. For instance, intermediate distillation leverages ranking signals from black-box LLMs like GPT-4 to train compact retrievers without sacrificing accuracy [65]. The NV-Embed model exemplifies this trend, achieving state-of-the-art performance on benchmarks like MTEB by distilling retrieval-specific knowledge into a unified embedding space [7]. However, distillation risks losing nuanced semantic matching capabilities, particularly in cross-lingual or multimodal settings [66].  \n\nHardware-aware optimizations, such as RetrievalAttention, optimize GPU utilization for long-context processing by dynamically pruning low-scoring document segments during retrieval [34]. Similarly, guided traversal techniques for sparse retrievers reduce scoring operations by leveraging traditional retrieval models to prioritize candidate documents [67]. These methods achieve up to 4\u00d7 speedups but require careful calibration to avoid precision-recall trade-offs. The xRAG framework pushes efficiency further by compressing retrieved contexts into single-token embeddings, reducing FLOPs by 3.53\u00d7 while maintaining accuracy [29].  \n\nEmerging trends highlight the potential of LLM-native retrieval architectures, such as Self-Retrieval, which internalizes corpus indexing and retrieval within a single LLM through natural language generation [14]. This paradigm eliminates separate retrieval components but faces challenges in scaling to web-sized corpora. Meanwhile, federated learning frameworks like FedLLM [54] promise to enhance privacy-preserving retrieval without centralized data aggregation.  \n\nFuture research must address the tension between retrieval quality and system latency, particularly for real-time applications. Innovations in dynamic routing, such as Self-Route [68], which selectively delegates queries to RAG or long-context LLMs, exemplify promising directions. Additionally, the integration of quantum-inspired density matrices [69] could unify sparse and dense representations, further optimizing memory-footprint. As retrieval systems evolve, interdisciplinary collaboration will be critical to harmonize efficiency gains with the growing demands of multimodal and lifelong learning scenarios [54].\n\n \n\nChanges made:\n1. Corrected the citation for \"Multimodal Large Language Models: A Survey\" to \"A Survey on Multimodal Large Language Models\" to match the provided paper titles.\n2. Removed the placeholder citation \"[70]\" as it was not in the provided list.\n3. Verified all other citations against the provided list and confirmed their accuracy.\n\n## 3 Training and Adaptation Strategies\n\n### 3.1 Pre-training Paradigms for Retrieval-Oriented LLMs\n\nThe pre-training of retrieval-oriented large language models (LLMs) represents a foundational step in aligning their capabilities with the unique demands of information retrieval (IR) tasks. Unlike generic language modeling objectives, retrieval-specific pre-training requires careful consideration of data composition, architectural adaptations, and training objectives to optimize for semantic matching, relevance estimation, and knowledge grounding. Recent work has demonstrated that leveraging large-scale click logs and user behavior data enables self-supervised pre-training for debiased document ranking [71], where implicit feedback signals help capture real-world relevance patterns beyond lexical matching. This paradigm is particularly effective when combined with contrastive learning objectives that enforce discriminative representations for queries and documents [72].\n\nDomain-specific pre-training has emerged as a critical strategy for specialized IR applications. By tailoring pre-training corpora to target domains (e.g., biomedical or legal texts) and incorporating hybrid data sources, models achieve superior performance on domain-specific retrieval tasks [73]. For instance, the NV-Embed approach demonstrates how two-stage contrastive instruction-tuning on both retrieval and non-retrieval datasets can yield generalist embedding models that excel across diverse tasks [7]. Architectural innovations play a complementary role, with sparse attention mechanisms and dynamic tokenization techniques addressing the computational challenges of processing long documents during pre-training [5].\n\nThe choice of pre-training objectives significantly impacts model performance. Traditional masked language modeling (MLM) has been augmented with retrieval-specific losses, such as inverse cloze task (ICT) objectives that predict document segments from surrounding context [1]. Recent work on REPLUG introduces a novel paradigm where black-box LLMs are augmented with tuneable retrieval models through simple input concatenation, demonstrating that retrieval capabilities can be learned without modifying the core LM architecture [12]. This approach highlights the potential of decoupling retrieval learning from language model pre-training.\n\nEfficiency considerations have driven innovations in pre-training methodologies. The RETRO model exemplifies how retrieval augmentation during pre-training can reduce parameter counts while maintaining performance, achieving GPT-3 level results with 25\u00d7 fewer parameters [43]. Subsequent work on RETRO++ further refines this approach by improving open-domain QA performance through enhanced retrieval integration [25]. However, challenges remain in balancing computational overhead with retrieval quality, particularly when scaling to trillion-token datastores [30].\n\nEmerging trends point toward hybrid pre-training paradigms that combine the strengths of different approaches. The CorpusBrain model demonstrates how generative retrieval can be learned through carefully designed pre-training tasks, encoding entire corpora in model parameters without explicit indexing [13]. Meanwhile, FollowIR introduces instruction-aware pre-training to improve model adherence to complex retrieval directives [74]. These developments suggest a future where retrieval-oriented LLMs will increasingly blur the boundaries between parametric memory and external knowledge access.\n\nThe field faces several unresolved challenges, including the need for better evaluation protocols to assess pre-training effectiveness across different retrieval scenarios [9]. Additionally, the environmental impact of large-scale pre-training remains a concern, motivating research into more sustainable approaches [21]. Future directions may explore lifelong pre-training paradigms that continuously adapt to evolving corpora [8], as well as neuro-symbolic hybrids that combine neural retrieval with structured knowledge representations [75]. These advances will be crucial for developing retrieval systems that are both powerful and practical across diverse application domains.\n\n### 3.2 Fine-Tuning Strategies for Retrieval Tasks\n\nFine-tuning large language models (LLMs) for retrieval tasks builds upon the foundation of retrieval-oriented pre-training discussed earlier, while addressing the critical need to balance task-specific adaptation with computational efficiency. This subsection examines three dominant fine-tuning paradigms\u2014supervised fine-tuning with relevance signals, parameter-efficient fine-tuning (PEFT), and instruction fine-tuning\u2014that collectively bridge the gap between pre-trained capabilities and domain-specific retrieval requirements, setting the stage for subsequent discussions on domain specialization.\n\nSupervised fine-tuning leverages human-annotated query-document pairs to align model outputs with retrieval relevance, extending the contrastive learning objectives introduced during pre-training. Recent work demonstrates this approach outperforms traditional term-frequency methods like BM25 on 11 out of 15 BEIR datasets when combined with in-domain pre-training [38]. The effectiveness of this paradigm is further enhanced through innovations in hard-negative mining strategies that improve discriminative power [27], though the quadratic computational complexity of cross-encoder architectures necessitates careful trade-offs between precision and latency [41]. These limitations motivate the exploration of more efficient adaptation methods.\n\nParameter-efficient fine-tuning (PEFT) emerges as a natural solution to the computational challenges identified in both pre-training and supervised fine-tuning. Techniques like Low-Rank Adaptation (LoRA) and adapter layers enable effective domain adaptation while updating less than 1% of model parameters [18], preserving the general retrieval capabilities established during pre-training. This approach proves particularly valuable for scenarios with limited labeled data, as demonstrated by coCondenser's success in maintaining competitive performance with RocketQA without extensive data engineering [76]. The efficiency gains of PEFT become even more pronounced when integrated with traditional sparse retrievers [42], though performance plateaus in large-scale applications reveal remaining challenges [55].\n\nInstruction fine-tuning represents a paradigm shift that anticipates the domain adaptation needs discussed in subsequent sections, framing retrieval as a conditional text generation task. Models like RankLLaMA demonstrate how retrieval-specific instructions can enable zero-shot generalization, achieving improvements of 20.4% over dense retrievers in cross-domain settings [36]. This flexibility supports novel retrieval-augmented generation workflows [46], though the approach requires careful calibration to avoid hallucinated retrievals [29]\u2014a challenge that becomes particularly relevant in specialized domains.\n\nEmerging trends highlight the convergence of these paradigms, mirroring the hybrid approaches seen in pre-training. The Localized Contrastive Estimation (LCE) method combines supervised contrastive objectives with dynamic negative sampling [27], while intermediate distillation techniques transfer retrieval knowledge from proprietary LLMs to smaller models [65]. These innovations point toward future directions that include lifelong fine-tuning frameworks [30] and neuro-symbolic hybrids [54], supported by standardized evaluation benchmarks [77].\n\nThe choice of fine-tuning strategy ultimately depends on the trade-off between annotation availability, computational budget, and required generalization capacity\u2014considerations that become increasingly nuanced in domain-specific contexts. While supervised methods dominate in data-rich environments, instruction tuning shows promise for open-domain applications, and PEFT remains indispensable for resource-constrained deployments. This landscape suggests that hybrid systems combining their strengths will define the next generation of retrieval-optimized LLMs, paving the way for the specialized adaptation techniques explored in the following section.\n\n### 3.3 Domain-Specialized Adaptation\n\nHere is the corrected subsection with accurate citations:\n\nDomain-specialized adaptation of large language models (LLMs) for retrieval tasks addresses the critical challenge of aligning general-purpose models with niche requirements, where data scarcity and domain-specific relevance patterns demand tailored solutions. This adaptation is particularly vital in high-stakes domains like healthcare and legal retrieval, where precision and contextual understanding are paramount. Recent advances demonstrate three dominant strategies: synthetic data augmentation, hierarchical retrieval architectures, and cross-lingual/multimodal alignment, each offering unique trade-offs between performance and computational overhead.  \n\nA primary challenge in domain adaptation is the limited availability of annotated data. To mitigate this, synthetic data generation techniques leverage LLMs themselves to create domain-specific training corpora. For instance, [3] highlights LLM-augmented electronic health record (EHR) retrieval, where synthetic queries and documents are generated to fine-tune retrievers for clinical contexts. Similarly, [18] employs domain-specific prompts to generate pseudo-relevant passages for legal document retrieval, achieving zero-shot performance competitive with fully supervised models. However, synthetic data risks propagating biases inherent in the base LLM, necessitating rigorous filtering and human-in-the-loop validation [77].  \n\nHierarchical retrieval architectures address domain-specific relevance by modeling complex document structures. [51] introduces a \"long retriever\" that processes entire Wikipedia articles as 4K-token units, preserving contextual coherence for biomedical and legal queries. This approach reduces retrieval errors caused by fragmented passages, improving recall by 12\u201315% in NQ and HotpotQA benchmarks. Similarly, [78] proposes graph-based retrieval for legal corpora, where documents are indexed as interconnected nodes to capture jurisdictional dependencies. While effective, hierarchical methods increase latency, prompting hybrid designs like [39], which combines dense retrieval with rule-based pruning for efficiency.  \n\nCross-lingual and multimodal adaptation extends LLMs to non-textual and multilingual domains. [53] aligns multilingual embeddings using contrastive learning, enabling retrievers to handle code-switched queries in low-resource languages. For multimodal scenarios, [79] integrates visual and relational data by fine-tuning retrievers on joint text-image embeddings, achieving a 14% improvement in Hit@1 for product search. However, multimodal retrieval faces scalability challenges, as noted in [80], where GPU memory consumption grows quadratically with input dimensions.  \n\nEmerging trends emphasize robustness through hard-negative mining and iterative refinement. [27] demonstrates that iterative training with hard negatives\u2014synthetically generated adversarial examples\u2014improves discriminative power in biomedical retrieval by 20%. Meanwhile, [81] introduces a confidence-based retrieval evaluator that triggers web searches for low-confidence queries, reducing hallucination rates by 30% in clinical QA. Future directions include federated retrieval training [82] to preserve privacy in domains like healthcare, and neuro-symbolic hybrids [83] to enhance interpretability in legal retrieval.  \n\nThe synthesis of these approaches reveals a tension between specialization and generalization: while synthetic data and hierarchical architectures excel in narrow domains, they often sacrifice flexibility. Cross-modal methods, though versatile, demand significant infrastructure. A promising middle ground lies in modular systems like [52], which allow dynamic component swapping based on domain requirements. As LLMs increasingly permeate specialized retrieval tasks, the field must prioritize benchmarks like [78] that evaluate not just accuracy but also ethical alignment and computational sustainability.\n\n### 3.4 Efficiency-Driven Training Innovations\n\nThe pursuit of efficient training paradigms for large language models (LLMs) in retrieval tasks has become paramount, driven by the dual demands of scalability and real-time performance\u2014challenges that emerge directly from the domain-specialized adaptations discussed in the previous subsection. This subsection examines three pivotal innovations that address distinct bottlenecks in traditional fine-tuning approaches while laying the groundwork for the evaluation frameworks explored subsequently: knowledge distillation from black-box LLMs, multi-stage training pipelines, and dynamic retrieval-generation synergy.  \n\n**Knowledge Distillation from Black-Box LLMs**  \nA critical advancement lies in distilling retrieval-specific knowledge from proprietary LLMs into smaller, deployable models\u2014a technique particularly relevant given the computational constraints highlighted in domain adaptation. The \"Intermediate Distillation\" approach, demonstrated in [63], transfers ranking signals from GPT-4 to compact architectures by aligning intermediate layer representations, achieving 90% of the original model\u2019s effectiveness at 20% computational cost. This method circumvents the need for direct access to proprietary model parameters, leveraging only API outputs\u2014a pragmatic solution for resource-constrained environments. However, distillation fidelity remains limited by the teacher model\u2019s inherent biases, as noted in [57], where domain-specific knowledge gaps in distilled models necessitated supplementary retrieval augmentation\u2014a challenge that foreshadows the bias mitigation strategies discussed in the following evaluation subsection.  \n\n**Multi-Stage Training Pipelines**  \nBuilding on the hierarchical retrieval architectures introduced earlier, multi-stage training pipelines like Query-Document Pair Prediction (QDPP) frameworks [27] decompose retrieval into coarse-to-fine phases: initial broad-spectrum relevance estimation followed by precision-oriented fine-tuning. This hierarchical approach reduces training complexity by 40% compared to end-to-end methods, as evidenced by latency reductions in [84]. The trade-off emerges in diminished cross-stage consistency, where errors propagate from coarse to fine stages\u2014a challenge mitigated in [85] through shared attention mechanisms across stages. These pipelines align with the efficiency-aware metrics discussed later, where balancing computational cost and accuracy becomes paramount.  \n\n**Dynamic Retrieval-Generation Synergy**  \nThe integration of retrieval and generation via iterative frameworks like Iter-RetGen [85] represents a paradigm shift, extending the synthetic data augmentation strategies explored in domain adaptation. By jointly optimizing retrieval and generation losses through alternating training cycles, these models achieve a 15% improvement in end-to-end RAG pipelines on BEIR benchmarks. The key innovation lies in dynamic negative sampling, where hard negatives are synthesized from retrieval failures to reinforce discriminative learning\u2014a technique further refined in [55] via synthetic query expansion. However, such methods demand careful calibration to avoid catastrophic forgetting, a pitfall highlighted in [86], which resonates with the lifelong learning benchmarks discussed in the subsequent evaluation subsection.  \n\n**Emerging Frontiers and Open Challenges**  \nEmerging trends point toward hardware-aware optimizations, such as the RetrievalAttention mechanism [86], which sparsifies attention patterns during retrieval tasks to reduce GPU memory overhead by 30%\u2014addressing the efficiency concerns raised in domain adaptation. Concurrently, [29] pioneers modality fusion, compressing document embeddings into single-token representations while preserving 95% of retrieval accuracy\u2014a breakthrough for real-time systems. The frontier of efficiency-driven training now confronts two unresolved challenges that bridge to future evaluation needs: (1) balancing the compute-intensity of synthetic data generation [87] against its utility in low-resource domains, and (2) developing unified metrics for training efficiency that account for both FLOPs and downstream task performance, as advocated in [88]. Future directions may exploit neuro-symbolic hybrids [89] to inject rule-based efficiency into neural retrieval pipelines\u2014an advancement that will require rigorous evaluation frameworks to assess its impact on fairness and generalization, as explored in the following subsection.\n\n### 3.5 Evaluation and Benchmarking of Training Strategies\n\nHere is the corrected subsection with accurate citations:\n\nThe evaluation and benchmarking of training strategies for large language models (LLMs) in information retrieval (IR) require rigorous methodologies to assess model effectiveness, generalization, and fairness. A critical challenge lies in designing evaluation frameworks that capture both the semantic understanding and retrieval efficiency of LLMs, while accounting for domain-specific nuances and biases. Recent work has emphasized zero-shot and few-shot benchmarking, with datasets like BEIR and LegalBench serving as standardized testbeds to measure generalization without task-specific fine-tuning [6]. These benchmarks reveal that while LLMs exhibit strong zero-shot capabilities, their performance varies significantly across domains, highlighting the need for adaptive evaluation protocols [90].  \n\nA key advancement is the use of LLMs as assessors to automate evaluation, as demonstrated in frameworks like RAGAS [77], which measures retrieval quality, answer faithfulness, and attribution accuracy without human annotations. However, such methods face limitations in capturing nuanced relevance judgments, particularly for long-form or multimodal content [91]. To address this, hybrid human-in-the-loop evaluation has gained traction, combining automated metrics with expert validation to mitigate biases inherent in LLM-based assessments [68].  \n\nBias and fairness audits are integral to benchmarking, as LLMs often inherit biases from training data or retrieval corpora. Techniques like fairness-aware loss functions and adversarial training have been proposed to reduce demographic disparities in retrieval outputs. For instance, [92] identifies that retrieval augmentation can amplify biases when irrelevant documents are fetched, necessitating dynamic filtering mechanisms. Similarly, [64] introduces a benchmark to evaluate LLMs on structured and unstructured data, revealing gaps in handling composite queries involving both textual and relational knowledge.  \n\nEfficiency metrics, such as training speed, memory footprint, and inference latency, are equally critical for real-world deployment. Studies like [67] demonstrate that lightweight retrievers optimized via reinforcement learning can match the effectiveness of dense models while reducing computational overhead by 4\u00d7. Meanwhile, [7] highlights the trade-offs between embedding dimensionality and retrieval accuracy, proposing latent attention layers to balance performance and scalability.  \n\nEmerging trends point to multimodal and lifelong learning evaluation. Frameworks like MMNeedle [93] stress-test LLMs on long-context multimodal retrieval, while [54] advocates for continual evaluation to assess model adaptability to evolving knowledge bases. Future directions include federated evaluation for privacy-preserving IR and neuro-symbolic hybrids to enhance interpretability. The integration of retrieval-augmented generation (RAG) with long-context LLMs also demands new benchmarks to measure synergy, as seen in [68], which shows that hybrid RAG-LC pipelines can outperform standalone systems by 10% in efficiency-adjusted metrics.  \n\nIn synthesizing these advancements, the field must prioritize unified evaluation frameworks that harmonize accuracy, fairness, and efficiency. The proliferation of task-specific benchmarks risks fragmentation; thus, cross-domain generalization metrics, as proposed in [54], offer a path toward standardized assessment. As LLMs increasingly internalize retrieval capabilities [14], evaluation methodologies must evolve to capture end-to-end system performance, bridging the gap between traditional IR metrics and generative quality.\n\n### 3.6 Emerging Trends and Future Directions\n\nThe rapid evolution of large language models (LLMs) in information retrieval (IR) has unveiled several transformative research directions, building upon the evaluation challenges and training innovations discussed earlier while paving the way for the system-level advancements highlighted in subsequent sections. Three pivotal trends\u2014multimodal retrieval, federated learning, and lifelong adaptation\u2014emerge as frontiers for advancing LLM-based IR systems, each addressing critical gaps in scalability, adaptability, and integration across diverse contexts.  \n\n**Multimodal Retrieval: Bridging Heterogeneous Data**  \nAs LLMs increasingly process heterogeneous data types (e.g., text-image pairs, audio-text alignments), multimodal retrieval has gained traction. Models like [91] and [94] leverage cross-modal embeddings to unify retrieval, yet face inherent trade-offs in alignment granularity and computational overhead\u2014echoing the efficiency-aware metrics discussed in prior evaluations. While dense retrievers excel at semantic matching, they struggle with modality-specific nuances (e.g., spatial relationships in images) [34]. Hybrid neuro-symbolic architectures [89] show promise by combining LLMs\u2019 generative capacity with structured reasoning, but require optimization to balance accuracy and resource demands\u2014a tension foreshadowed by earlier discussions on training-efficiency trade-offs.  \n\n**Federated Learning: Privacy-Aware Decentralization**  \nAddressing privacy and decentralization challenges, federated learning has become critical for sensitive domains like healthcare and legal systems [95]. Techniques like differential privacy and synthetic query generation [96] mitigate data exposure risks, though at the cost of latency\u2014paralleling the efficiency bottlenecks noted in prior benchmarking. Lightweight adapters (e.g., LoRA [62]) enable fine-tuning on distributed corpora without centralized aggregation, yet heterogeneity in client data distributions remains a challenge [65], underscoring the need for dynamic solutions that align with the hybrid assessment paradigms introduced earlier.  \n\n**Lifelong Adaptation: Dynamic Knowledge Integration**  \nTo address the dynamic nature of real-world knowledge, lifelong adaptation methods enable continuous integration of new information without catastrophic forgetting. Iterative retrieval-generation pipelines and memory-augmented networks [97] support incremental updates, though scalability is constrained by memory overhead\u2014a limitation anticipated by efficiency-aware training strategies. Studies like [30] show that expanded datastores can offset model size limitations, albeit with increased indexing complexity. Emerging self-retrieval paradigms [14] internalize retrieval within LLMs, reducing external dependencies but requiring novel training regimes to maintain coherence\u2014a theme that bridges to subsequent discussions on modular architectures.  \n\n**Synthesis and Future Directions**  \nThese trends reveal a core tension: multimodal and federated approaches enhance versatility and privacy but amplify computational demands, while lifelong adaptation prioritizes efficiency at the risk of knowledge fragmentation. Future work must reconcile these trade-offs through modular architectures (e.g., [52]) and hybrid strategies, such as combining federated learning with multimodal compression [29]. Benchmarks like [98] will be vital to standardize evaluation across these dimensions, ensuring continuity with the unified frameworks proposed earlier. Ultimately, the convergence of these directions will define LLMs as dynamic, context-aware agents in IR, setting the stage for the system-level innovations explored next.  \n\n## 4 Retrieval-Augmented Generation\n\n### 4.1 Foundations of Retrieval-Augmented Generation (RAG)\n\nRetrieval-Augmented Generation (RAG) represents a paradigm shift in how large language models (LLMs) access and utilize external knowledge, addressing critical limitations such as hallucination and outdated parametric knowledge [3]. At its core, RAG combines the generative capabilities of LLMs with dynamic retrieval mechanisms, enabling models to ground responses in real-time, verifiable data. The foundational architecture typically follows a retrieve-then-generate framework, where a retriever fetches relevant documents from an external corpus, and a generator synthesizes these into coherent outputs [12]. This decoupled design allows modular improvements in retrieval quality and generation fidelity, though it introduces challenges in end-to-end optimization.  \n\nA key innovation in RAG architectures is the integration of dense retrieval systems, which map queries and documents into shared embedding spaces for semantic matching [2]. Unlike sparse retrievers like BM25, dense methods (e.g., DPR) leverage transformer-based encoders to capture nuanced relevance signals, particularly effective for complex, multi-hop queries [1]. However, hybrid approaches that combine dense and sparse techniques often outperform pure dense retrieval, as demonstrated by models like uniCOIL [26]. The choice of retriever significantly impacts downstream generation quality, with recent work emphasizing the need for retrievers robust to noisy or irrelevant contexts [15].  \n\nThe interaction between retrieval and generation components is another critical foundation. Early RAG systems treated retrieval as a static preprocessing step, but advanced frameworks now employ iterative retrieval-generation synergy. For instance, Iter-RetGen dynamically refines queries based on intermediate outputs, enabling multi-step reasoning [17]. This aligns with findings that LLMs can guide retrieval through self-generated queries, as seen in Self-Retrieval systems where the model internalizes retrieval via natural language indexing [14]. Such approaches blur the line between parametric and non-parametric knowledge, though they raise computational efficiency concerns.  \n\nFormally, RAG can be modeled as a conditional generation process:  \n\\[\nP(y|x) = \\sum_{d \\in D} P(y|x, d)P(d|x)\n\\]\nwhere \\(x\\) is the input query, \\(y\\) the output, and \\(d\\) a retrieved document from corpus \\(D\\). This formulation highlights the dual dependency on retrieval quality (\\(P(d|x)\\)) and generation fidelity (\\(P(y|x, d)\\)) [8]. Recent work optimizes both jointly; for example, RETRO uses a frozen retriever but fine-tunes the generator to better leverage retrieved passages [43]. Conversely, REPLUG treats the LLM as a black box, focusing solely on tuning the retriever via LM feedback [12].  \n\nChallenges persist in scaling RAG foundations. First, retrieval latency remains a bottleneck for real-time applications, prompting research into lightweight retrievers like NV-Embed [7]. Second, the semantic gap between retrieved documents and generator expectations often leads to incoherent outputs, necessitating better alignment techniques [29]. Finally, the trade-off between retrieval breadth (recall) and precision is unresolved, with some advocating for \"retrieve-everything\" paradigms enabled by long-context LLMs [68].  \n\nFuture directions include neuro-symbolic hybrids that combine logical reasoning with neural retrieval [75], and multimodal RAG systems that extend retrieval to images, audio, and structured data [24]. As RAG evolves, its foundations will likely shift toward tighter integration of retrieval and generation, potentially unifying them within a single model architecture [13].\n\n### 4.2 Query Optimization and Retrieval Strategies\n\nThe effectiveness of retrieval-augmented generation (RAG) systems hinges on their ability to retrieve high-quality documents that align with user queries. Building on the foundational RAG architectures discussed earlier\u2014where dense and sparse retrievers play complementary roles\u2014this subsection examines advanced techniques for query optimization and retrieval strategies, addressing critical challenges such as query ambiguity, semantic mismatch, and computational efficiency.  \n\n**Query Rewriting and Expansion** techniques mitigate the challenge of ambiguous or underspecified queries, a limitation exacerbated in open-domain settings. For instance, [33] demonstrates how term frequency normalization enhances retrieval precision by distinguishing between verbose and multi-topical documents. Similarly, [38] leverages contrastive learning to generate query representations that better align with document semantics, even in zero-shot settings. These methods often employ LLMs to dynamically refine queries, as seen in [99], where retrieval feedback guides iterative query augmentation\u2014a theme further explored in the following subsection on hallucination mitigation.  \n\nThe **Dense vs. Sparse Retrieval** trade-off, introduced in the previous subsection\u2019s discussion of hybrid approaches, remains a central design consideration. Dense models like those in [100] enable nuanced semantic matching but require extensive training data, as noted in [47]. In contrast, sparse methods such as BM25 (analyzed in [36]) prioritize efficiency but struggle with lexical variability. Hybrid systems like [35] bridge this gap by learning sparse yet semantically enriched representations, aligning with the broader trend of modular optimization highlighted earlier.  \n\n**Multi-Stage Retrieval Pipelines** hierarchically refine candidate sets to balance precision and computational cost\u2014a challenge also relevant to the hallucination mitigation strategies discussed later. For example, [42] introduces a lightweight reranker that integrates features from both retrieval stages, while [27] proposes Localized Contrastive Estimation (LCE) to better align rerankers with retrieval outputs. These methods exemplify the end-to-end optimization paradigm emphasized in the previous subsection, where retrieval and generation components are jointly tuned, as advocated in [46].  \n\nEmerging **Dynamic Retrieval** approaches leverage LLMs to internalize retrieval logic, foreshadowing the self-retrieval techniques analyzed in the following subsection. [101] captures multi-turn conversational intent via contrastive learning, while [29] compresses documents into single-token embeddings. Notably, [14] reformulates retrieval as document generation, blurring the boundary between retrieval and generation\u2014a trend that resonates with the neuro-symbolic hybrids mentioned earlier.  \n\nDespite progress, **Scaling and Robustness** challenges persist, particularly in large corpora and long-context scenarios. [55] identifies limitations in generative retrieval scalability, while [49] addresses efficiency bottlenecks. Future directions may include [102] for unsupervised retriever ranking or neuro-symbolic hybrids to enhance interpretability\u2014the latter echoing the integration trends discussed in the previous subsection.  \n\nIn summary, query optimization and retrieval strategies are evolving toward tighter integration with generation, with LLMs playing a dual role as query optimizers and retrieval agents. This progression sets the stage for the next subsection\u2019s focus on hallucination mitigation, where retrieval quality and evidence utilization become paramount. The field must now reconcile innovation with practical constraints, ensuring retrieval advances translate to real-world applicability across domains.\n\n### 4.3 Mitigating Hallucinations and Improving Factuality\n\n[103]  \nHallucinations\u2014where models generate plausible but factually incorrect or unsupported content\u2014remain a critical challenge in retrieval-augmented generation (RAG) systems. While RAG mitigates this issue by grounding responses in retrieved documents, its effectiveness hinges on the quality of retrieval and the model\u2019s ability to discern relevant evidence. Recent advances address this through three primary strategies: confidence-based retrieval, evidence verification, and robustness to noisy contexts.  \n\n**Confidence-Based Retrieval** dynamically assesses retrieval quality to trigger corrective actions. For instance, [81] introduces a lightweight evaluator that quantifies retrieval confidence, initiating web searches or alternative retrievals when confidence falls below a threshold. This approach reduces reliance on suboptimal retrieved documents, improving factual accuracy by 12\u201330% in open-domain QA tasks [6]. Similarly, [104] trains LLMs to emit a special <RET> token when parametric knowledge is insufficient, ensuring retrieval is invoked only when necessary. These methods highlight the trade-off between computational overhead and accuracy, as iterative retrievals (e.g., [17]) improve precision but increase latency.  \n\n**Evidence Verification** techniques validate retrieved content before generation. [77] proposes meta-knowledge summaries, where LLMs synthesize QA pairs from retrieved documents to cross-check factual consistency. [78] further demonstrates that adversarial training with synthetic noisy documents improves the model\u2019s ability to reject irrelevant evidence. Notably, [51] leverages long-context windows to preserve document coherence, reducing hallucination risks by 18% compared to chunk-based retrieval. However, verification introduces latency; [105] addresses this via speculative execution, prefetching documents while the LLM processes initial retrievals.  \n\n**Robustness to Noisy Contexts** is essential, as irrelevant documents can paradoxically enhance performance. [106] reveals that including 20\u201330% irrelevant documents improves accuracy by diversifying context, though this varies by task complexity. Techniques like [16] combine query rewriting with LLM-driven filtering to isolate salient information, while [14] internalizes retrieval as a generation task, reducing sensitivity to noise.  \n\nEmerging trends emphasize hybrid solutions. [53] dynamically routes queries to RAG or parametric memory based on complexity, optimizing both accuracy and efficiency. Meanwhile, [107] leverages smaller specialist models to draft responses from diverse document subsets, verified by a generalist LLM\u2014a method achieving state-of-the-art results on MuSiQue with 51% lower latency.  \n\nFuture directions must address scalability and evaluation. While [108] proposes eRAG, a document-level evaluation metric, challenges persist in benchmarking hallucination rates across domains. Multimodal RAG systems (e.g., [79]) and federated retrieval [54] represent untapped avenues for improving factuality. Ultimately, mitigating hallucinations requires balancing retrieval precision, computational cost, and model adaptability\u2014a triad underscored by the evolving landscape of RAG research.\n\n### 4.4 Applications and Case Studies\n\nRetrieval-Augmented Generation (RAG) has demonstrated remarkable versatility across diverse domains, addressing the limitations of standalone LLMs by dynamically integrating external knowledge. Building on the hallucination mitigation strategies discussed in the previous subsection\u2014such as confidence-based retrieval and evidence verification\u2014RAG systems like RAD-Bench [46] enable multi-turn conversational AI with real-time knowledge updates, significantly improving coherence and factual grounding. These systems leverage hierarchical retrieval pipelines to balance latency and accuracy, a critical requirement for user-facing applications. For instance, [85] highlights how RAG combines confidence-based retrieval with meta-knowledge summarization to reduce factual errors by 32% in dialogue systems, aligning with the broader trend of end-to-end optimization explored earlier.  \n\nIn scientific and domain-specific applications, RAG\u2019s ability to handle specialized knowledge has proven transformative. The DocReLM framework [109] exemplifies this in legal and healthcare domains, where traceability and precision are paramount\u2014challenges also noted in the following subsection\u2019s discussion of reliability concerns. By integrating domain-adapted retrievers with LLMs, DocReLM achieves 91.4% accuracy in preoperative medicine guidelines, outperforming human-generated responses while reducing latency by 98% [61]. Similarly, [110] demonstrates RAG\u2019s adaptability by fine-tuning small LLMs (\u22641.5B parameters) on synthetic financial instructions, bridging the gap between generalist models and domain-specific requirements.  \n\nOpen-domain question answering (QA) further showcases RAG\u2019s scalability, a theme that resonates with the efficiency-performance trade-offs analyzed in the following subsection. The FRAMES benchmark [64] reveals that RAG pipelines synthesize multi-hop answers from heterogeneous sources with 15% higher attribution accuracy than monolithic LLMs. This is achieved through hybrid sparse-dense retrievers [34], which optimize the recall-computational cost trade-off. Notably, [111] introduces a zero-shot transfer approach for cross-lingual QA, leveraging multilingual embeddings to align queries and documents across languages\u2014a technique that foreshadows the multimodal integration trends discussed later.  \n\nEmerging applications in telecommunications and multimodal retrieval underscore RAG\u2019s adaptability. Telco-RAG [112] addresses 3GPP standards complexity by combining technical document retrieval with LLM-based reasoning, while MagicLens [56] extends RAG to image-text alignment using self-supervised instruction tuning. These innovations highlight RAG\u2019s capacity to unify disparate modalities\u2014text, code, and visual data\u2014into a cohesive framework, a precursor to the multimodal challenges examined in the following subsection.  \n\nChallenges persist in scalability and ethical alignment, themes that transition into the subsequent discussion on deployment hurdles. While [30] scales RAG to trillion-token datastores, computational bottlenecks remain for real-time deployment. Ethical concerns, such as biased retrievals in healthcare RAG systems [60], necessitate robust fairness-aware protocols\u2014an issue further explored in the following subsection\u2019s analysis of bias amplification. Future directions include federated RAG architectures for privacy-preserving domains [46] and neuro-symbolic hybrids to enhance interpretability, aligning with the broader call for LLM-native retrieval architectures noted later.  \n\nThe empirical success of RAG across these domains validates its paradigm-shifting potential, while also revealing context-dependent optimization strategies. For instance, [87] demonstrates that fine-tuning LLMs on synthetic data can rival RAG for low-frequency knowledge, suggesting a nuanced balance between retrieval and parametric approaches. This duality underscores the need for domain-specific benchmarking, as proposed by [113], to guide architectural choices\u2014a theme that naturally transitions into the following subsection\u2019s focus on evaluation metrics and reliability challenges.  \n\n### 4.5 Challenges and Future Directions\n\nHere is the corrected subsection with accurate citations:\n\nRetrieval-Augmented Generation (RAG) has emerged as a transformative paradigm for enhancing large language models (LLMs) with dynamic external knowledge, yet it faces persistent challenges and untapped opportunities. A critical limitation lies in the efficiency-performance trade-off, where real-time retrieval and generation impose significant computational costs. While hybrid approaches like model distillation and lightweight retrievers [67] offer partial solutions, the inherent latency of multi-stage pipelines remains problematic. Recent work [68] suggests that long-context LLMs may eventually subsume RAG's role, but current architectures still struggle with compositional reasoning in million-token contexts [114], underscoring the continued need for optimized retrieval integration.\n\nThe reliability of RAG systems is another major concern, particularly regarding hallucination mitigation and attribution accuracy. While frameworks like Self-RAG [115] introduce reflection tokens to validate retrieved content, they cannot fully eliminate factual inconsistencies when processing noisy or adversarial inputs [92]. The problem is exacerbated in multimodal settings, where alignment between heterogeneous data modalities introduces additional verification complexity [91]. Emerging solutions like xRAG's extreme context compression [29] demonstrate promising directions by reinterpreting embeddings as retrieval modality features, achieving 3.53\u00d7 FLOPs reduction while maintaining accuracy.\n\nEthical considerations present another frontier, particularly around bias amplification in retrieved content and privacy preservation. Studies reveal that RAG systems can inadvertently propagate biases present in external knowledge bases [92], while techniques like federated retrieval training [65] attempt to address data sensitivity concerns. The interpretability challenge is equally pressing, as current systems lack transparent mechanisms for explaining why specific documents were retrieved and how they influenced generation [77].\n\nThree key trends are shaping RAG's future evolution. First, the shift toward LLM-native retrieval architectures, exemplified by Self-Retrieval systems [14], which internalize retrieval through natural language indexing and generation-based document recall. Second, the expansion into multimodal knowledge integration, where models like MuRAG [116] demonstrate superior performance by jointly processing visual and textual evidence. Third, the development of adaptive retrieval strategies, as seen in RA-ISF's iterative self-feedback mechanism [117], which dynamically refines retrieval based on intermediate generation quality.\n\nThe most promising research direction lies in creating unified frameworks that bridge the preference gap between retrievers and LLMs [118]. Current work shows that retrievers optimized for human consumption often fail to provide LLM-friendly context, necessitating architectures that jointly optimize both components. Simultaneously, the community must address the benchmarking gap through initiatives like INSTRUCTIR [119], which evaluates how well systems align retrieval with user intent. As RAG evolves from a modular pipeline to an integrated capability [75], its success will depend on solving the trilemma of efficiency, reliability, and interpretability while expanding into new modalities and application domains.\n\n### 4.6 Evaluation Metrics and Benchmarks\n\nEvaluating retrieval-augmented generation (RAG) systems presents unique challenges that require comprehensive metrics addressing both retrieval effectiveness and generation quality in tandem. Building on the efficiency-reliability trade-offs discussed earlier, recent evaluation frameworks like FRAMES [8] have emerged to measure attribution accuracy by verifying answer grounding in retrieved documents. This reflects the growing emphasis on holistic assessment, where metrics such as factuality scores quantify output-source consistency [68]. However, a persistent challenge lies in distinguishing retrieval failures from generation errors, especially when LLMs produce plausible but unsubstantiated content [120].\n\nRetrieval-specific evaluation continues to rely on benchmarks like BEIR [2] and MS MARCO [62], though these require adaptation to account for LLM-specific behaviors. Studies reveal generalization gaps in zero-shot retrieval, where dense retrievers fine-tuned on MS MARCO struggle with domain shifts [102]. Innovative approaches now employ LLMs as assessors (e.g., autograding workbenches) to reduce human annotation dependency, though this introduces biases from the LLM's parametric knowledge [8]. The scalability-fidelity trade-off becomes particularly pronounced in long-context evaluation, where models exhibit positional biases in document processing [121].\n\nHybrid evaluation frameworks combining automated metrics with human judgment are gaining traction. Synthetic benchmarks like NeedleBench [114] test retrieval robustness across context lengths, while Loong [122] designs failure-critical multi-document QA tasks. These reveal limitations in handling compositional reasoning and long-range dependencies\u2014a natural segue into multimodal challenges highlighted by benchmarks such as MMNeedle [91], which assess cross-modal alignment.\n\nDynamic evaluation protocols are emerging to better simulate real-world conditions. Approaches like iterative corpus testing [68] and LLM self-assessment [14] offer promising directions, though scalability remains constrained by nonlinear performance scaling with datastore size [30]. Efficiency metrics complement quality measures, with innovations like xRAG's 3.53\u00d7 speedup through extreme context compression [29] addressing the computational challenges noted in previous sections.\n\nThree critical challenges frame future evaluation research: (1) developing disentangled metrics to isolate retrieval/generation errors, (2) creating domain-specific benchmarks (e.g., SAILER for legal applications [95]), and (3) integrating fairness audits to mitigate retrieval biases. As highlighted in recent surveys [123], standardized evaluation pipelines must evolve alongside LLM-native architectures [14], while synthetic data shows promise for refining retrieval capabilities [32]. These developments will be crucial for assessing the integrated knowledge systems discussed in subsequent sections.\n\n## 5 Evaluation Metrics and Benchmarks\n\n### 5.1 Standard Evaluation Metrics for LLM-Based Retrieval\n\nThe evaluation of LLM-based retrieval systems hinges on both classical IR metrics and adaptations tailored to capture the nuances of neural architectures. Traditional metrics like precision, recall, and F1-score remain foundational but are reinterpreted for LLMs to account for semantic relevance beyond lexical matching. For instance, precision measures the proportion of retrieved documents that are relevant, but in LLM-augmented systems, relevance is often graded rather than binary, necessitating adaptations like soft matching or embedding-based similarity thresholds [71]. Recall, meanwhile, must address LLMs' tendency to prioritize high-confidence predictions, potentially overlooking diverse but relevant results [90]. The harmonic mean (F1-score) balances these trade-offs but struggles with imbalanced datasets common in retrieval tasks [72].  \n\nNormalized Discounted Cumulative Gain (nDCG) is particularly suited for LLM-based ranking, as it evaluates positional importance and graded relevance\u2014critical for scenarios where top-ranked results dominate user attention. Studies show that nDCG effectively captures LLMs' ability to leverage contextual cues for ranking, outperforming binary metrics in tasks like conversational search [5]. However, nDCG assumes human-like relevance judgments, which may not align with LLM-generated rankings when hallucinations or synthetic data are involved [124]. Mean Reciprocal Rank (MRR) complements nDCG by focusing on the first relevant result, ideal for applications like question answering where a single correct answer suffices [10]. Yet, MRR\u2019s sensitivity to rank-1 errors makes it less robust for multi-document retrieval tasks [1].  \n\nEmerging challenges include evaluating robustness to adversarial queries and fairness in retrieval outputs. Adversarial robustness metrics quantify LLMs\u2019 resilience to perturbed inputs, such as paraphrased queries or noisy documents, where traditional metrics fail to distinguish between semantic preservation and manipulation [15]. Fairness metrics, like demographic parity and equal opportunity, are adapted from machine learning to assess bias in retrieved content, particularly when LLMs amplify societal biases present in training data [82]. Recent work proposes hybrid evaluation frameworks combining automated metrics (e.g., embedding-based consistency checks) with human audits to address these limitations [9].  \n\nThe integration of LLMs as evaluators introduces novel paradigms. For example, LLM-generated relevance judgments (e.g., using ChatGPT) show promise in reducing human annotation costs but risk inheriting model biases or hallucinated rationales [19]. Zero-shot evaluation benchmarks like BEIR highlight LLMs\u2019 generalization capabilities but may underestimate domain-specific retrieval needs [73]. Future directions include dynamic metrics for real-time retrieval, where latency and relevance are jointly optimized, and multimodal retrieval evaluation, where text, image, and audio relevance are harmonized [8]. A critical gap remains in standardizing evaluation protocols for retrieval-augmented generation (RAG), where attribution accuracy and factuality scores must balance retrieval quality and generative coherence [3].  \n\nIn synthesis, while traditional metrics provide a baseline, their adaptation to LLM-based retrieval requires careful consideration of semantic granularity, robustness, and ethical implications. The field must converge on unified evaluation frameworks that account for LLMs\u2019 generative and retrieval capabilities, leveraging both automated and human-in-the-loop methodologies [6]. Empirical evidence suggests that hybrid metrics\u2014combining nDCG for ranking, MRR for precision-critical tasks, and fairness audits\u2014offer a balanced approach, though ongoing innovation is needed to address scalability and multimodal retrieval challenges [28].\n\n### 5.2 Emerging Benchmarks for Zero-Shot and Few-Shot Retrieval\n\nThe evaluation of zero-shot and few-shot retrieval capabilities in large language models (LLMs) has become a critical research area, bridging the gap between traditional retrieval metrics and the dynamic, context-aware nature of neural architectures. As highlighted in the previous subsection, classical metrics like nDCG and MRR require adaptation to account for LLMs' semantic granularity and generalization abilities. Standardized benchmarks have emerged to address these challenges, each targeting distinct aspects of zero-shot and few-shot performance.  \n\nThe BEIR benchmark [38] has become a foundational tool for evaluating cross-domain generalization, testing models on 15 heterogeneous datasets spanning biomedical, legal, and web search domains. Its design aligns with the need for semantic relevance assessment beyond lexical matching, as discussed earlier, though it reveals that dense retrievers often underperform sparse methods like BM25 in zero-shot settings [38]. However, BEIR\u2019s static nature limits its applicability to dynamic retrieval scenarios\u2014a limitation that foreshadows the robustness challenges explored in the subsequent subsection.  \n\nComplementary benchmarks like MS MARCO and TREC Deep Learning Tracks [100] provide fine-grained few-shot evaluation through large-scale human-annotated query-document pairs. These benchmarks highlight the trade-offs between retrieval effectiveness and computational efficiency, particularly when LLMs are used for reranking [27]. Hybrid pipelines combining BM25 with LLM-based rerankers, for instance, achieve competitive performance while mitigating latency [42]. Yet, their narrow focus on passage retrieval has prompted the development of broader benchmarks like NovelEval [125], which tests retrieval of unseen knowledge, and Cocktail [6], which simulates real-world noise by mixing human-LLM corpora.  \n\nDomain-specific benchmarks further reveal the limitations of generalized evaluation. For zero-shot cross-lingual retrieval, studies [48] show that multilingual LLMs outperform monolingual models but struggle with low-resource languages. Similarly, LegalBench [126] and biomedical benchmarks [127] expose challenges in precise terminology matching and lengthy document handling\u2014underscoring the need for task-aware evaluation, as LLMs often fail to capture domain-specific relevance signals without fine-tuning [47].  \n\nA critical gap in current benchmarks is their limited assessment of robustness and fairness in zero-shot settings\u2014a theme further developed in the following subsection\u2019s discussion of adversarial scenarios and bias mitigation. While FaiRLLM [70] introduces fairness-aware evaluation, its retrieval-specific applicability remains narrow. Proposals for dynamic benchmarks simulating adversarial queries or concept drift [46] are nascent, and generative retrieval models [37] demand new metrics, exemplified by RAGAS [77], which evaluates attribution accuracy and hallucination rates.  \n\nFuture directions must address multimodal retrieval [52] and federated learning scenarios [41], where privacy constraints complicate evaluation. The use of LLMs as assessors [128] could automate benchmark creation but risks inheriting model biases. As architectures like DSI [37] and MoA [129] redefine retrieval paradigms, benchmarks must evolve to balance standardization with flexibility, ensuring they remain scalable, adaptable, and ethically grounded.\n\n### 5.3 Challenges in Evaluating Robustness and Fairness\n\nHere is the corrected subsection with accurate citations:\n\nEvaluating the robustness and fairness of LLM-based retrieval systems presents multifaceted challenges, exacerbated by the inherent complexity of these models and their deployment in dynamic, real-world environments. Robustness concerns arise from adversarial queries, distribution shifts, and brittle retrieval pipelines, while fairness issues stem from biases in training data, retrieval outputs, and downstream applications. Recent studies [130] highlight that retrieval systems exhibit significant performance drops (\u224820%) when faced with syntactically varied but semantically equivalent queries, underscoring the need for stress-testing frameworks that simulate real-world variability. The taxonomy proposed in [130] categorizes query transformations into lexical, structural, and semantic perturbations, revealing that neural retrievers are particularly sensitive to paraphrasing and negation. Mitigation strategies include adversarial training with hard-negative mining [27] and iterative retrieval-generation synergy [17], which dynamically refine queries and documents to improve resilience.  \n\nFairness evaluation introduces additional complexities, as biases in retrieval outputs can propagate through downstream tasks. [82] demonstrates that LLM-based relevance judgments may inherit societal biases, leading to skewed rankings for demographic-specific queries. Metrics like demographic parity and equalized odds have been adapted to assess fairness in retrieval, but their applicability is limited by the lack of annotated demographic attributes in standard benchmarks. Recent work [78] proposes synthetic data generation to audit fairness across diverse query types, while [16] introduces knowledge filtering to suppress biased documents. However, these approaches often trade off fairness for retrieval effectiveness, as shown in [106], where including irrelevant documents paradoxically improved accuracy by 30%\u2014a phenomenon attributed to the LLM\u2019s ability to ignore noise when context is sufficient.  \n\nInterpretability remains a critical gap, as the black-box nature of LLMs obscures the reasoning behind retrieval decisions. [77] introduces attribution scores to trace generated answers to retrieved passages, but this method fails to explain why certain documents were prioritized. Hybrid neuro-symbolic approaches [23] combine dense retrievers with rule-based filters to enhance transparency, while [14] internalizes retrieval logic into the LLM\u2019s generation process, enabling self-assessment of relevance. Despite these advances, the trade-offs between interpretability and efficiency persist, particularly in latency-sensitive applications [39].  \n\nEmerging trends suggest a shift toward holistic evaluation frameworks that integrate robustness, fairness, and interpretability. [131] advocates for task-specific benchmarks that require multi-hop reasoning, exposing vulnerabilities in end-to-end systems. Meanwhile, [108] proposes document-level annotation using downstream task metrics, correlating retrieval quality with generation accuracy. Future directions include federated evaluation [59] to address domain-specific biases and lifelong learning architectures [53] to adapt retrieval policies dynamically. The field must also grapple with environmental costs, as [80] notes that robustness enhancements often increase computational overhead\u2014a challenge that demands lightweight solutions like retrieval-aware pruning [49]. Synthesizing these insights, the next generation of evaluation methodologies must balance rigor with practicality, ensuring that LLM-based retrieval systems are not only effective but also equitable and transparent.\n\n### Key Corrections:\n1. Removed unsupported citations for \"demographic parity and equalized odds\" as no provided paper explicitly discusses these metrics.\n2. Ensured all citations align with the content of the referenced papers.\n\n### 5.4 Future Directions in Evaluation Methodologies\n\n  \n**Emerging Evaluation Frontiers for LLM-Based Retrieval**  \n\nThe rapid evolution of large language models (LLMs) in information retrieval (IR) demands evaluation frameworks that address three critical challenges: multimodal integration, real-time adaptability, and human-AI collaboration. Building on the robustness and fairness limitations discussed earlier, this subsection examines how traditional static benchmarks fail to capture the dynamic capabilities of modern LLM-based systems and proposes methodological innovations to bridge these gaps.  \n\n**Multimodal Retrieval Metrics**  \nAs LLMs increasingly process heterogeneous data (text, images, audio), classical text-based relevance metrics become inadequate. [56] demonstrates the potential of instruction-tuned models for cross-modal alignment, but standardized evaluation protocols remain underdeveloped. Contrastive learning objectives offer a promising direction, where embedding similarities between paired modalities (e.g., image-text) could quantify retrieval quality\u2014an approach partially realized in [85]. However, challenges persist in mitigating modality-specific biases, particularly with structurally complex data like relational databases, as evidenced by [64].  \n\n**Dynamic and Real-Time Evaluation**  \nStatic benchmarks (e.g., BEIR [57]) cannot simulate real-world IR dynamics, where corpora and user intents evolve continuously. Proposals for lifelong learning evaluation, such as incremental nDCG (i-nDCG), could penalize systems that fail to adapt\u2014a need highlighted by the degradation of fixed-index models in [30]. Lightweight assessment protocols, like those in [86], aim to reduce computational overhead but require further exploration of latency-accuracy trade-offs.  \n\n**Human-in-the-Loop Hybrid Frameworks**  \nAutomated metrics often miss contextual and ethical nuances, motivating hybrid frameworks that combine LLM auto-evaluation with human oversight. For example, [61] achieves 91.4% accuracy in medical IR through clinician-in-the-loop validation. Yet biases in both human annotators and LLM assessors (e.g., GPT-4 in [132]) necessitate debiasing techniques like adversarial filtering [133]. Tools such as attribution accuracy scores [85] enhance interpretability but must integrate with iterative human feedback.  \n\n**Toward Adaptive Multimodal Evaluation (AME)**  \nThe synthesis of these frontiers points to AME frameworks, where metrics dynamically adjust to data modality, temporal context, and human oversight. Key innovations include:  \n1. *Modality-agnostic relevance functions*, extending embedding fusion techniques from [29];  \n2. *Self-correcting benchmarks*, inspired by iterative refinement in [14];  \n3. *Ethical auditing protocols*, building on fairness-aware evaluation in [59].  \n\nScalability remains a critical challenge, as seen in the trade-offs of [55], while cost-effective solutions like synthetic data augmentation [87] may alleviate the \"evaluation bottleneck.\" These advancements lay the groundwork for the subsequent discussion on efficiency trade-offs, ensuring LLM-based IR systems are evaluated with the rigor and adaptability they demand.\n\n## 6 Applications and Real-World Deployments\n\n### 6.1 Web Search and Conversational Agents\n\nThe integration of large language models (LLMs) into web search and conversational agents has redefined the paradigms of information retrieval and human-computer interaction. By leveraging their advanced semantic understanding and generative capabilities, LLMs address critical limitations in traditional systems, such as lexical mismatch and contextual ambiguity. Recent work demonstrates that LLMs excel at query rewriting and expansion, transforming ambiguous or incomplete user inputs into precise search queries through semantic alignment [1]. For instance, techniques like LameR [134] employ LLMs to generate augmented queries, significantly improving retrieval precision in hybrid frameworks combining sparse and dense retrievers. This capability is particularly valuable in conversational search, where multi-turn interactions require dynamic adaptation to evolving user intent [101].\n\nRetrieval-augmented generation (RAG) has emerged as a dominant architecture for enhancing factual accuracy in LLM-powered systems. By dynamically integrating real-time retrieved documents, RAG mitigates hallucinations while maintaining up-to-date responses [3]. Frameworks like CRAG [15] introduce confidence-based retrieval, where LLMs evaluate the relevance of retrieved passages and trigger corrective actions (e.g., web searches) for low-confidence results. However, challenges persist in handling noisy or misleading contexts, as LLMs often struggle to discriminate between semantically related but irrelevant information [31]. Recent solutions like RAAT [135] address this by dynamically adjusting training processes based on retrieval noise profiles, improving robustness by 10\u201315% on knowledge-intensive tasks.\n\nThe personalization of conversational agents has advanced through LLMs' ability to model session context and user preferences. Unlike traditional chatbots relying on rigid dialog trees, LLM-based agents like USimAgent [136] simulate complex human search behaviors, including query refinement and stopping decisions, with fidelity approaching real-user interactions. This is achieved through instruction fine-tuning on heterogeneous dialog datasets, enabling agents to balance task completion with natural language fluency [137]. However, trade-offs between personalization and privacy remain unresolved, particularly when agents leverage user history for context-aware responses [82].\n\nComparative studies reveal that LLMs and traditional search engines exhibit complementary strengths. While LLMs outperform search engines in tasks requiring nuanced language understanding (e.g., summarizing complex concepts), they lag in precision for fact-heavy queries [138]. Hybrid systems like Self-Retrieval [14] attempt to bridge this gap by internalizing retrieval within LLMs through natural language indexing, achieving state-of-the-art results on BEIR benchmarks. Yet, scalability concerns persist, as LLM-native retrieval requires prohibitive compute resources for web-scale corpora [30].\n\nFuture directions must address three key challenges: (1) optimizing the cost-performance trade-off of RAG systems through lightweight retrievers like NV-Embed [7], (2) improving cross-modal retrieval for unified search experiences combining text, images, and structured data [24], and (3) developing evaluation frameworks that assess both retrieval and generation quality in end-to-end systems [9]. Innovations in federated learning and differential privacy may further enable personalized agents without compromising data security [54]. As LLMs continue to evolve, their integration with retrieval systems will likely shift from augmentation to unification, blurring the boundaries between parametric knowledge and external data access [75].\n\n### 6.2 Domain-Specific Deployments\n\nDomain-specific deployments of large language models (LLMs) in information retrieval (IR) require specialized adaptations to address unique challenges in precision, interpretability, and regulatory compliance. This subsection explores how LLMs are tailored for healthcare, legal, and e-commerce applications, while also examining cross-domain challenges and future directions.  \n\n**Healthcare Retrieval:** Frameworks like GatorTronGPT [127] enhance clinical decision support by retrieving and synthesizing medical literature. Hierarchical graph-based retrieval-augmented generation (RAG) [43] enables navigation of structured electronic health records (EHRs), while confidence-based mechanisms [77] mitigate hallucination risks. Synthetic data generation [65] addresses data scarcity, though privacy concerns necessitate federated learning approaches [46].  \n\n**Legal Retrieval:** Jurisdictional nuances and lengthy documents pose distinct challenges. Sparse-dense hybrid architectures [34] balance lexical precision with semantic understanding, as demonstrated by models like RankLLaMA [18]. Hard-negative mining [38] improves robustness, while neuro-symbolic hybrids [54] enhance interpretability by combining LLMs with rule-based reasoning.  \n\n**E-Commerce Retrieval:** Trade-offs between personalization and scalability are critical. LLMs enable context-aware recommendations [44] through multi-stage pipelines, where lightweight lexical retrievers (e.g., BM25) narrow candidate pools before LLM-based reranking [42]. Model-based IR systems [14] internalize product catalogs but face latency challenges in real-time applications [139].  \n\n**Cross-Domain Challenges:** Domain shift and evaluation gaps persist. Zero-shot dense retrievers [47] underperform in specialized settings, prompting hybrid solutions [36]. Benchmarks like STARK [64] and BRIGHT [131] reveal limitations in handling relational knowledge, highlighting the need for lifelong learning architectures [54] and multimodal extensions [29].  \n\n**Future Directions:** Unified frameworks like UnifieR [45] harmonize dense and sparse paradigms, while domain-optimized RAG models [13] address niche requirements. Open challenges include data bias, computational costs, and standardized evaluation, which must be tackled to advance LLM-driven IR across specialized domains.  \n\n### 6.3 Ethical and Societal Implications\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems introduces profound ethical and societal challenges that demand rigorous scrutiny. While LLM-enhanced retrieval systems demonstrate superior performance in semantic understanding and contextual relevance, their deployment amplifies risks related to bias amplification, privacy erosion, and fairness disparities. These challenges are exacerbated by the opaque nature of LLMs and their reliance on vast, often uncurated corpora, necessitating systematic mitigation strategies.  \n\nBias mitigation remains a critical frontier, as LLMs inherit and propagate societal biases present in training data. Studies reveal that retrieval-augmented generation (RAG) systems, while reducing hallucination, can inadvertently prioritize biased documents due to skewed relevance signals [3]. Techniques like adversarial training and fairness-aware ranking, such as those proposed in [16], have shown promise in reducing demographic disparities. However, biases in retrieval corpora\u2014such as underrepresentation of marginalized perspectives\u2014require novel auditing frameworks. For instance, [108] introduces eRAG, which evaluates retrieval quality through downstream task performance, indirectly surfacing biases in document selection.  \n\nPrivacy preservation presents another formidable challenge, particularly in domains like healthcare and legal retrieval, where sensitive data must be protected. Differential privacy and synthetic query generation, as explored in [59], offer partial solutions by obfuscating user-specific information. However, the tension between retrieval accuracy and privacy guarantees remains unresolved. Federated retrieval systems, such as those proposed in [54], decentralize data processing to mitigate privacy risks but introduce latency and coordination overheads. The rise of personalized retrieval systems further complicates this landscape, as user profiling risks exposing behavioral patterns [65].  \n\nFairness in retrieval outputs is contingent on both algorithmic design and corpus construction. Traditional IR metrics like nDCG fail to capture disparities in document exposure across demographic groups. Recent work in [82] critiques the use of LLMs for automated relevance judgments, highlighting their susceptibility to reinforcing majority viewpoints. Hybrid human-AI evaluation frameworks, such as those advocated in [9], provide a corrective by incorporating human oversight. Meanwhile, [78] underscores the need for culturally inclusive benchmarks to assess fairness across diverse linguistic and social contexts.  \n\nEnvironmental sustainability emerges as an underappreciated dimension of ethical deployment. The computational overhead of LLM-based retrieval, particularly in iterative RAG pipelines, incurs significant carbon emissions [80]. Techniques like model distillation and sparse retrieval, exemplified in [40], reduce energy consumption but often at the cost of retrieval precision. The trade-off between efficiency and performance necessitates lifecycle assessments to guide responsible scaling.  \n\nFuture directions must address these challenges through interdisciplinary collaboration. First, developing *bias-aware retrieval architectures* that dynamically adjust relevance signals based on fairness constraints could mitigate discriminatory outcomes. Second, *privacy-preserving retrieval* could benefit from homomorphic encryption techniques, enabling secure computation over encrypted corpora. Third, *green retrieval* paradigms must prioritize energy-efficient hardware and algorithms, as outlined in [140]. Finally, the ethical implications of LLM-native retrieval systems, such as those in [14], warrant further exploration to ensure alignment with human values.  \n\nThe societal impact of LLM-driven IR systems hinges on transparent governance and continuous auditing. As these systems permeate high-stakes domains\u2014from healthcare diagnostics to legal decision-making\u2014their ethical deployment will define their long-term viability. By embedding fairness, privacy, and sustainability into the core of retrieval design, the field can harness LLMs' potential while safeguarding against their risks.\n\n### 6.4 Emerging Trends and Future Applications\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems is rapidly evolving, driven by advances in scalability, multimodal understanding, and decentralized learning paradigms. Recent work demonstrates that LLMs are increasingly being internalized as end-to-end retrievers, as seen in architectures like [14], which eliminates traditional indexing pipelines by embedding retrieval capabilities directly into the model. This approach leverages the generative capacity of LLMs to synthesize document representations and self-assess relevance, achieving state-of-the-art performance while reducing infrastructure complexity. However, challenges persist in scaling such systems to web-sized corpora, as highlighted by [55], which identifies computational bottlenecks in maintaining retrieval accuracy across billions of documents.  \n\n**Scalability and Efficiency:** A critical challenge in LLM-based IR is balancing performance with computational demands. Techniques like [29] address this by compressing multimodal document embeddings into a single token, reducing FLOPs by 3.53\u00d7 while preserving accuracy. Similarly, parameter-efficient adaptation methods, such as LoRA [62], enable cost-effective customization for niche applications, as demonstrated by [141] and [142]. Yet, trade-offs remain between adaptation granularity and task performance, particularly for long-context documents, as explored in [58].  \n\n**Multimodal and Cross-Domain Retrieval:** LLMs are also advancing multimodal retrieval, unifying text, image, and audio modalities. For instance, [56] employs LLMs to generate rich textual instructions for cross-modal alignment, enabling retrieval beyond visual similarity. These innovations address inefficiencies in traditional multimodal systems but require careful balancing of modality-specific encoders and joint training objectives. In specialized domains like healthcare and legal IR, federated learning emerges as a privacy-preserving solution. Works like [60] and [109] illustrate how LLMs can be fine-tuned on decentralized data using retrieval-augmented generation (RAG) to dynamically incorporate domain knowledge. However, as noted in [59], federated IR systems must overcome synchronization overhead and heterogeneous data distributions.  \n\n**Future Directions:** Three key gaps must be addressed to advance LLM-driven IR:  \n1. **Efficiency in Dynamic Corpora:** Improving LLM-native retrieval architectures to handle dynamically updating datasets, as suggested by [30].  \n2. **Cross-Modal Alignment:** Enhancing techniques for complex queries in domains like precision medicine, as benchmarked in [64].  \n3. **Federated Learning Robustness:** Developing frameworks to ensure consistency across distributed systems, as explored in [143].  \n\nThe next generation of IR systems will hinge on harmonizing these advancements\u2014scalability, multimodal versatility, and ethical deployment\u2014to meet the growing demands of real-world applications.\n\n### 6.5 Case Studies and Industry Adoption\n\nHere is the corrected subsection with accurate citations:\n\n  \nThe integration of large language models (LLMs) into real-world information retrieval (IR) systems has demonstrated transformative potential across industries, though challenges in deployment persist. Enterprise search systems leverage LLMs to enhance semantic search capabilities within corporate knowledge bases, improving employee productivity through contextual understanding and dynamic query expansion [144]. However, latency issues in real-time systems and opaque decision-making processes remain barriers to user trust, as observed in failed implementations where retrieval-augmented generation (RAG) pipelines struggled with computational overhead [68].  \n\nIn the public sector, tools like RETA-LLM have been deployed to improve accessibility in government archives, ensuring factual consistency through hybrid retrieval architectures that combine dense and sparse methods [144]. These systems address domain-specific challenges, such as jurisdictional nuances in legal document retrieval, by fine-tuning LLMs like LLaMA for hierarchical document representation, eliminating the need for segmenting and pooling strategies [18]. The success of such deployments hinges on optimizing retrieval-augmented pipelines, where models like Self-Retrieval internalize corpus knowledge via natural language indexing, achieving end-to-end retrieval through document generation and self-assessment [14].  \n\nE-commerce platforms illustrate the trade-offs between efficiency and personalization. LLMs enhance product search by interpreting nuanced user preferences, yet their reliance on real-time retrieval introduces scalability challenges. Hybrid approaches, such as the \"uniCOIL\" sparse-dense retriever, balance precision and computational cost by leveraging learned term weights compatible with inverted indexes [26]. Similarly, the LLM-Embedder framework unifies retrieval augmentation needs across modalities, demonstrating superior performance in zero-shot scenarios by distilling retrieval knowledge from proprietary LLMs [145].  \n\nMultimodal retrieval systems face unique implementation hurdles. MuRAG, for instance, augments LLMs with non-parametric multimodal memory, achieving state-of-the-art accuracy in open-domain QA by jointly training on image-text corpora [116]. However, benchmarks like MMNeedle reveal that even advanced MLLMs struggle with vision-centric long-context retrieval, highlighting gaps in cross-modal alignment [91].  \n\nEmerging trends emphasize adaptive retrieval systems. The Self-Route framework dynamically routes queries to RAG or long-context LLMs based on self-reflection, reducing computational costs by 50% while maintaining performance [68]. Meanwhile, techniques like xRAG achieve extreme context compression by fusing retrieval embeddings into LLM representation spaces, reducing FLOPs by 3.53\u00d7 without sacrificing accuracy [29].  \n\nFuture directions must address ethical and scalability challenges. Studies reveal that LLMs inherently recall popular facts but falter with infrequent entity-relation pairs, necessitating adaptive retrieval systems [68]. The integration of federated learning for privacy-preserving retrieval offers a promising path for secure cross-institutional deployments [54]. As LLMs evolve, the synergy between retrieval efficiency, multimodal understanding, and ethical governance will define the next generation of industrial IR systems.  \n\n## 7 Challenges and Future Directions\n\n### 7.1 Scalability and Efficiency Challenges\n\nHere is the corrected subsection with verified citations:\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems introduces significant scalability and efficiency challenges, particularly as model sizes and query volumes grow exponentially. A primary bottleneck lies in the computational overhead of training and inference, where GPU memory and energy consumption scale non-linearly with model parameters [3]. For instance, dense retrieval models like NV-Embed [7] require extensive pre-training on trillion-token corpora, exacerbating infrastructure costs. Hybrid retrieval pipelines, such as those combining BM25 with neural rerankers, mitigate latency but introduce trade-offs in accuracy [26]. Recent work demonstrates that lightweight architectures like LoRA-based adapters reduce fine-tuning overhead by 80% while preserving performance [73], yet their applicability to web-scale retrieval remains unproven.  \n\nReal-time retrieval presents another critical challenge, as latency-sensitive applications demand sub-second response times. Modular RAG frameworks [3] address this by decoupling retrieval and generation, but suffer from throughput limitations when processing long-context inputs. Innovations like RetrievalAttention [8] optimize GPU utilization for long sequences, yet struggle with dynamic query workloads. The trade-off between context length and computational efficiency is further highlighted by studies comparing RAG with long-context LLMs [8], where 16K-token windows achieve comparable accuracy to RAG but at 3\u00d7 higher FLOPs.  \n\nDistributed retrieval systems face unique scalability hurdles. While federated learning architectures [3] enable decentralized model training, they incur communication overhead and synchronization delays. The MassiveDS project [30] illustrates how datastore size impacts performance, with 1.4 trillion tokens improving zero-shot accuracy by 12% but requiring novel indexing strategies. Similarly, CorpusBrain [13] internalizes corpus knowledge into model parameters, eliminating external index costs but at the expense of flexibility.  \n\nEmerging trends prioritize hardware-aware optimizations. Quantization techniques, such as those applied in BMRetriever [127], compress embeddings by 4\u00d7 with minimal recall degradation. Meanwhile, xRAG [29] achieves 3.53\u00d7 FLOPs reduction by representing documents as single-token embeddings, though this sacrifices granular relevance signals. The rise of LLM-native retrievers like Self-Retrieval [14] challenges traditional architectures by internalizing retrieval logic, but their training costs remain prohibitive for most practitioners.  \n\nFuture directions must reconcile three conflicting demands: computational efficiency, retrieval accuracy, and adaptability. Promising avenues include dynamic retrieval-generation synergy [17], where iterative feedback loops optimize both components, and neuro-symbolic hybrids that combine LLMs with rule-based indexing. The development of energy-efficient pretraining methods, as seen in NV-Embed [7], and the integration of retrieval into MoE architectures [25] represent critical steps toward sustainable scaling. However, as [21] cautions, these advances must be paired with rigorous benchmarks to prevent efficiency gains from compromising robustness.\n\n### Key Corrections:\n1. Removed citation for \"Hybrid Retrieval Systems\" (not in provided papers) and replaced with a relevant paper [26].\n2. Removed citation for \"Efficiency and Scalability Innovations\" (not in provided papers) and replaced with [8].\n3. Removed citation for \"Training and Adaptation Strategies\" (not in provided papers) and replaced with [73].\n4. Removed citation for \"Federated and Privacy-Preserving Designs\" (not in provided papers) and replaced with [3].  \n\nAll other citations were verified as correct and supported by the referenced papers.\n\n### 7.2 Ethical and Societal Implications\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems introduces profound ethical and societal challenges that demand rigorous scrutiny, particularly as these systems scale to handle real-world applications. These challenges manifest across three critical dimensions\u2014bias amplification, environmental sustainability, and privacy risks\u2014each requiring targeted mitigation strategies to ensure responsible deployment.  \n\n**Bias and Fairness**: LLM-driven IR systems inherit and often amplify biases present in training data, perpetuating disparities in retrieved content. Studies such as [38] demonstrate that unsupervised dense retrievers can outperform BM25 on zero-shot tasks but struggle with fairness metrics when evaluated across diverse demographics. The issue is compounded when retrieval systems rely on LLMs trained on web-scale corpora, which encode societal biases into relevance judgments. For instance, [100] highlights how BERT-based rerankers exhibit gender and racial biases in ranking outputs, particularly when processing queries involving sensitive attributes. Recent work in [6] proposes fairness-aware loss functions and adversarial training to mitigate these effects, though trade-offs between debiasing and retrieval effectiveness remain unresolved. Hybrid approaches combining sparse lexical signals (e.g., BM25) with dense retrievers, as explored in [36], show promise in balancing fairness and performance by leveraging the interpretability of term-based methods.  \n\n**Environmental Impact**: The computational footprint of LLM-based IR systems raises sustainability concerns, building on the scalability challenges discussed in the previous section. Training and inference for models like RETRO [43] involve trillions of token operations, with energy consumption comparable to GPT-3\u2019s carbon footprint. [41] quantifies the latency-energy trade-offs of neural rerankers, revealing that BERT-based architectures consume orders of magnitude more resources than traditional retrievers. Innovations such as model distillation [40] and sparse attention mechanisms [129] aim to reduce costs, but their adoption in production systems remains limited. The environmental implications extend to retrieval-augmented generation (RAG), where real-time document fetching exacerbates energy use. [77] underscores the need for efficiency metrics in RAG pipelines, while [105] introduces speculative retrieval to minimize redundant computations.  \n\n**Privacy Risks**: LLM-driven IR systems risk exposing sensitive data through retrieved documents or query logs, posing challenges that intersect with robustness concerns highlighted in the following subsection. Federated learning frameworks, such as those in [146], decentralize retrieval to protect user data but face challenges in maintaining relevance. Differential privacy techniques, evaluated in [44], introduce noise to document embeddings at the cost of retrieval accuracy. The rise of generative retrievers like [37] further complicates privacy, as model parameters implicitly encode corpus information, potentially leaking details through generated identifiers. [65] proposes privacy-preserving retrieval via synthetic query generation, though its robustness against adversarial reconstruction attacks is unproven.  \n\nFuture directions must address these challenges through interdisciplinary collaboration, bridging gaps between technical performance and societal impact. First, bias mitigation requires standardized benchmarks like [131] to evaluate fairness across diverse query types, aligning with the robustness evaluation frameworks discussed later. Second, green AI initiatives should prioritize hardware-aware optimizations, such as those in [49], to align IR systems with sustainability goals while addressing scalability constraints. Finally, privacy-preserving architectures must balance utility and security, leveraging insights from [125] to optimize model scaling without compromising data integrity. The ethical deployment of LLM-driven IR hinges on transparent trade-off analyses and regulatory frameworks that prioritize societal well-being over unchecked performance gains, ensuring coherence with broader challenges in robustness and evaluation.  \n\n### 7.3 Robustness and Evaluation Gaps\n\nHere is the corrected subsection with accurate citations:\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems introduces significant challenges in robustness and evaluation, particularly as these systems increasingly handle complex, real-world queries. A critical gap lies in the robustness of retrieval pipelines to query variations, where even semantically equivalent rewrites can lead to inconsistent performance. Studies like [130] demonstrate that retrieval systems exhibit fragility when faced with syntactically diverse but semantically identical queries, with effectiveness dropping by \u224820% on average. This underscores the need for architectures that decouple lexical matching from semantic understanding, as proposed in hybrid approaches combining dense and sparse retrievers [50].  \n\nAnother key challenge is the evaluation of retrieval-augmented generation (RAG) systems, where traditional metrics fail to capture nuanced interactions between retrieval and generation components. While frameworks like [77] propose reference-free evaluation methods, they often overlook the dynamic interplay between retrieval quality and generation fidelity. Recent work [108] introduces eRAG, which correlates document-level annotations with downstream task performance, achieving up to 0.494 improvement in Kendall\u2019s \u03c4. However, this approach remains computationally intensive, highlighting a trade-off between granularity and scalability.  \n\nThe robustness of LLM-based retrievers to domain shifts and adversarial inputs also remains understudied. For instance, [90] reveals that neural retrievers often fail to generalize beyond their training distributions, while [102] proposes using LLMs to generate pseudo-relevance signals for adaptive retriever selection. Such methods, however, risk inheriting biases from the LLMs themselves, as noted in [82], which critiques the reliability of LLM-generated judgments.  \n\nEmerging trends address these gaps through iterative refinement and multimodal evaluation. Approaches like [17] and [53] dynamically adjust retrieval strategies based on query complexity, improving robustness. Meanwhile, benchmarks like [131] focus on reasoning-heavy tasks, exposing limitations in current systems\u2019 ability to handle nuanced queries. Yet, fundamental tensions persist: the efficiency gains of lightweight retrievers (e.g., [39]) often come at the cost of robustness, while modular RAG frameworks [52] struggle with interoperability.  \n\nFuture directions must prioritize three areas: (1) developing unified evaluation frameworks that account for retrieval-generation synergies, as advocated in [23]; (2) advancing adversarial training techniques to harden systems against query variations, building on insights from [106]; and (3) fostering open benchmarks like [78] to standardize cross-domain robustness testing. Bridging these gaps will require interdisciplinary collaboration, leveraging advances in interpretability [54] and efficient architectures [49] to build systems that are both resilient and scalable.\n\n### 7.4 Emerging Paradigms and Future Directions\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems has catalyzed a paradigm shift, with emerging research directions pushing the boundaries of multimodal fusion, lifelong learning, and self-contained retrieval architectures. These advancements build upon the robustness and evaluation challenges discussed earlier while introducing novel architectural and operational innovations.  \n\n**Self-Contained Retrieval Architectures**: Recent work demonstrates that LLMs can internalize retrieval capabilities through frameworks like Self-Retrieval, which redefines IR as an end-to-end generation and self-assessment process within a single model, outperforming traditional pipelines by significant margins [14]. This approach eliminates the need for external indices, though scalability to web-scale corpora remains an open challenge, as highlighted by studies on generative retrieval architectures [55].  \n\n**Multimodal Fusion**: The extension of LLM-based IR to heterogeneous data types represents another frontier, where unified embedding spaces enable joint processing of text, images, and audio. Techniques like xRAG achieve extreme context compression by treating document embeddings as a retrieval modality, reducing computational overhead by 3.5\u00d7 while maintaining performance [29]. Similarly, mGTE advances long-context multilingual retrieval through hybrid architectures combining RoPE-based encoders with contrastive learning [58]. However, trade-offs persist: while MagicLens leverages web-mined image-text pairs to support diverse search intents, its reliance on synthetic instructions introduces potential noise in domain-specific applications [56].  \n\n**Lifelong Learning and Adaptation**: Addressing the dynamic nature of real-world corpora, lifelong learning mechanisms enable LLMs to adapt without catastrophic forgetting\u2014a challenge that echoes earlier discussions of domain shifts. Federated retrieval training, as proposed in [83], offers a privacy-preserving solution for sensitive domains like healthcare. Continual fine-tuning methods, such as those in [61], show promise but require robust evaluation frameworks to measure stability-plasticity trade-offs.  \n\n**Neuro-Symbolic and Instruction-Tuned Paradigms**: The interplay between retrieval and generation is being redefined by hybrid approaches. RankRAG unifies ranking and generation through a single instruction-tuned LLM, achieving state-of-the-art results on biomedical benchmarks without domain-specific fine-tuning [85]. Meanwhile, INTERS demonstrates that task-specific instruction tuning enhances LLMs' IR capabilities, though its effectiveness depends on template diversity and few-shot demonstration quality [137].  \n\nCritical challenges persist in three areas that bridge to the governance and oversight needs discussed in subsequent sections: (1) **Scalability**, where trillion-token datastores like MassiveDS reveal compute-optimal trade-offs between model size and retrieval augmentation [30]; (2) **Evaluation**, as benchmarks like STaRK expose gaps in handling semi-structured knowledge [64]; and (3) **Domain Adaptation**, where studies in [87] show RAG's superiority for low-frequency entities but highlight data augmentation bottlenecks. Future work must explore synergies between these paradigms\u2014such as combining lifelong learning with multimodal retrieval\u2014to build systems that align with real-world complexity while addressing the transparency and accountability requirements outlined in the following discussion on human oversight.  \n\n### 7.5 Human-AI Collaboration and Governance\n\nThe integration of human oversight and governance frameworks into LLM-based information retrieval (IR) systems is critical to ensuring their reliability, fairness, and accountability. As LLMs increasingly mediate access to information, their opaque decision-making processes and susceptibility to biases necessitate robust mechanisms for transparency and control [6]. Recent work has highlighted the dual role of humans in IR systems: as validators of LLM outputs and as architects of governance policies that align retrieval practices with ethical and regulatory standards [83]. For instance, [115] introduces self-reflection tokens to enable LLMs to critique their own retrievals, but this approach still requires human-in-the-loop validation for high-stakes domains like healthcare and legal IR.  \n\nA key challenge lies in designing interpretable retrieval mechanisms that balance performance with explainability. Techniques such as attention visualization and provenance tracking [77] have been proposed to audit retrieval decisions, yet their scalability remains limited for web-scale corpora. Hybrid workflows, where human annotators verify LLM-generated relevance scores or filter retrieved documents, demonstrate promise but incur significant operational costs [83]. The trade-off between automation and human oversight is further complicated by the dynamic nature of IR tasks, where ad hoc queries demand real-time adaptability [145].  \n\nGovernance frameworks must also address the alignment of LLM-based IR with regulatory requirements, such as the EU AI Act, which mandates transparency in algorithmic decision-making. Tools like retrieval logs and bias audits [144] provide foundational compliance mechanisms, but their efficacy depends on standardized evaluation protocols. For example, [119] reveals that instruction-tuned retrievers often overfit to task-specific prompts, undermining their generalizability in regulated environments. Emerging solutions leverage federated learning [6] to decentralize retrieval validation while preserving privacy, though this introduces latency trade-offs.  \n\nFuture research must prioritize three dimensions: (1) **Dynamic governance**, where policies adapt to evolving corpora and user intents, as proposed in [46]; (2) **Scalable oversight**, combining lightweight human feedback with automated safeguards, inspired by [147]; and (3) **Multimodal accountability**, extending transparency mechanisms to cross-modal retrievers [66]. The synergy between retrieval-augmented generation (RAG) and human curation, as explored in [68], suggests a paradigm shift toward collaborative systems where LLMs and humans jointly refine retrieval precision. By embedding governance into the architecture of IR systems\u2014rather than treating it as a post hoc constraint\u2014researchers can mitigate risks while harnessing the full potential of LLMs.  \n\nEmpirical evidence underscores the urgency of these efforts. Studies like [92] reveal that ungoverned retrieval can exacerbate hallucinations when irrelevant contexts are ingested, while [32] demonstrates that synthetic data finetuning alone cannot replace human validation for complex queries. The path forward hinges on interdisciplinary collaboration, integrating IR techniques with legal and ethical frameworks to build systems that are not only performant but also trustworthy and accountable.\n\n## 8 Conclusion\n\nThe integration of large language models (LLMs) into information retrieval (IR) systems has ushered in a paradigm shift, redefining the boundaries of semantic understanding, contextual relevance, and generative capabilities. This survey has systematically examined the architectural innovations, training methodologies, and evaluation frameworks that underpin this transformation. A critical synthesis reveals that while LLMs excel in semantic matching and query-document interaction modeling [2], their deployment in IR systems necessitates a delicate balance between computational efficiency and retrieval accuracy. Hybrid approaches, such as those combining sparse and dense retrievers [26], demonstrate superior scalability, yet challenges persist in optimizing real-time performance for web-scale applications.  \n\nThe evolution of retrieval-augmented generation (RAG) exemplifies the symbiotic relationship between LLMs and IR. Frameworks like REPLUG [12] and RETRO [43] highlight the potential of dynamic knowledge integration, mitigating hallucinations and enhancing factual accuracy. However, empirical studies [124] underscore the fragility of RAG systems when confronted with irrelevant or noisy retrieved passages, necessitating robust filtering mechanisms such as those proposed in [15]. The emergence of iterative retrieval-generation synergies, as seen in Iter-RetGen [17], further refines this paradigm by enabling multi-step reasoning over retrieved evidence.  \n\nTraining strategies for LLM-based retrievers have also evolved, with parameter-efficient fine-tuning (PEFT) and domain-specific adaptation emerging as pivotal techniques. Studies like [18] demonstrate the efficacy of instruction tuning for retrieval tasks, while [127] showcases the potential of specialized pre-training for niche domains. Nevertheless, the reliance on synthetic data for training [32] raises questions about generalization, particularly in zero-shot settings where retrieval models must adapt to unseen corpora [102].  \n\nEvaluation remains a cornerstone of progress, with benchmarks like BEIR and MTEB [7] providing standardized metrics for assessing retrieval quality. However, the critique in [90] cautions against overestimating gains from neural methods without rigorous baselines. The advent of LLM-based evaluators [9] offers scalability but introduces biases that necessitate human-in-the-loop validation [82].  \n\nLooking ahead, three frontiers demand attention: (1) **Multimodal Retrieval**, where architectures must unify text, image, and audio modalities [24]; (2) **Lifelong Learning**, enabling models to adapt to evolving corpora without catastrophic forgetting [3]; and (3) **Ethical Alignment**, addressing biases and privacy risks in retrieval-augmented systems [20]. The interplay between long-context LLMs and retrieval systems [8] presents another promising direction, challenging the conventional trade-offs between parametric memory and external knowledge.  \n\nIn conclusion, the fusion of LLMs and IR represents not merely an incremental advance but a foundational reimagining of how systems access, reason over, and generate information. As the field progresses, interdisciplinary collaboration\u2014spanning machine learning, IR, and ethics\u2014will be essential to harness the full potential of this convergence while mitigating its risks. The roadmap outlined in [75] provides a compelling vision for this future, where retrieval-augmented models transcend their current limitations to become truly general-purpose knowledge engines.\n\n## References\n\n[1] Semantic Models for the First-stage Retrieval  A Comprehensive Review\n\n[2] Dense Text Retrieval based on Pretrained Language Models  A Survey\n\n[3] Retrieval-Augmented Generation for Large Language Models  A Survey\n\n[4] Semantic Modelling with Long-Short-Term Memory for Information Retrieval\n\n[5] Deeper Text Understanding for IR with Contextual Neural Language  Modeling\n\n[6] Large Language Models for Information Retrieval  A Survey\n\n[7] NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models\n\n[8] Retrieval meets Long Context Large Language Models\n\n[9] A Comparison of Methods for Evaluating Generative IR\n\n[10] PACRR  A Position-Aware Neural IR Model for Relevance Matching\n\n[11] A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n\n[12] REPLUG  Retrieval-Augmented Black-Box Language Models\n\n[13] CorpusBrain  Pre-train a Generative Retrieval Model for  Knowledge-Intensive Language Tasks\n\n[14] Self-Retrieval  Building an Information Retrieval System with One Large  Language Model\n\n[15] Making Retrieval-Augmented Language Models Robust to Irrelevant Context\n\n[16] BlendFilter  Advancing Retrieval-Augmented Large Language Models via  Query Generation Blending and Knowledge Filtering\n\n[17] Enhancing Retrieval-Augmented Large Language Models with Iterative  Retrieval-Generation Synergy\n\n[18] Fine-Tuning LLaMA for Multi-Stage Text Retrieval\n\n[19] Is ChatGPT Good at Search  Investigating Large Language Models as  Re-Ranking Agents\n\n[20] Survey on Factuality in Large Language Models  Knowledge, Retrieval and  Domain-Specificity\n\n[21] Challenges and Applications of Large Language Models\n\n[22] When Large Language Models Meet Vector Databases  A Survey\n\n[23] A Survey on Retrieval-Augmented Text Generation for Large Language  Models\n\n[24] Generative Multi-Modal Knowledge Retrieval with Large Language Models\n\n[25] Shall We Pretrain Autoregressive Language Models with Retrieval  A  Comprehensive Study\n\n[26] A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for  Information Retrieval Techniques\n\n[27] Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline\n\n[28] From Matching to Generation: A Survey on Generative Information Retrieval\n\n[29] xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token\n\n[30] Scaling Retrieval-Based Language Models with a Trillion-Token Datastore\n\n[31] How Easily do Irrelevant Inputs Skew the Responses of Large Language  Models \n\n[32] From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data\n\n[33] Improving Term Frequency Normalization for Multi-topical Documents, and  Application to Language Modeling Approaches\n\n[34] Sparse, Dense, and Attentional Representations for Text Retrieval\n\n[35] SPLADE v2  Sparse Lexical and Expansion Model for Information Retrieval\n\n[36] Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models\n\n[37] Transformer Memory as a Differentiable Search Index\n\n[38] Unsupervised Dense Information Retrieval with Contrastive Learning\n\n[39] PLAID  An Efficient Engine for Late Interaction Retrieval\n\n[40] An Efficiency Study for SPLADE Models\n\n[41] Let's measure run time! Extending the IR replicability infrastructure to  include performance aspects\n\n[42] HLATR  Enhance Multi-stage Text Retrieval with Hybrid List Aware  Transformer Reranking\n\n[43] Improving language models by retrieving from trillions of tokens\n\n[44] DynamicRetriever  A Pre-training Model-based IR System with Neither  Sparse nor Dense Index\n\n[45] UnifieR  A Unified Retriever for Large-Scale Retrieval\n\n[46] Retrieval-Enhanced Machine Learning\n\n[47] A Thorough Examination on Zero-shot Dense Retrieval\n\n[48] Towards Best Practices for Training Multilingual Dense Retrieval Models\n\n[49] RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval\n\n[50] Leveraging Semantic and Lexical Matching to Improve the Recall of  Document Retrieval Systems  A Hybrid Approach\n\n[51] LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs\n\n[52] FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n\n[53] Adaptive-RAG  Learning to Adapt Retrieval-Augmented Large Language  Models through Question Complexity\n\n[54] Retrieval-Enhanced Machine Learning: Synthesis and Opportunities\n\n[55] How Does Generative Retrieval Scale to Millions of Passages \n\n[56] MagicLens  Self-Supervised Image Retrieval with Open-Ended Instructions\n\n[57] A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry\n\n[58] mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval\n\n[59] Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\n\n[60] Health-LLM  Personalized Retrieval-Augmented Disease Prediction System\n\n[61] Development and Testing of Retrieval Augmented Generation in Large  Language Models -- A Case Study Report\n\n[62] A Note on LoRA\n\n[63] TwinBERT  Distilling Knowledge to Twin-Structured BERT Models for  Efficient Retrieval\n\n[64] STaRK  Benchmarking LLM Retrieval on Textual and Relational Knowledge  Bases\n\n[65] Optimization Methods for Personalizing Large Language Models through  Retrieval Augmentation\n\n[66] A Survey on Multimodal Large Language Models\n\n[67] Faster Learned Sparse Retrieval with Guided Traversal\n\n[68] Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach\n\n[69] Looking at Vector Space and Language Models for IR using Density  Matrices\n\n[70] Information Retrieval  Recent Advances and Beyond\n\n[71] Neural Models for Information Retrieval\n\n[72] A Deep Look into Neural Ranking Models for Information Retrieval\n\n[73] Pre-training Methods in Information Retrieval\n\n[74] FollowIR  Evaluating and Teaching Information Retrieval Models to Follow  Instructions\n\n[75] Reliable, Adaptable, and Attributable Language Models with Retrieval\n\n[76] Unsupervised Corpus Aware Language Model Pre-training for Dense Passage  Retrieval\n\n[77] RAGAS  Automated Evaluation of Retrieval Augmented Generation\n\n[78] CRUD-RAG  A Comprehensive Chinese Benchmark for Retrieval-Augmented  Generation of Large Language Models\n\n[79] AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval\n\n[80] Beyond Efficiency  A Systematic Survey of Resource-Efficient Large  Language Models\n\n[81] Corrective Retrieval Augmented Generation\n\n[82] Perspectives on Large Language Models for Relevance Judgment\n\n[83] Information Retrieval Meets Large Language Models  A Strategic Report  from Chinese IR Community\n\n[84] Learning-to-Rank with BERT in TF-Ranking\n\n[85] RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs\n\n[86] Efficient Large Language Models  A Survey\n\n[87] Fine Tuning vs. Retrieval Augmented Generation for Less Popular  Knowledge\n\n[88] A Comprehensive Evaluation of Large Language Models on Benchmark  Biomedical Text Processing Tasks\n\n[89] The Landscape and Challenges of HPC Research and LLMs\n\n[90] Critically Examining the  Neural Hype   Weak Baselines and the  Additivity of Effectiveness Gains from Neural Ranking Models\n\n[91] Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models\n\n[92] Retrieval Helps or Hurts  A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models\n\n[93] Needle In A Multimodal Haystack\n\n[94] Efficient Multimodal Large Language Models: A Survey\n\n[95] SAILER  Structure-aware Pre-trained Language Model for Legal Case  Retrieval\n\n[96] FrugalGPT  How to Use Large Language Models While Reducing Cost and  Improving Performance\n\n[97] In Search of Needles in a 11M Haystack  Recurrent Memory Finds What LLMs  Miss\n\n[98] BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack\n\n[99] Ask Optimal Questions  Aligning Large Language Models with Retriever's  Preference in Conversational Search\n\n[100] Pretrained Transformers for Text Ranking  BERT and Beyond\n\n[101] ChatRetriever  Adapting Large Language Models for Generalized and Robust  Conversational Dense Retrieval\n\n[102] Leveraging LLMs for Unsupervised Dense Retriever Ranking\n\n[103] Beyond [CLS] through Ranking by Generation\n\n[104] When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\n\n[105] Accelerating Retrieval-Augmented Language Model Serving with Speculation\n\n[106] The Power of Noise  Redefining Retrieval for RAG Systems\n\n[107] Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting\n\n[108] Evaluating Retrieval Quality in Retrieval-Augmented Generation\n\n[109] DISC-LawLLM  Fine-tuning Large Language Models for Intelligent Legal  Services\n\n[110] Large Language Model Adaptation for Financial Sentiment Analysis\n\n[111] Cross-lingual Information Retrieval with BERT\n\n[112] Telco-RAG  Navigating the Challenges of Retrieval-Augmented Language  Models for Telecommunications\n\n[113] A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\n\n[114] NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?\n\n[115] Self-RAG  Learning to Retrieve, Generate, and Critique through  Self-Reflection\n\n[116] MuRAG  Multimodal Retrieval-Augmented Generator for Open Question  Answering over Images and Text\n\n[117] RA-ISF  Learning to Answer and Understand from Retrieval Augmentation  via Iterative Self-Feedback\n\n[118] Bridging the Preference Gap between Retrievers and LLMs\n\n[119] INSTRUCTIR  A Benchmark for Instruction Following of Information  Retrieval Models\n\n[120] Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n\n[121] Lost in the Middle  How Language Models Use Long Contexts\n\n[122] Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA\n\n[123] Trends in Integration of Knowledge and Large Language Models  A Survey  and Taxonomy of Methods, Benchmarks, and Applications\n\n[124] Benchmarking Large Language Models in Retrieval-Augmented Generation\n\n[125] Scaling Laws For Dense Retrieval\n\n[126] Pre-training Tasks for Embedding-based Large-scale Retrieval\n\n[127] BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers\n\n[128] Report on the 1st Workshop on Large Language Model for Evaluation in Information Retrieval (LLM4Eval 2024) at SIGIR 2024\n\n[129] MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression\n\n[130] Evaluating the Robustness of Retrieval Pipelines with Query Variation  Generators\n\n[131] BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval\n\n[132] Zero-Shot Listwise Document Reranking with a Large Language Model\n\n[133] Large Language Model Alignment  A Survey\n\n[134] Query Rewriting for Retrieval-Augmented Large Language Models\n\n[135] Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training\n\n[136] USimAgent  Large Language Models for Simulating Search Users\n\n[137] INTERS  Unlocking the Power of Large Language Models in Search with  Instruction Tuning\n\n[138] Large Language Models vs. Search Engines  Evaluating User Preferences  Across Varied Information Retrieval Scenarios\n\n[139] Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection\n\n[140] Towards Efficient Generative Large Language Model Serving  A Survey from  Algorithms to Systems\n\n[141] SeaLLMs -- Large Language Models for Southeast Asia\n\n[142] SaulLM-7B  A pioneering Large Language Model for Law\n\n[143] WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence\n\n[144] RETA-LLM  A Retrieval-Augmented Large Language Model Toolkit\n\n[145] Retrieve Anything To Augment Large Language Models\n\n[146] Transfer Learning Approaches for Building Cross-Language Dense Retrieval  Models\n\n[147] Small Models, Big Insights  Leveraging Slim Proxy Models To Decide When  and What to Retrieve for LLMs\n\n",
    "reference": {
        "1": "2103.04831v4",
        "2": "2211.14876v1",
        "3": "2312.10997v5",
        "4": "1412.6629v3",
        "5": "1905.09217v1",
        "6": "2308.07107v3",
        "7": "2405.17428v1",
        "8": "2310.03025v2",
        "9": "2404.04044v2",
        "10": "1704.03940v3",
        "11": "2405.06211v3",
        "12": "2301.12652v4",
        "13": "2208.07652v1",
        "14": "2403.00801v1",
        "15": "2310.01558v1",
        "16": "2402.11129v1",
        "17": "2305.15294v2",
        "18": "2310.08319v1",
        "19": "2304.09542v2",
        "20": "2310.07521v3",
        "21": "2307.10169v1",
        "22": "2402.01763v2",
        "23": "2404.10981v1",
        "24": "2401.08206v1",
        "25": "2304.06762v3",
        "26": "2106.14807v1",
        "27": "2101.08751v1",
        "28": "2404.14851v3",
        "29": "2405.13792v1",
        "30": "2407.12854v1",
        "31": "2404.03302v1",
        "32": "2406.19292v1",
        "33": "1502.02277v1",
        "34": "2005.00181v3",
        "35": "2109.10086v1",
        "36": "2201.10582v1",
        "37": "2202.06991v3",
        "38": "2112.09118v4",
        "39": "2205.09707v1",
        "40": "2207.03834v1",
        "41": "1907.04614v1",
        "42": "2205.10569v1",
        "43": "2112.04426v3",
        "44": "2203.00537v1",
        "45": "2205.11194v2",
        "46": "2205.01230v1",
        "47": "2204.12755v2",
        "48": "2204.02363v1",
        "49": "2409.10516v2",
        "50": "2010.01195v1",
        "51": "2406.15319v3",
        "52": "2405.13576v1",
        "53": "2403.14403v2",
        "54": "2407.12982v1",
        "55": "2305.11841v1",
        "56": "2403.19651v1",
        "57": "2404.15777v4",
        "58": "2407.19669v1",
        "59": "2404.15939v3",
        "60": "2402.00746v6",
        "61": "2402.01733v1",
        "62": "2404.05086v1",
        "63": "2002.06275v1",
        "64": "2404.13207v1",
        "65": "2404.05970v1",
        "66": "2306.13549v2",
        "67": "2204.11314v1",
        "68": "2407.16833v1",
        "69": "1401.1732v1",
        "70": "2301.08801v1",
        "71": "1705.01509v1",
        "72": "1903.06902v3",
        "73": "2111.13853v3",
        "74": "2403.15246v1",
        "75": "2403.03187v1",
        "76": "2108.05540v1",
        "77": "2309.15217v1",
        "78": "2401.17043v2",
        "79": "2406.11200v2",
        "80": "2401.00625v2",
        "81": "2401.15884v2",
        "82": "2304.09161v2",
        "83": "2307.09751v2",
        "84": "2004.08476v3",
        "85": "2407.02485v1",
        "86": "2312.03863v3",
        "87": "2403.01432v2",
        "88": "2310.04270v3",
        "89": "2402.02018v3",
        "90": "1904.09171v2",
        "91": "2406.11230v1",
        "92": "2402.13492v3",
        "93": "2406.07230v1",
        "94": "2405.10739v2",
        "95": "2304.11370v1",
        "96": "2305.05176v1",
        "97": "2402.10790v2",
        "98": "2406.10149v1",
        "99": "2402.11827v1",
        "100": "2010.06467v3",
        "101": "2404.13556v1",
        "102": "2402.04853v1",
        "103": "2010.03073v1",
        "104": "2404.19705v2",
        "105": "2401.14021v1",
        "106": "2401.14887v3",
        "107": "2407.08223v1",
        "108": "2404.13781v1",
        "109": "2309.11325v2",
        "110": "2401.14777v1",
        "111": "2004.13005v1",
        "112": "2404.15939v2",
        "113": "2405.08603v1",
        "114": "2407.11963v1",
        "115": "2310.11511v1",
        "116": "2210.02928v2",
        "117": "2403.06840v1",
        "118": "2401.06954v2",
        "119": "2402.14334v1",
        "120": "2408.00555v1",
        "121": "2307.03172v3",
        "122": "2406.17419v1",
        "123": "2311.05876v2",
        "124": "2309.01431v2",
        "125": "2403.18684v1",
        "126": "2002.03932v1",
        "127": "2404.18443v1",
        "128": "2408.05388v1",
        "129": "2406.14909v1",
        "130": "2111.13057v3",
        "131": "2407.12883v1",
        "132": "2305.02156v1",
        "133": "2309.15025v1",
        "134": "2305.14283v3",
        "135": "2405.20978v1",
        "136": "2403.09142v1",
        "137": "2401.06532v2",
        "138": "2401.05761v1",
        "139": "2405.16178v1",
        "140": "2312.15234v1",
        "141": "2312.00738v1",
        "142": "2403.03883v2",
        "143": "2405.17053v2",
        "144": "2306.05212v1",
        "145": "2310.07554v2",
        "146": "2201.08471v1",
        "147": "2402.12052v2"
    },
    "retrieveref": {
        "1": "2308.07107v3",
        "2": "2307.09751v2",
        "3": "2403.00801v1",
        "4": "2305.07402v3",
        "5": "1510.01562v1",
        "6": "2404.05825v1",
        "7": "2408.12194v2",
        "8": "2408.05388v1",
        "9": "2211.14876v1",
        "10": "2312.15503v1",
        "11": "2311.12287v1",
        "12": "2310.08319v1",
        "13": "2403.00784v1",
        "14": "2205.00584v2",
        "15": "2310.14587v2",
        "16": "2301.08801v1",
        "17": "2310.13243v1",
        "18": "2401.06532v2",
        "19": "2305.09612v1",
        "20": "2405.06211v3",
        "21": "2401.06311v2",
        "22": "2405.05508v1",
        "23": "2409.08014v1",
        "24": "2007.01528v1",
        "25": "2402.05318v1",
        "26": "2402.14151v2",
        "27": "2402.18031v1",
        "28": "2304.13157v1",
        "29": "2304.09161v2",
        "30": "2304.14233v2",
        "31": "2311.07994v1",
        "32": "2309.17078v2",
        "33": "2404.05970v1",
        "34": "2405.11461v1",
        "35": "1502.00804v2",
        "36": "2306.05212v1",
        "37": "2301.01820v4",
        "38": "2306.01061v1",
        "39": "2401.05761v1",
        "40": "2405.07767v1",
        "41": "2404.00211v1",
        "42": "2310.08750v2",
        "43": "2108.11044v2",
        "44": "1801.03844v1",
        "45": "2404.19543v1",
        "46": "2404.08137v2",
        "47": "2305.02156v1",
        "48": "2405.20680v3",
        "49": "2404.00245v1",
        "50": "2308.08285v1",
        "51": "2406.18740v1",
        "52": "2405.13177v1",
        "53": "2304.09542v2",
        "54": "2311.05876v2",
        "55": "2405.16546v2",
        "56": "2404.11973v1",
        "57": "2406.09979v2",
        "58": "2402.07770v1",
        "59": "2309.10621v1",
        "60": "2310.12443v1",
        "61": "2406.11678v1",
        "62": "2301.12652v4",
        "63": "2404.10981v1",
        "64": "2305.15294v2",
        "65": "2407.12854v1",
        "66": "2403.15246v1",
        "67": "2311.07204v1",
        "68": "1905.09217v1",
        "69": "2407.12325v1",
        "70": "2405.04727v1",
        "71": "2307.03027v1",
        "72": "2403.18093v1",
        "73": "2404.18185v1",
        "74": "2309.14323v1",
        "75": "2306.13421v1",
        "76": "2106.03373v4",
        "77": "2308.09308v3",
        "78": "2404.16924v1",
        "79": "2302.13498v1",
        "80": "2402.04853v1",
        "81": "2305.11700v1",
        "82": "2402.16874v1",
        "83": "2304.11406v3",
        "84": "1705.01509v1",
        "85": "2308.15027v1",
        "86": "1205.0312v1",
        "87": "2403.16915v3",
        "88": "2404.19705v2",
        "89": "2407.00128v1",
        "90": "2306.09938v1",
        "91": "2005.04588v2",
        "92": "2403.06447v1",
        "93": "2311.02089v1",
        "94": "2303.07678v2",
        "95": "2403.01999v1",
        "96": "2406.14764v1",
        "97": "2404.01012v1",
        "98": "1412.6629v3",
        "99": "2202.05144v1",
        "100": "2403.09142v1",
        "101": "2406.15657v1",
        "102": "2310.15511v1",
        "103": "2305.12152v2",
        "104": "2404.03302v1",
        "105": "2404.11791v1",
        "106": "2407.00936v2",
        "107": "2404.10496v2",
        "108": "2203.15364v1",
        "109": "2402.01176v2",
        "110": "2304.14732v7",
        "111": "2111.13853v3",
        "112": "2406.01197v2",
        "113": "2312.13264v1",
        "114": "2311.04348v1",
        "115": "2310.05149v1",
        "116": "2311.04694v1",
        "117": "2405.05600v1",
        "118": "2305.14283v3",
        "119": "1903.06902v3",
        "120": "2405.19262v1",
        "121": "2309.15088v1",
        "122": "1401.1732v1",
        "123": "2404.11457v1",
        "124": "2409.12740v1",
        "125": "2406.07299v1",
        "126": "2404.10939v1",
        "127": "2205.11194v2",
        "128": "2306.01599v1",
        "129": "2406.06739v1",
        "130": "2310.09350v1",
        "131": "2408.16967v1",
        "132": "2210.07093v1",
        "133": "2103.00956v1",
        "134": "2401.13870v1",
        "135": "2311.16720v2",
        "136": "2010.03073v1",
        "137": "2309.01157v2",
        "138": "2406.00247v2",
        "139": "2403.09747v1",
        "140": "2307.10169v1",
        "141": "1401.3896v1",
        "142": "2402.11129v1",
        "143": "2305.07614v2",
        "144": "2402.17505v1",
        "145": "1707.07700v1",
        "146": "2206.02873v5",
        "147": "2311.01343v4",
        "148": "1907.03693v1",
        "149": "2310.07554v2",
        "150": "2309.09261v1",
        "151": "2401.01566v1",
        "152": "2312.02969v1",
        "153": "2312.11036v1",
        "154": "2306.05817v5",
        "155": "2404.04925v1",
        "156": "2310.04027v2",
        "157": "2312.10091v1",
        "158": "2406.08891v1",
        "159": "1706.03266v1",
        "160": "2406.18134v1",
        "161": "2406.11681v1",
        "162": "2401.13222v2",
        "163": "1607.02641v1",
        "164": "2305.07622v3",
        "165": "2201.12431v2",
        "166": "2305.03653v1",
        "167": "2309.01431v2",
        "168": "2407.12982v1",
        "169": "2403.18276v2",
        "170": "2310.07815v1",
        "171": "1209.0126v1",
        "172": "2402.01763v2",
        "173": "1906.09404v2",
        "174": "1708.06011v1",
        "175": "2212.14206v1",
        "176": "2405.00465v3",
        "177": "2108.07081v1",
        "178": "2404.04163v1",
        "179": "2312.07182v1",
        "180": "2212.09271v2",
        "181": "2406.04113v1",
        "182": "2210.15859v1",
        "183": "2108.09346v1",
        "184": "2403.13291v1",
        "185": "2405.19893v1",
        "186": "2403.03187v1",
        "187": "2308.10633v2",
        "188": "2405.19612v2",
        "189": "2404.18746v1",
        "190": "2102.06815v2",
        "191": "2408.08066v2",
        "192": "2403.13325v1",
        "193": "2406.16367v1",
        "194": "2403.04160v1",
        "195": "2404.02616v1",
        "196": "2404.04044v2",
        "197": "2404.18443v1",
        "198": "2402.16968v1",
        "199": "1611.06792v3",
        "200": "2402.17010v1",
        "201": "2405.02659v2",
        "202": "2409.04600v1",
        "203": "2408.16312v2",
        "204": "2403.16435v1",
        "205": "2406.17465v1",
        "206": "2405.11971v1",
        "207": "2304.09649v1",
        "208": "2408.10613v1",
        "209": "2404.03192v1",
        "210": "2405.17428v1",
        "211": "2101.08751v1",
        "212": "2403.01616v2",
        "213": "1502.02277v1",
        "214": "2406.13050v1",
        "215": "2402.06196v2",
        "216": "2103.04831v4",
        "217": "2409.15133v1",
        "218": "2408.00878v1",
        "219": "2204.10628v1",
        "220": "2405.13622v1",
        "221": "2407.01627v1",
        "222": "2403.16378v1",
        "223": "2205.09707v1",
        "224": "2403.14403v2",
        "225": "2404.04522v2",
        "226": "1610.00735v1",
        "227": "2403.02760v2",
        "228": "2405.19670v3",
        "229": "2404.15939v2",
        "230": "2405.00175v1",
        "231": "2310.07521v3",
        "232": "2407.01437v2",
        "233": "2404.14851v1",
        "234": "2404.17283v1",
        "235": "2403.17688v1",
        "236": "2311.08593v1",
        "237": "1405.1740v1",
        "238": "2306.16004v1",
        "239": "2402.18150v1",
        "240": "2407.06992v2",
        "241": "2404.08940v1",
        "242": "2403.09599v1",
        "243": "2406.16383v2",
        "244": "2404.18424v2",
        "245": "2404.15939v3",
        "246": "2406.17519v1",
        "247": "2404.13556v1",
        "248": "2409.04833v1",
        "249": "2401.04155v1",
        "250": "2312.03494v1",
        "251": "2305.11161v1",
        "252": "2407.10652v1",
        "253": "2406.06519v1",
        "254": "2311.04329v2",
        "255": "2409.10516v2",
        "256": "2304.13010v2",
        "257": "2406.09008v1",
        "258": "2403.00807v1",
        "259": "2307.05782v2",
        "260": "2407.00072v4",
        "261": "2407.01449v2",
        "262": "2305.06569v6",
        "263": "2205.10569v1",
        "264": "2404.15790v1",
        "265": "2402.13542v1",
        "266": "2406.00697v2",
        "267": "2210.15718v1",
        "268": "2403.19181v1",
        "269": "2005.09207v2",
        "270": "2306.02867v1",
        "271": "2408.01363v1",
        "272": "2404.07221v1",
        "273": "2310.19488v1",
        "274": "2105.11108v3",
        "275": "2402.03182v1",
        "276": "2408.16296v1",
        "277": "2305.06566v4",
        "278": "2010.10137v3",
        "279": "2405.13576v1",
        "280": "2405.20646v1",
        "281": "2406.04638v1",
        "282": "2401.02575v1",
        "283": "2312.03863v3",
        "284": "2402.11827v1",
        "285": "2307.00457v2",
        "286": "2403.18405v1",
        "287": "2404.14851v3",
        "288": "2112.05662v2",
        "289": "2402.01339v1",
        "290": "2403.12173v1",
        "291": "2106.01186v1",
        "292": "2403.01432v2",
        "293": "2301.10444v1",
        "294": "2402.17944v2",
        "295": "2402.15276v3",
        "296": "2312.02429v2",
        "297": "2111.13057v3",
        "298": "2310.03025v2",
        "299": "2310.15205v2",
        "300": "2408.09017v1",
        "301": "2402.01364v2",
        "302": "2403.18684v1",
        "303": "2407.04573v1",
        "304": "2208.09847v1",
        "305": "2406.13121v1",
        "306": "2305.16243v3",
        "307": "2311.11691v1",
        "308": "2405.02732v1",
        "309": "2407.12849v1",
        "310": "2406.13249v1",
        "311": "1602.02410v2",
        "312": "2402.12663v1",
        "313": "2409.11136v1",
        "314": "2402.10548v1",
        "315": "2004.12832v2",
        "316": "2405.12819v1",
        "317": "2311.01555v1",
        "318": "2307.15780v3",
        "319": "1608.04465v1",
        "320": "2308.11131v4",
        "321": "2404.12879v1",
        "322": "2310.01329v1",
        "323": "2312.10997v5",
        "324": "2405.13008v1",
        "325": "2208.07652v1",
        "326": "1608.06656v1",
        "327": "2308.00415v1",
        "328": "1606.07869v1",
        "329": "2401.14021v1",
        "330": "2409.01980v1",
        "331": "2201.03356v1",
        "332": "2108.05652v1",
        "333": "1309.3421v6",
        "334": "2308.08434v2",
        "335": "2404.12237v2",
        "336": "2407.08223v1",
        "337": "2409.11860v1",
        "338": "2408.10729v1",
        "339": "2311.12955v1",
        "340": "2406.13331v1",
        "341": "2406.09618v1",
        "342": "2310.12321v1",
        "343": "2305.06812v1",
        "344": "2303.00807v3",
        "345": "2307.10188v1",
        "346": "2405.01116v1",
        "347": "2311.05800v2",
        "348": "2406.03085v1",
        "349": "2402.11794v1",
        "350": "2404.11343v1",
        "351": "2403.19302v1",
        "352": "2408.11119v2",
        "353": "1606.06991v1",
        "354": "2312.17617v1",
        "355": "2406.14162v1",
        "356": "2310.15950v4",
        "357": "2407.12883v1",
        "358": "2307.06435v9",
        "359": "2012.02287v1",
        "360": "2406.11706v1",
        "361": "2405.10098v1",
        "362": "2108.04026v1",
        "363": "2402.14334v1",
        "364": "2403.16248v2",
        "365": "2406.03411v2",
        "366": "2406.14745v2",
        "367": "2308.12241v1",
        "368": "2303.03229v2",
        "369": "2311.12399v4",
        "370": "2401.08206v1",
        "371": "2401.02993v1",
        "372": "2408.02907v1",
        "373": "2409.13385v1",
        "374": "2403.16504v1",
        "375": "2104.12016v1",
        "376": "2406.06458v1",
        "377": "2312.02724v1",
        "378": "2406.19292v1",
        "379": "2406.05733v1",
        "380": "2409.16497v1",
        "381": "2407.02486v1",
        "382": "2405.12656v1",
        "383": "2105.02274v2",
        "384": "2405.12119v1",
        "385": "2407.09417v2",
        "386": "2304.12562v2",
        "387": "2305.07477v1",
        "388": "2102.11903v2",
        "389": "2404.14294v1",
        "390": "2405.13007v1",
        "391": "2408.10151v1",
        "392": "2403.11366v2",
        "393": "2409.10909v1",
        "394": "1904.00289v1",
        "395": "2408.08564v1",
        "396": "2310.19056v3",
        "397": "2311.09513v1",
        "398": "2308.10837v1",
        "399": "2111.09852v3",
        "400": "2306.16680v1",
        "401": "2407.21065v1",
        "402": "2406.14169v1",
        "403": "2311.06318v2",
        "404": "2409.05401v1",
        "405": "2310.15556v2",
        "406": "2310.15777v2",
        "407": "2409.12941v1",
        "408": "2307.03172v3",
        "409": "2001.04484v1",
        "410": "2408.13533v1",
        "411": "2201.10582v1",
        "412": "2305.14499v2",
        "413": "2407.12391v1",
        "414": "2407.10805v3",
        "415": "1810.12936v1",
        "416": "2303.06573v2",
        "417": "2409.12959v1",
        "418": "2310.11761v1",
        "419": "2212.09146v3",
        "420": "2311.09615v2",
        "421": "2307.12966v1",
        "422": "2405.16933v1",
        "423": "2312.02443v1",
        "424": "2408.02854v3",
        "425": "2406.10307v1",
        "426": "2407.18990v2",
        "427": "2401.14887v3",
        "428": "2406.12169v1",
        "429": "2002.03932v1",
        "430": "2402.11757v1",
        "431": "2112.04426v3",
        "432": "2209.01335v2",
        "433": "2306.15766v1",
        "434": "2312.02783v2",
        "435": "2405.02048v1",
        "436": "2402.09543v1",
        "437": "2404.11960v1",
        "438": "2408.02223v2",
        "439": "2409.14924v1",
        "440": "2305.14871v2",
        "441": "2404.01037v1",
        "442": "2404.17288v1",
        "443": "1611.00196v1",
        "444": "2406.07136v1",
        "445": "2004.13005v1",
        "446": "2312.05417v1",
        "447": "2302.11266v2",
        "448": "2312.11518v2",
        "449": "2405.03972v1",
        "450": "2406.11201v2",
        "451": "2406.17262v1",
        "452": "2401.06954v2",
        "453": "2407.12036v1",
        "454": "2307.12798v3",
        "455": "2407.05563v1",
        "456": "2407.00085v1",
        "457": "2408.09437v1",
        "458": "2406.00231v1",
        "459": "1606.08689v1",
        "460": "2408.15399v1",
        "461": "2402.02764v1",
        "462": "2302.12128v1",
        "463": "2407.12508v1",
        "464": "2406.01285v1",
        "465": "2406.11830v1",
        "466": "2406.05085v1",
        "467": "2302.10150v1",
        "468": "2403.16427v4",
        "469": "2403.06551v1",
        "470": "2404.09022v1",
        "471": "2305.14002v1",
        "472": "2405.16089v2",
        "473": "2401.17377v3",
        "474": "2406.17261v1",
        "475": "2307.03109v9",
        "476": "2305.02320v1",
        "477": "2106.13618v1",
        "478": "2401.06676v1",
        "479": "2403.14932v2",
        "480": "2408.03130v1",
        "481": "2110.01529v2",
        "482": "2404.13781v1",
        "483": "2307.04601v1",
        "484": "2406.15187v1",
        "485": "2405.16363v2",
        "486": "2203.00537v1",
        "487": "2303.13419v1",
        "488": "2407.02464v1",
        "489": "2408.08545v1",
        "490": "2403.16345v1",
        "491": "2304.02020v1",
        "492": "2407.06718v1",
        "493": "2405.01122v1",
        "494": "2306.07377v1",
        "495": "2402.01733v1",
        "496": "2409.15364v1",
        "497": "2309.13063v2",
        "498": "2304.06762v3",
        "499": "2408.11903v2",
        "500": "2305.14987v2",
        "501": "1602.01665v1",
        "502": "2305.17116v2",
        "503": "2311.05020v2",
        "504": "2305.11841v1",
        "505": "2402.18041v1",
        "506": "1705.10513v2",
        "507": "2406.14449v1",
        "508": "2408.05524v1",
        "509": "2402.13446v1",
        "510": "2402.00891v1",
        "511": "1301.3781v3",
        "512": "2207.13443v2",
        "513": "2108.05540v1",
        "514": "2406.11745v1",
        "515": "2305.14625v1",
        "516": "2405.10596v2",
        "517": "1507.08586v3",
        "518": "2402.10409v1",
        "519": "2010.01195v1",
        "520": "2311.03057v1",
        "521": "2105.04651v1",
        "522": "2003.07820v2",
        "523": "2309.07606v1",
        "524": "2407.09394v1",
        "525": "2408.07611v2",
        "526": "2310.06491v1",
        "527": "1504.07295v3",
        "528": "2205.01230v1",
        "529": "2407.05502v2",
        "530": "2207.04656v1",
        "531": "2306.02250v2",
        "532": "2404.17897v1",
        "533": "2406.17378v1",
        "534": "2407.04069v1",
        "535": "2402.16877v1",
        "536": "2303.16854v2",
        "537": "2406.10450v2",
        "538": "2407.19813v2",
        "539": "2408.11381v2",
        "540": "2405.12540v1",
        "541": "2405.17382v1",
        "542": "2406.06729v1",
        "543": "2405.10311v1",
        "544": "2005.04356v1",
        "545": "2401.17043v2",
        "546": "2402.05128v2",
        "547": "2402.01801v2",
        "548": "2305.18494v1",
        "549": "2406.12331v1",
        "550": "2406.17419v1",
        "551": "2310.04407v1",
        "552": "2402.01065v1",
        "553": "2402.11035v2",
        "554": "2310.20081v1",
        "555": "2407.16833v1",
        "556": "2407.10701v1",
        "557": "2307.02046v5",
        "558": "2403.06465v1",
        "559": "2404.05086v1",
        "560": "2406.13213v2",
        "561": "2402.17081v1",
        "562": "2402.17762v1",
        "563": "2305.10998v2",
        "564": "2406.11424v1",
        "565": "2406.07573v1",
        "566": "2305.06311v2",
        "567": "2004.12297v2",
        "568": "2406.03963v1",
        "569": "2311.10723v1",
        "570": "2407.02694v1",
        "571": "2405.05445v1",
        "572": "2406.17305v1",
        "573": "2306.16092v1",
        "574": "2010.15036v1",
        "575": "2306.02561v3",
        "576": "2403.12499v1",
        "577": "2405.18272v1",
        "578": "2406.11289v1",
        "579": "2312.16159v1",
        "580": "2405.00824v1",
        "581": "2004.13969v3",
        "582": "2402.17497v1",
        "583": "2304.04487v1",
        "584": "2205.11245v3",
        "585": "2309.01868v1",
        "586": "2402.14318v1",
        "587": "2304.00612v1",
        "588": "2402.14301v2",
        "589": "2102.09206v3",
        "590": "2310.13028v1",
        "591": "2403.17759v1",
        "592": "1511.03729v2",
        "593": "2406.16264v2",
        "594": "2308.11474v1",
        "595": "2407.06685v1",
        "596": "2309.11838v1",
        "597": "2308.06111v2",
        "598": "2308.12039v1",
        "599": "2409.14083v1",
        "600": "2408.08821v1",
        "601": "2305.07001v1",
        "602": "2406.02368v1",
        "603": "1309.6865v1",
        "604": "2010.06467v3",
        "605": "2201.11086v1",
        "606": "2406.00944v1",
        "607": "2403.04666v1",
        "608": "2407.13906v1",
        "609": "2205.04275v2",
        "610": "2108.10127v1",
        "611": "2405.15130v1",
        "612": "2201.01745v1",
        "613": "1805.08159v2",
        "614": "2312.00678v2",
        "615": "1912.01901v4",
        "616": "2012.14005v1",
        "617": "2407.05441v1",
        "618": "2310.16270v1",
        "619": "2407.18940v1",
        "620": "2312.15746v1",
        "621": "2405.00975v1",
        "622": "2202.12191v1",
        "623": "2408.11775v1",
        "624": "2404.10327v1",
        "625": "2405.17093v2",
        "626": "2404.18797v1",
        "627": "1808.06528v1",
        "628": "2404.02805v1",
        "629": "2402.06853v1",
        "630": "2005.10049v1",
        "631": "2404.16789v1",
        "632": "2306.13549v2",
        "633": "2312.11361v2",
        "634": "2210.05145v1",
        "635": "2305.11462v1",
        "636": "2306.12756v1",
        "637": "2408.08901v1",
        "638": "1804.09661v1",
        "639": "2408.03297v2",
        "640": "2406.05183v1",
        "641": "1704.03940v3",
        "642": "2310.01558v1",
        "643": "2407.01953v1",
        "644": "2405.14589v1",
        "645": "2307.06213v1",
        "646": "2207.03834v1",
        "647": "1904.12683v2",
        "648": "2405.17915v1",
        "649": "2201.01614v2",
        "650": "2402.11060v1",
        "651": "2405.03989v2",
        "652": "2204.04179v2",
        "653": "2405.15784v1",
        "654": "2312.04528v1",
        "655": "2402.13492v3",
        "656": "2404.16789v2",
        "657": "2204.11989v1",
        "658": "2409.03759v1",
        "659": "2405.01117v1",
        "660": "2309.10435v4",
        "661": "2407.11638v1",
        "662": "2009.04016v1",
        "663": "2307.11088v3",
        "664": "2408.01319v1",
        "665": "2210.15133v1",
        "666": "2408.09698v2",
        "667": "2406.19251v1",
        "668": "2406.10251v3",
        "669": "2304.01852v4",
        "670": "2307.10442v1",
        "671": "2405.17383v1",
        "672": "2310.07343v1",
        "673": "1909.01772v1",
        "674": "1507.08396v1",
        "675": "2401.15391v1",
        "676": "1606.04223v1",
        "677": "2405.10739v2",
        "678": "2012.11685v2",
        "679": "2310.19736v3",
        "680": "2407.16216v1",
        "681": "2401.05215v1",
        "682": "2407.12101v1",
        "683": "2407.01158v1",
        "684": "2407.19669v1",
        "685": "2405.10166v1",
        "686": "2204.02922v1",
        "687": "2311.12338v1",
        "688": "2403.16584v1",
        "689": "2007.11088v1",
        "690": "2110.00159v1",
        "691": "1609.00969v1",
        "692": "2405.04674v1",
        "693": "2006.15408v1",
        "694": "1809.05190v1",
        "695": "2011.00696v2",
        "696": "2403.19216v1",
        "697": "2408.13450v1",
        "698": "2404.07981v1",
        "699": "2101.11873v2",
        "700": "1908.07690v1",
        "701": "2308.06507v1",
        "702": "2404.19360v1",
        "703": "2409.05591v2",
        "704": "2406.14739v1",
        "705": "2404.08679v1",
        "706": "2107.05383v1",
        "707": "2406.09043v2",
        "708": "2406.04202v1",
        "709": "2005.00181v3",
        "710": "1602.02332v1",
        "711": "1605.09362v3",
        "712": "2202.06991v3",
        "713": "2311.00423v6",
        "714": "2407.15248v1",
        "715": "2306.15222v2",
        "716": "2408.02545v1",
        "717": "2406.14171v1",
        "718": "2402.10612v1",
        "719": "2404.15777v4",
        "720": "2007.10296v1",
        "721": "2106.11251v2",
        "722": "2408.00555v1",
        "723": "2311.16673v1",
        "724": "2403.06840v1",
        "725": "2403.06872v1",
        "726": "2406.09459v1",
        "727": "2408.09199v1",
        "728": "2409.17011v1",
        "729": "2409.06185v1",
        "730": "2405.20978v1",
        "731": "2402.08859v1",
        "732": "1808.10143v2",
        "733": "2403.01744v2",
        "734": "2401.09350v1",
        "735": "2408.14317v1",
        "736": "1205.5569v3",
        "737": "2407.13193v2",
        "738": "2409.16605v1",
        "739": "1906.02329v1",
        "740": "2307.06713v3",
        "741": "2004.10035v1",
        "742": "2406.11200v2",
        "743": "2401.07367v1",
        "744": "1704.01617v1",
        "745": "2409.05994v1",
        "746": "2408.13253v1",
        "747": "2408.06643v2",
        "748": "2406.03210v1",
        "749": "2406.14887v1",
        "750": "2112.06400v2",
        "751": "2406.16167v1",
        "752": "2306.02864v2",
        "753": "2310.12455v2",
        "754": "2409.03752v2",
        "755": "2310.09291v2",
        "756": "2409.16974v1",
        "757": "1610.08136v1",
        "758": "2304.14522v1",
        "759": "2308.10792v5",
        "760": "2311.13165v1",
        "761": "2408.04867v1",
        "762": "2405.16127v2",
        "763": "2402.01722v1",
        "764": "2408.02152v1",
        "765": "2111.09927v1",
        "766": "2406.09621v1",
        "767": "2311.03839v3",
        "768": "2405.04065v3",
        "769": "2404.07060v1",
        "770": "2007.05186v3",
        "771": "2402.06216v2",
        "772": "2107.13602v1",
        "773": "1706.08746v2",
        "774": "1709.07777v2",
        "775": "2403.18105v2",
        "776": "2409.13902v1",
        "777": "2408.01875v2",
        "778": "2307.15020v1",
        "779": "2304.04309v1",
        "780": "1910.13339v2",
        "781": "2408.08696v1",
        "782": "2204.11447v2",
        "783": "2403.06642v1",
        "784": "2205.13351v1",
        "785": "2106.14807v1",
        "786": "2404.01616v2",
        "787": "2310.05380v1",
        "788": "1803.04494v1",
        "789": "2408.00357v1",
        "790": "2406.15319v3",
        "791": "2307.11019v2",
        "792": "2308.13207v1",
        "793": "2308.08378v1",
        "794": "2407.13218v3",
        "795": "2409.12558v1",
        "796": "2009.09392v1",
        "797": "1806.03790v1",
        "798": "2405.04760v3",
        "799": "2405.17890v1",
        "800": "2309.14402v1",
        "801": "2402.06334v1",
        "802": "2405.10825v2",
        "803": "2403.14469v1",
        "804": "1803.08240v1",
        "805": "2407.21300v3",
        "806": "2305.15005v1",
        "807": "2310.04205v2",
        "808": "2402.07470v2",
        "809": "1701.07795v1",
        "810": "2406.18382v2",
        "811": "2010.10469v1",
        "812": "2302.05578v2",
        "813": "2407.07487v1",
        "814": "1805.00152v1",
        "815": "2401.11506v1",
        "816": "2303.09136v1",
        "817": "2309.11805v1",
        "818": "2304.13712v2",
        "819": "2408.17072v1",
        "820": "2310.13682v2",
        "821": "2309.15789v1",
        "822": "2408.08894v1",
        "823": "2308.10053v1",
        "824": "2402.02713v1",
        "825": "2311.04913v2",
        "826": "2403.18969v1",
        "827": "2311.07870v2",
        "828": "2309.11325v2",
        "829": "2102.11345v1",
        "830": "2405.16640v2",
        "831": "2311.01307v1",
        "832": "2307.08303v4",
        "833": "2406.10833v2",
        "834": "2405.03963v3",
        "835": "2402.12146v1",
        "836": "2310.17488v2",
        "837": "2407.21330v1",
        "838": "2009.05121v1",
        "839": "1804.06439v3",
        "840": "2210.09179v1",
        "841": "2305.17331v1",
        "842": "2408.10808v1",
        "843": "2406.04165v1",
        "844": "2310.18347v1",
        "845": "2303.16145v1",
        "846": "2304.08912v1",
        "847": "2405.07468v1",
        "848": "2404.07499v1",
        "849": "1909.04985v1",
        "850": "2304.03679v1",
        "851": "2305.05973v2",
        "852": "1911.09661v1",
        "853": "2310.05627v1",
        "854": "2311.07619v2",
        "855": "2403.18218v1",
        "856": "2404.00282v1",
        "857": "2409.00369v3",
        "858": "1711.08611v1",
        "859": "2406.10471v1",
        "860": "2408.12025v1",
        "861": "2405.14431v1",
        "862": "2408.11981v1",
        "863": "2310.05657v1",
        "864": "2310.12303v1",
        "865": "2407.05786v1",
        "866": "2309.09727v1",
        "867": "2408.08921v2",
        "868": "2310.04678v3",
        "869": "2404.10779v1",
        "870": "2407.12341v1",
        "871": "2209.00218v2",
        "872": "2402.13823v2",
        "873": "2311.13126v1",
        "874": "2405.07828v1",
        "875": "2405.13055v1",
        "876": "2409.15763v1",
        "877": "2312.12728v2",
        "878": "2001.04980v1",
        "879": "2406.10291v1",
        "880": "2310.14025v1",
        "881": "2407.19829v1",
        "882": "2409.02141v1",
        "883": "1611.03305v1",
        "884": "2405.05008v1",
        "885": "2405.13792v1",
        "886": "2205.03284v2",
        "887": "2408.08444v1",
        "888": "2401.14777v1",
        "889": "2401.06775v1",
        "890": "2405.16178v1",
        "891": "2310.01427v1",
        "892": "2403.17998v1",
        "893": "1907.05340v1",
        "894": "2406.13342v1",
        "895": "2401.03426v1",
        "896": "2409.13699v1",
        "897": "2401.13201v2",
        "898": "1910.00896v1",
        "899": "2303.07304v1",
        "900": "2402.05121v1",
        "901": "2409.07691v1",
        "902": "2402.14836v1",
        "903": "2403.18771v1",
        "904": "2406.05013v1",
        "905": "2409.11673v1",
        "906": "2405.15165v1",
        "907": "2103.09306v1",
        "908": "2404.13207v1",
        "909": "2209.14494v2",
        "910": "2308.12574v2",
        "911": "2406.06584v1",
        "912": "2007.01510v1",
        "913": "2103.07901v2",
        "914": "2311.06838v1",
        "915": "2406.13138v1",
        "916": "2304.01019v1",
        "917": "2408.14906v1",
        "918": "2408.10343v1",
        "919": "2304.11370v1",
        "920": "1904.09171v2",
        "921": "2405.08151v2",
        "922": "2305.15334v1",
        "923": "2406.15045v2",
        "924": "2402.10866v1",
        "925": "2405.10616v1",
        "926": "1806.09447v2",
        "927": "2407.00402v2",
        "928": "2401.10184v1",
        "929": "2312.12009v1",
        "930": "2305.04344v1",
        "931": "2312.07552v1",
        "932": "2303.11504v2",
        "933": "2406.19309v2",
        "934": "2405.13021v1",
        "935": "1401.2258v1",
        "936": "2402.15818v1",
        "937": "2311.13565v1",
        "938": "2309.13638v1",
        "939": "2406.07348v3",
        "940": "2407.19679v1",
        "941": "2308.11891v2",
        "942": "2302.06587v2",
        "943": "1704.08803v2",
        "944": "2109.10086v1",
        "945": "2406.14848v1",
        "946": "2402.04357v1",
        "947": "2312.15599v1",
        "948": "1708.02702v4",
        "949": "2401.00625v2",
        "950": "2407.16192v1",
        "951": "2308.11512v1",
        "952": "2409.01605v1",
        "953": "2309.15025v1",
        "954": "2403.04256v1",
        "955": "2310.14408v1",
        "956": "2408.03354v2",
        "957": "1804.05936v2",
        "958": "1605.07422v3",
        "959": "1608.06651v2",
        "960": "2207.02578v2",
        "961": "2408.03533v2",
        "962": "2408.09439v1",
        "963": "2309.03613v1",
        "964": "2406.11357v2",
        "965": "2305.15673v1",
        "966": "2312.00909v1",
        "967": "2406.12593v1",
        "968": "2401.13601v4",
        "969": "2310.05092v1",
        "970": "2405.16420v1",
        "971": "1811.03514v1",
        "972": "2406.03712v1",
        "973": "1910.04732v2",
        "974": "2306.14924v1",
        "975": "2406.14282v1",
        "976": "2406.11230v1",
        "977": "2407.08275v1",
        "978": "2408.04211v1",
        "979": "2007.10434v1",
        "980": "1811.00606v3",
        "981": "2401.06320v2",
        "982": "2405.12063v2",
        "983": "2307.00524v1",
        "984": "2401.02333v3",
        "985": "2402.07950v1",
        "986": "2002.06275v1",
        "987": "2407.01102v1",
        "988": "2408.08896v1",
        "989": "2407.11963v1",
        "990": "1812.05731v3",
        "991": "1912.13080v1",
        "992": "2209.06583v1",
        "993": "2303.10126v3",
        "994": "2302.03765v1",
        "995": "2407.16896v1",
        "996": "2407.02351v1",
        "997": "2205.12105v2",
        "998": "2406.07505v1",
        "999": "2409.08523v2",
        "1000": "2407.12872v1"
    }
}