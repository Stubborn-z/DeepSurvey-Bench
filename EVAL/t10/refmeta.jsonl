{"paper_id": 5450801, "title": "Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots", "author_names": ["Yu Wu", "Wei Yu Wu", "Ming Zhou", "Zhoujun Li"], "venue": "arXiv.org", "abstract": "We study response selection for multi-turn conversation in retrieval based chatbots. Existing works either ignores relationships among utterances, or misses important information in context when matching a response with a highly abstract context vector finally. We propose a new session based matching model to address both problems. The model first matches a response with each utterance on multiple granularities, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models the relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that our model can significantly outperform the state-of-the-art methods for response selection in multi-turn conversation.", "year": 2016, "publicationdate": "2016-12-06", "externalids": {}, "doi_lower": null}
{"paper_id": 4325193, "title": "From Eliza to XiaoIce: challenges and opportunities with social chatbots", "author_names": ["H. Shum", "Xiaodong He", "Di Li"], "venue": "Frontiers of Information Technology & Electronic Engineering", "abstract": "Conversational systems have come a long way since their inception in the 1960s. After decades of research and development, we have seen progress from Eliza and Parry in the 1960s and 1970s, to task-completion systems as in the Defense Advanced Research Projects Agency (DARPA) communicator program in the 2000s, to intelligent personal assistants such as Siri, in the 2010s, to today’s social chatbots like XiaoIce. Social chatbots’ appeal lies not only in their ability to respond to users’ diverse requests, but also in being able to establish an emotional connection with users. The latter is done by satisfying users’ need for communication, affection, as well as social belonging. To further the advancement and adoption of social chatbots, their design must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Users should want to engage with a social chatbot; as such, we define the success metric for social chatbots as conversation-turns per session (CPS). Using XiaoIce as an illustrative example, we discuss key technologies in building social chatbots from core chat to visual awareness to skills. We also show how XiaoIce can dynamically recognize emotion and engage the user throughout long conversations with appropriate interpersonal responses. As we become the first generation of humans ever living with artificial intelligenc (AI), we have a responsibility to design social chatbots to be both useful and empathetic, so they will become ubiquitous and help society as a whole.", "year": 2018, "publicationdate": "2018-01-01", "externalids": {"DOI": "10.1631/FITEE.1700826"}, "doi_lower": "10.1631/fitee.1700826"}
{"paper_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "author_names": ["Vladimir Karpukhin", "Barlas Oğuz", "Sewon Min", "Patrick Lewis", "Ledell Yu Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.", "year": 2020, "publicationdate": "2020-04-10", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.550"}, "doi_lower": "10.18653/v1/2020.emnlp-main.550"}
{"paper_id": 7060187, "title": "Image retrieval: Ideas, influences, and trends of the new age", "author_names": ["R. Datta", "Dhiraj Joshi", "Jia Li", "James Ze Wang"], "venue": "CSUR", "abstract": null, "year": 2008, "publicationdate": "2008-04-01", "externalids": {"DOI": "10.1145/1348246.1348248"}, "doi_lower": "10.1145/1348246.1348248"}
{"paper_id": 202776649, "title": "Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots", "author_names": ["Chunyuan Yuan", "W. Zhou", "Mingming Li", "Shangwen Lv", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Multi-turn retrieval-based conversation is an important task for building intelligent dialogue systems. Existing works mainly focus on matching candidate responses with every context utterance on multiple levels of granularity, which ignore the side effect of using excessive context information. Context utterances provide abundant information for extracting more matching features, but it also brings noise signals and unnecessary information. In this paper, we will analyze the side effect of using too many context utterances and propose a multi-hop selector network (MSN) to alleviate the problem. Specifically, MSN firstly utilizes a multi-hop selector to select the relevant utterances as context. Then, the model matches the filtered context with the candidate response and obtains a matching score. Experimental results show that MSN outperforms some state-of-the-art methods on three public multi-turn dialogue datasets.", "year": 2019, "publicationdate": "2019-11-01", "externalids": {"DOI": "10.18653/v1/D19-1011"}, "doi_lower": "10.18653/v1/d19-1011"}
{"paper_id": 231662221, "title": "Content Selection Network for Document-grounded Retrieval-based Chatbots", "author_names": ["Yutao Zhu", "J. Nie", "Kun Zhou", "Pan Du", "Zhicheng Dou"], "venue": "European Conference on Information Retrieval", "abstract": "Grounding human-machine conversation in a document is an effective way to improve the performance of retrieval-based chatbots. However, only a part of the document content may be relevant to help select the appropriate response at a round. It is thus crucial to select the part of document content relevant to the current conversation context. In this paper, we propose a document content selection network (CSN) to perform explicit selection of relevant document contents, and filter out the irrelevant parts. We show in experiments on two public document-grounded conversation datasets that CSN can effectively help select the relevant document contents to the conversation context, and it produces better results than the state-of-the-art approaches. Our code and datasets are available at https://github.com/DaoD/CSN.", "year": 2021, "publicationdate": "2021-01-21", "externalids": {"DOI": "10.1007/978-3-030-72113-8_50"}, "doi_lower": "10.1007/978-3-030-72113-8_50"}
{"paper_id": 235792322, "title": "Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals", "author_names": ["Yutao Zhu", "J. Nie", "Kun Zhou", "Pan Du", "Hao Jiang", "Zhicheng Dou"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "A proactive dialogue system has the ability to proactively lead the conversation. Different from the general chatbots which only react to the user, proactive dialogue systems can be used to achieve some goals, e.g., to recommend some items to the user. Background knowledge is essential to enable smooth and natural transitions in dialogue. In this paper, we propose a new multi-task learning framework for retrieval-based knowledge-grounded proactive dialogue. To determine the relevant knowledge to be used, we frame knowledge prediction as a complementary task and use explicit signals to supervise its learning. The final response is selected according to the predicted knowledge, the goal to achieve, and the context. Experimental results show that explicit modeling of knowledge prediction and goal selection can greatly improve the final response selection. Our code is available at https://github.com/DaoD/KPN/.", "year": 2021, "publicationdate": "2021-07-11", "externalids": {"DOI": "10.1145/3404835.3463011"}, "doi_lower": "10.1145/3404835.3463011"}
{"paper_id": 237194722, "title": "Learning Implicit User Profile for Personalized Retrieval-Based Chatbot", "author_names": ["Hongjin Qian", "Zhicheng Dou", "Yutao Zhu", "Yueyuan Ma", "Ji-rong Wen"], "venue": "International Conference on Information and Knowledge Management", "abstract": "In this paper, we explore the problem of developing personalized chatbots. A personalized chatbot is designed as a digital chatting assistant for a user. The key characteristic of a personalized chatbot is that it should have a consistent personality with the corresponding user. It can talk the same way as the user when it is delegated to respond to others' messages. Many methods have been proposed to assign a personality to dialogue chatbots, but most of them utilize explicit user profiles, including several persona descriptions or key-value-based personal information. In a practical scenario, however, users might be reluctant to write detailed persona descriptions, and obtaining a large number of explicit user profiles requires tremendous manual labour. To tackle the problem, we present a retrieval-based personalized chatbot model, namely IMPChat, to learn an implicit user profile from the user's dialogue history. We argue that the implicit user profile is superior to the explicit user profile regarding accessibility and flexibility. IMPChat aims to learn an implicit user profile through modeling user's personalized language style and personalized preferences separately. To learn a user's personalized language style, we elaborately build language models from shallow to deep using the user's historical responses; To model a user's personalized preferences, we explore the conditional relations underneath each post-response pair of the user. The personalized preferences are dynamic and context-aware: we assign higher weights to those historical pairs that are topically related to the current query when aggregating the personalized preferences. We match each response candidate with the personalized language style and personalized preference, respectively, and fuse the two matching signals to determine the final ranking score. We conduct comprehensive experiments on two large datasets, and the results show that our method outperforms all baseline models.", "year": 2021, "publicationdate": "2021-08-18", "externalids": {"DOI": "10.1145/3459637.3482269"}, "doi_lower": "10.1145/3459637.3482269"}
{"paper_id": 231815627, "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "author_names": ["Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.", "year": 2020, "publicationdate": "2020-10-16", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.466"}, "doi_lower": "10.18653/v1/2021.naacl-main.466"}
{"paper_id": 7343609, "title": "Analyzing and evaluating query reformulation strategies in web search logs", "author_names": ["Jeff Huang", "E. Efthimiadis"], "venue": "International Conference on Information and Knowledge Management", "abstract": null, "year": 2009, "publicationdate": "2009-11-02", "externalids": {"DOI": "10.1145/1645953.1645966"}, "doi_lower": "10.1145/1645953.1645966"}
{"paper_id": 220730235, "title": "An Analysis of BERT in Document Ranking", "author_names": ["Jingtao Zhan", "Jiaxin Mao", "Yiqun Liu", "Min Zhang", "Shaoping Ma"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Although BERT has shown its effectiveness in a number of IR-related tasks, especially document ranking, the understanding of its internal mechanism remains insufficient. To increase the explainability of the ranking process performed by BERT, we investigate a state-of-the-art BERT-based ranking model with focus on its attention mechanism and interaction behavior. Firstly, we look into the evolving of the attention distribution. It shows that in each step, BERT dumps redundant attention weights on tokens with high document frequency (such as periods). This may lead to a potential threat to the model robustness and should be considered in future studies. Secondly, we study how BERT models interactions between query and document and find that BERT aggregates document information to query token representations through their interactions, but extracts query-independent representations for document tokens. It indicates that it is possible to transform BERT into a more efficient representation-focused model. These findings help us better understand the ranking process by BERT and may inspire future improvement.", "year": 2020, "publicationdate": "2020-07-25", "externalids": {"DOI": "10.1145/3397271.3401325"}, "doi_lower": "10.1145/3397271.3401325"}
{"paper_id": 237277941, "title": "Contrastive Learning of User Behavior Sequence for Context-Aware Document Ranking", "author_names": ["Yutao Zhu", "J. Nie", "Zhicheng Dou", "Zhengyi Ma", "Xinyu Zhang", "Pan Du", "Xiaochen Zuo", "Hao Jiang"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Context information in search sessions has proven to be useful for capturing user search intent. Existing studies explored user behavior sequences in sessions in different ways to enhance query suggestion or document ranking. However, a user behavior sequence has often been viewed as a definite and exact signal reflecting a user's behavior. In reality, it is highly variable: user's queries for the same intent can vary, and different documents can be clicked. To learn a more robust representation of the user behavior sequence, we propose a method based on contrastive learning, which takes into account the possible variations in user's behavior sequences. Specifically, we propose three data augmentation strategies to generate similar variants of user behavior sequences and contrast them with other sequences. In so doing, the model is forced to be more robust regarding the possible variations. The optimized sequence representation is incorporated into document ranking. Experiments on two real query log datasets show that our proposed model outperforms the state-of-the-art methods significantly, which demonstrates the effectiveness of our method for context-aware document ranking.", "year": 2021, "publicationdate": "2021-08-24", "externalids": {"DOI": "10.1145/3459637.3482243"}, "doi_lower": "10.1145/3459637.3482243"}
{"paper_id": 47066000, "title": "Personalizing Search via Automated Analysis of Interests and Activities", "author_names": ["J. Teevan", "S. Dumais", "E. Horvitz"], "venue": "SIGIR Forum", "abstract": null, "year": 2018, "publicationdate": "2018-02-22", "externalids": {"DOI": "10.1145/3190580.3190582"}, "doi_lower": "10.1145/3190580.3190582"}
{"paper_id": 1292249, "title": "Modeling the impact of short- and long-term behavior on search personalization", "author_names": ["Paul N. Bennett", "Ryen W. White", "Wei Chu", "S. Dumais", "P. Bailey", "Fedor Borisyuk", "Xiaoyuan Cui"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": null, "year": 2012, "publicationdate": "2012-08-12", "externalids": {"DOI": "10.1145/2348283.2348312"}, "doi_lower": "10.1145/2348283.2348312"}
{"paper_id": 53034987, "title": "Personalizing Search Results Using Hierarchical RNN with Query-aware Attention", "author_names": ["Songwei Ge", "Zhicheng Dou", "Zhengbao Jiang", "Jian-Yun Nie", "Ji-Rong Wen"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Search results personalization has become an effective way to improve the quality of search engines. Previous studies extracted information such as past clicks, user topical interests, query click entropy and so on to tailor the original ranking. However, few studies have taken into account the sequential information underlying previous queries and sessions. Intuitively, the order of issued queries is important in inferring the real user interests. And more recent sessions should provide more reliable personal signals than older sessions. In addition, the previous search history and user behaviors should influence the personalization of the current query depending on their relatedness. To implement these intuitions, in this paper we employ a hierarchical recurrent neural network to exploit such sequential information and automatically generate user profile from historical data. We propose a query-aware attention model to generate a dynamic user profile based on the input query. Significant improvement is observed in the experiment with data from a commercial search engine when compared with several traditional personalization models. Our analysis reveals that the attention model is able to attribute higher weights to more related past sessions after fine training.", "year": 2018, "publicationdate": "2018-10-17", "externalids": {"DOI": "10.1145/3269206.3271728"}, "doi_lower": "10.1145/3269206.3271728"}
{"paper_id": 240230721, "title": "PSSL: Self-supervised Learning for Personalized Search with Contrastive Sampling", "author_names": ["Yujia Zhou", "Zhicheng Dou", "Yutao Zhu", "Ji-rong Wen"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Personalized search plays a crucial role in improving user search experience owing to its ability to build user profiles based on historical behaviors. Previous studies have made great progress in extracting personal signals from the query log and learning user representations. However, neural personalized search is extremely dependent on sufficient data to train the user model. Data sparsity is an inevitable challenge for existing methods to learn high-quality user representations. Moreover, the overemphasis on final ranking quality leads to rough data representations and impairs the generalizability of the model. To tackle these issues, we propose a Personalized Search framework with Self-supervised Learning (PSSL) to enhance data representations. Specifically, we adopt a contrastive sampling method to extract paired self-supervised information from sequences of user behaviors in query logs. Four auxiliary tasks are designed to pre-train the sentence encoder and the sequence encoder used in the ranking model. They are optimized by contrastive loss which aims to close the distance between similar user sequences, queries, and documents. Experimental results on two datasets demonstrate that our proposed model PSSL achieves state-of-the-art performance compared with existing baselines.", "year": 2021, "publicationdate": "2021-10-26", "externalids": {"DOI": "10.1145/3459637.3482379"}, "doi_lower": "10.1145/3459637.3482379"}
{"paper_id": 6334682, "title": "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries", "author_names": ["J. Carbonell", "Jade Goldstein-Stewart"], "venue": "SIGIR Forum", "abstract": null, "year": 1998, "publicationdate": "1998-08-01", "externalids": {"DOI": "10.1145/3130348.3130369"}, "doi_lower": "10.1145/3130348.3130369"}
{"paper_id": 17530455, "title": "Diversifying search results", "author_names": ["R. Agrawal", "Sreenivas Gollapudi", "A. Halverson", "Samuel Ieong"], "venue": "Web Search and Data Mining", "abstract": null, "year": 2009, "publicationdate": "2009-02-09", "externalids": {"DOI": "10.1145/1498759.1498766"}, "doi_lower": "10.1145/1498759.1498766"}
{"paper_id": 220730230, "title": "DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features", "author_names": ["Jiongnan Liu", "Zhicheng Dou", "Xiaojie Wang", "Shuqi Lu", "Ji-rong Wen"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Search result diversification aims to retrieve diverse results to cover as many subtopics related to the query as possible. Recent studies showed that supervised diversification models are able to outperform the heuristic approaches, by automatically learning a diversification function other than using manually designed score functions. The main challenge of training a diversification model is the lack of high-quality training samples. Due to the involvement of dependence between documents in the ranker, it is very hard for training algorithms to select effective positive and negative ranking lists to train a reliable ranking model, given a large number of candidate documents within which different documents are relevant to different subtopics. To tackle this problem, we propose a supervised diversification framework based on Generative Adversarial Network (GAN). It consists of a generator and a discriminator interacting with each other in a minimax game. Specifically, the generator generates more confusing negative samples for the discriminator, and the discriminator sends back complementary ranking signals to the generator. Furthermore, we explicitly exploit subtopics in the generator, whereas focusing on modeling document similarity in the discriminator. Through such a minimax game, we are able to obtain better ranking models by combining ranking signals learned by the generator and the discriminator. Experimental results on the TREC Web Track dataset show that the proposed method can significantly outperform existing diversification methods.", "year": 2020, "publicationdate": "2020-07-25", "externalids": {"DOI": "10.1145/3397271.3401084"}, "doi_lower": "10.1145/3397271.3401084"}
{"paper_id": 235792531, "title": "Modeling Intent Graph for Search Result Diversification", "author_names": ["Zhan Su", "Zhicheng Dou", "Yutao Zhu", "Xubo Qin", "Ji-rong Wen"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Search result diversification aims to offer diverse documents that cover as many intents as possible. Most existing implicit diversification approaches model diversity through the similarity of document representation, which is indirect and unnatural. To handle the diversity more precisely, we measure the similarity of documents by their similarity of the intent coverage. Specifically, we build a classifier to judge whether two different documents contain the same intent based on the document's content. Then we construct an intent graph to present the complicated relationship of documents and the query. On the intent graph, documents are connected if they are similar, while the query and the document are gradually connected based on the document selection result. Then we employ graph convolutional networks (GCNs) to update the representation of the query and each document by aggregating its neighbors. By this means, we can obtain the context-aware query representation and the intent-aware document representations through the dynamic intent graph during the document selection process. Furthermore, these representations and intent graph features are fused into diversity features. Combined with the traditional relevance features, we obtain the final ranking score that balances the relevance and the diversity. Experimental results show that this implicit diversification model significantly outperforms all existing implicit diversification methods, and it can even beat the state-of-the-art explicit models.", "year": 2021, "publicationdate": "2021-07-11", "externalids": {"DOI": "10.1145/3404835.3462872"}, "doi_lower": "10.1145/3404835.3462872"}
{"paper_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "author_names": ["Sebastian Borgeaud", "A. Mensch", "Jordan Hoffmann", "Trevor Cai", "Eliza Rutherford", "Katie Millican", "George van den Driessche", "Jean-Baptiste Lespiau", "Bogdan Damoc", "Aidan Clark", "Diego de Las Casas", "Aurelia Guy", "Jacob Menick", "Roman Ring", "T. Hennigan", "Saffron Huang", "Lorenzo Maggiore", "Chris Jones", "Albin Cassirer", "Andy Brock", "Michela Paganini", "G. Irving", "O. Vinyals", "Simon Osindero", "K. Simonyan", "Jack W. Rae", "Erich Elsen", "L. Sifre"], "venue": "International Conference on Machine Learning", "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.", "year": 2021, "publicationdate": "2021-12-08", "externalids": {}, "doi_lower": null}
{"paper_id": 245329531, "title": "WebGPT: Browser-assisted question-answering with human feedback", "author_names": ["Reiichiro Nakano", "Jacob Hilton", "S. Balaji", "Jeff Wu", "Ouyang Long", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "W. Saunders", "Xu Jiang", "K. Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "venue": "arXiv.org", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "year": 2021, "publicationdate": "2021-12-17", "externalids": {}, "doi_lower": null}
{"paper_id": 6473756, "title": "A vector space model for automatic indexing", "author_names": ["G. Salton", "A. Wong", "Chung-Shu Yang"], "venue": "CACM", "abstract": null, "year": 1975, "publicationdate": "1975-11-01", "externalids": {"DOI": "10.1145/361219.361220"}, "doi_lower": "10.1145/361219.361220"}
{"paper_id": 8264008, "title": "A general language model for information retrieval", "author_names": ["Fei Song", "W. Bruce Croft"], "venue": "International Conference on Information and Knowledge Management", "abstract": null, "year": 1999, "publicationdate": "1999-11-01", "externalids": {"DOI": "10.1145/319950.320022"}, "doi_lower": "10.1145/319950.320022"}
{"paper_id": 10334230, "title": "Delta TFIDF: An Improved Feature Space for Sentiment Analysis", "author_names": ["Justin Martineau", "Timothy W. Finin"], "venue": "International Conference on Web and Social Media", "abstract": "Mining opinions and sentiment from social networking sites is a popular application for social media systems. Common approaches use a machine learning system with a bag of words feature set. We present Delta TFIDF, an intuitive general purpose technique to efficiently weight word scores before classification. Delta TFIDF is easy to compute, implement, and understand. We use Support Vector Machines to show that Delta TFIDF significantly improves accuracy for sentiment analysis problems using three well known data sets.", "year": 2009, "publicationdate": "2009-03-20", "externalids": {"DOI": "10.1609/icwsm.v3i1.13979"}, "doi_lower": "10.1609/icwsm.v3i1.13979"}
{"paper_id": 41563977, "title": "Okapi at TREC-3", "author_names": ["S. Robertson", "S. Walker", "S. Jones", "Micheline Hancock-Beaulieu", "M. Gatford"], "venue": "Text Retrieval Conference", "abstract": null, "year": 1994, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 5688521, "title": "A Deep Relevance Matching Model for Ad-hoc Retrieval", "author_names": ["J. Guo", "Yixing Fan", "Qingyao Ai", "W. Bruce Croft"], "venue": "International Conference on Information and Knowledge Management", "abstract": "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.", "year": 2016, "publicationdate": "2016-10-24", "externalids": {"DOI": "10.1145/2983323.2983769"}, "doi_lower": "10.1145/2983323.2983769"}
{"paper_id": 220302524, "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval", "author_names": ["Lee Xiong", "Chenyan Xiong", "Ye Li", "Kwok-Fung Tang", "Jialin Liu", "Paul N. Bennett", "Junaid Ahmed", "Arnold Overwijk"], "venue": "International Conference on Learning Representations", "abstract": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.", "year": 2020, "publicationdate": "2020-07-01", "externalids": {}, "doi_lower": null}
{"paper_id": 253441220, "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "author_names": ["Suzan Verberne"], "venue": "International Conference on Computational Logic", "abstract": "Text ranking takes a central place in Information Retrieval (IR), with Web search as its best-known application. More generally, text ranking models are applicable to any Natural Language Processing (NLP) task in which relevance of information plays a role, from filtering and recommendation applications to question answering and semantic similarity comparisons. Since the rise of BERT in 2019, Transformer models have become the most used and studied architectures in both NLP and IR, and they have been applied to basically any task in our research fields—including text ranking. In a fast-changing research context, it can be challenging to keep lecture materials up to date. Lecturers in NLP are grateful for Dan Jurafsky and James Martin for yearly updating the 3rd edition of their textbook, making Speech and Language Processing the most comprehensive, modern textbook for NLP. The IR field is less fortunate, still relying on older textbooks, extended with a collection of recent materials that address neural models. The textbook Pretrained Transformers for Text Ranking: BERT and Beyond by Jimmy Lin, Rodrigo Nogueira, and Andrew Yates is a great effort to collect the recent developments in the use of Transformers for text ranking. The introduction of the book is well-scoped with clear guidance for the reader about topics that are out of scope (such as user aspects). This is followed by an excellent history section, stating for example:", "year": 2022, "publicationdate": "2022-11-07", "externalids": {"DOI": "10.1162/coli_r_00468"}, "doi_lower": "10.1162/coli_r_00468"}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 258686540, "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems", "author_names": ["Yupeng Hou", "Junjie Zhang", "Zihan Lin", "Hongyu Lu", "Ruobing Xie", "Julian McAuley", "Wayne Xin Zhao"], "venue": "European Conference on Information Retrieval", "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08845"}, "doi_lower": "10.48550/arxiv.2305.08845"}
{"paper_id": 259202547, "title": "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models", "author_names": ["Yunjia Xi", "Weiwen Liu", "Jianghao Lin", "Jieming Zhu", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Rui Zhang", "Yong Yu"], "venue": "ACM Conference on Recommender Systems", "abstract": "Recommender system plays a vital role in various online services. However, its insulated nature of training and deploying separately within a specific closed domain limits its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capabilities. Nevertheless, previous attempts to directly use LLMs as recommenders cannot meet the inference latency demand of industrial recommender systems. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs — the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei’s news and music recommendation platforms and gain a 7% and 1.7% improvement in the online A/B test, respectively.", "year": 2023, "publicationdate": "2023-06-19", "externalids": {"DOI": "10.1145/3640457.3688104"}, "doi_lower": "10.1145/3640457.3688104"}
{"paper_id": 257833842, "title": "BloombergGPT: A Large Language Model for Finance", "author_names": ["Shijie Wu", "Ozan Irsoy", "Steven Lu", "Vadim Dabravolski", "Mark Dredze", "Sebastian Gehrmann", "P. Kambadur", "D. Rosenberg", "Gideon Mann"], "venue": "arXiv.org", "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 259137456, "title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective", "author_names": ["Jiatong Li", "Yunqing Liu", "Wenqi Fan", "Xiao Wei", "Hui Liu", "Jiliang Tang", "Qing Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.", "year": 2023, "publicationdate": "2023-06-11", "externalids": {"DOI": "10.1109/TKDE.2024.3393356"}, "doi_lower": "10.1109/tkde.2024.3393356"}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 246465132, "title": "Survey of Pre-trained Models for Natural Language Processing", "author_names": ["Jiajia Peng", "Kaixu Han"], "venue": "2021 International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB)", "abstract": "Deep learning has developed rapidly. The pre-training technology of natural language processing has also made great progress. Early natural language processing used static pre-training techniques, such as Word2Vec, GLoVe, and other word vector methods to segment text. However, this segmentation method does not consider, and the context cannot solve the problem of polysemy. The BERT model has made many typical downstream tasks a typical improvement, which has greatly promoted technological development in the field of NLP and has since entered the epoch of dynamic pre-training technology. On this basis, we analyze the shortcomings of current pre-training technology and finally looks forward to the future development trend of pre-training technology.", "year": 2021, "publicationdate": "2021-12-10", "externalids": {"DOI": "10.1109/iceib53692.2021.9686420"}, "doi_lower": "10.1109/iceib53692.2021.9686420"}
{"paper_id": 257405349, "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT", "author_names": ["Yihan Cao", "Siyu Li", "Yixin Liu", "Zhiling Yan", "Yutong Dai", "Philip S. Yu", "Lichao Sun"], "venue": "arXiv.org", "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.04226"}, "doi_lower": "10.48550/arxiv.2303.04226"}
{"paper_id": 263886074, "title": "A Survey for In-context Learning", "author_names": ["Qingxiu Dong", "Lei Li", "Damai Dai", "Ce Zheng", "Zhiyong Wu", "Baobao Chang", "Xu Sun", "Jingjing Xu", "Lei Li", "Zhifang Sui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 275570199, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "author_names": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "J. Feng", "Chen Gao", "Yong Li"], "venue": "arXiv.org", "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of\"thought\"-- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to\"think\"with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.", "year": 2025, "publicationdate": "2025-01-16", "externalids": {"DOI": "10.48550/arXiv.2501.09686"}, "doi_lower": "10.48550/arxiv.2501.09686"}
{"paper_id": 259982533, "title": "Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community", "author_names": ["Qingyao Ai", "Ting Bai", "Zhao Cao", "Yi Chang", "Jiawei Chen", "Zhumin Chen", "Zhiyong Cheng", "Shoubin Dong", "Zhicheng Dou", "Fuli Feng", "Shengling Gao", "J. Guo", "Xiangnan He", "Yanyan Lan", "Chenliang Li", "Yiqun Liu", "Ziyu Lyu", "Weizhi Ma", "Jun Ma", "Z. Ren", "Pengjie Ren", "Zhiqiang Wang", "Min Wang", "Jirong Wen", "Lei Wu", "Xin Xin", "Jun Xu", "Dawei Yin", "Peng Zhang", "Fan Zhang", "Wei-na Zhang", "M. Zhang", "Xiaofei Zhu"], "venue": "AI Open", "abstract": "The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop's outcomes, including the rethinking of IR's core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.", "year": 2023, "publicationdate": "2023-07-19", "externalids": {"DOI": "10.48550/arXiv.2307.09751"}, "doi_lower": "10.48550/arxiv.2307.09751"}
{"paper_id": 45558661, "title": "Statistical language modeling for information retrieval", "author_names": ["Xiaoyong Liu", "W. Bruce Croft"], "venue": "Annual Review of Information Science and Technology", "abstract": null, "year": 2006, "publicationdate": "2006-10-18", "externalids": {"DOI": "10.1002/ARIS.1440390108"}, "doi_lower": "10.1002/aris.1440390108"}
{"paper_id": 23137465, "title": "Neural Models for Information Retrieval", "author_names": ["Bhaskar Mitra", "Nick Craswell"], "venue": "arXiv.org", "abstract": "Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ machine learning techniques over hand-crafted IR features. By contrast, neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical IR models, these new machine learning based approaches are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of traditional retrieval models. We begin by introducing fundamental concepts of IR and different neural and non-neural approaches to learning vector representations of text. We then review shallow neural IR methods that employ pre-trained neural term embeddings without learning the IR task end-to-end. We introduce deep neural networks next, discussing popular deep architectures. Finally, we review the current DNN models for information retrieval. We conclude with a discussion on potential future directions for neural IR.", "year": 2017, "publicationdate": "2017-05-03", "externalids": {}, "doi_lower": null}
{"paper_id": 254044526, "title": "Dense Text Retrieval Based on Pretrained Language Models: A Survey", "author_names": ["Wayne Xin Zhao", "Jing Liu", "Ruiyang Ren", "Ji-rong Wen"], "venue": "ACM Trans. Inf. Syst.", "abstract": "Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user’s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models (PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.", "year": 2022, "publicationdate": "2022-11-27", "externalids": {"DOI": "10.1145/3637870"}, "doi_lower": "10.1145/3637870"}
{"paper_id": 3626819, "title": "Deep Contextualized Word Representations", "author_names": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {"DOI": "10.18653/v1/N18-1202"}, "doi_lower": "10.18653/v1/n18-1202"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "author_names": ["M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdel-rahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "publicationdate": "2019-10-29", "externalids": {"DOI": "10.18653/v1/2020.acl-main.703"}, "doi_lower": "10.18653/v1/2020.acl-main.703"}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 246473179, "title": "Unified Scaling Laws for Routed Language Models", "author_names": ["Aidan Clark", "Diego de Las Casas", "Aurelia Guy", "A. Mensch", "Michela Paganini", "Jordan Hoffmann", "Bogdan Damoc", "Blake A. Hechtman", "Trevor Cai", "Sebastian Borgeaud", "George van den Driessche", "Eliza Rutherford", "T. Hennigan", "Matthew G. Johnson", "Katie Millican", "Albin Cassirer", "Chris Jones", "Elena Buchatskaya", "D. Budden", "L. Sifre", "Simon Osindero", "O. Vinyals", "Jack W. Rae", "Erich Elsen", "K. Kavukcuoglu", "K. Simonyan"], "venue": "International Conference on Machine Learning", "abstract": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.", "year": 2022, "publicationdate": "2022-02-02", "externalids": {}, "doi_lower": null}
{"paper_id": 230433941, "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "author_names": ["Xiang Lisa Li", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.353"}, "doi_lower": "10.18653/v1/2021.acl-long.353"}
{"paper_id": 233296808, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "author_names": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.243"}, "doi_lower": "10.18653/v1/2021.emnlp-main.243"}
{"paper_id": 257505063, "title": "Query2doc: Query Expansion with Large Language Models", "author_names": ["Liang Wang", "Nan Yang", "Furu Wei"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.48550/arXiv.2303.07678"}, "doi_lower": "10.48550/arxiv.2303.07678"}
{"paper_id": 8718737, "title": "Query Expansion Techniques for Information Retrieval: a Survey", "author_names": ["Dr. Hiteshwar Kumar Azad", "A. Deepak"], "venue": "Information Processing & Management", "abstract": null, "year": 2017, "publicationdate": "2017-08-01", "externalids": {"DOI": "10.1016/j.ipm.2019.05.009"}, "doi_lower": "10.1016/j.ipm.2019.05.009"}
{"paper_id": 57814228, "title": "WordNet: An Electronic Lexical Database", "author_names": ["Dekang Lin"], "venue": "", "abstract": null, "year": 1998, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 9735929, "title": "Lexical relations: enhancing effectiveness of information retrieval systems", "author_names": ["E. Fox"], "venue": "SIGF", "abstract": null, "year": 1980, "publicationdate": "1980-12-01", "externalids": {"DOI": "10.1145/1095403.1095404"}, "doi_lower": "10.1145/1095403.1095404"}
{"paper_id": 17408454, "title": "Automatic thesaurus construction for cross generation corpus", "author_names": ["Hadas Zohar", "Chaya Liebeskind", "Jonathan Schler", "Ido Dagan"], "venue": "JOCCH", "abstract": null, "year": 2013, "publicationdate": "2013-03-01", "externalids": {"DOI": "10.1145/2442080.2442084"}, "doi_lower": "10.1145/2442080.2442084"}
{"paper_id": 1813799, "title": "A corpus analysis approach for automatic query expansion and its extension to multiple databases", "author_names": ["Susan Gauch", "Jianying Wang", "Satya Mahesh Rachakonda"], "venue": "ACM Trans. Inf. Syst.", "abstract": "Searching online text collections can be both rewarding and frustrating. While valuable information can be found, typically many irrelevant documents are also retrieved, while many relevant ones are missed. Terminology mismatches between the user's query and document contents are a main cause of retrieval failures. Expanding a user's query with related words can improve search performances, but finding and using related words is an open problem. This research uses corpus analysis techniques to automatically discover similar words directly from the contents of the databases which are not tagged with part-of-speech labels. Using these similarities, user queries are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and documents. We are able to achieve a 7.6% improvement for TREC 5 queries and up to a 28.5% improvement on the narrow-domain Cystic Fibrosis collection. This work has been extended to multidatabase collections where each subdatabase has a collection-specific similarity matrix associated with it. If the best matrix is selected, substantial search improvements are possible. Various techniques to select the appropriate matrix for a particular query are analyzed, and a 4.8% improvement in the results is validated.", "year": 1999, "publicationdate": "1999-07-01", "externalids": {"DOI": "10.1145/314516.314519"}, "doi_lower": "10.1145/314516.314519"}
{"paper_id": 16884773, "title": "Improving weak ad-hoc queries using wikipedia asexternal corpus", "author_names": ["Yinghao Li", "R. Luk", "E. K. Ho", "K. F. Chung"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": null, "year": 2007, "publicationdate": "2007-07-23", "externalids": {"DOI": "10.1145/1277741.1277914"}, "doi_lower": "10.1145/1277741.1277914"}
{"paper_id": 7311615, "title": "Query Expansion with Freebase", "author_names": ["Chenyan Xiong", "Jamie Callan"], "venue": "International Conference on the Theory of Information Retrieval", "abstract": null, "year": 2015, "publicationdate": "2015-09-27", "externalids": {"DOI": "10.1145/2808194.2809446"}, "doi_lower": "10.1145/2808194.2809446"}
{"paper_id": 254023408, "title": "A new fuzzy logic-based query expansion model for efficient information retrieval using relevance feedback approach", "author_names": ["Jagendra Singh", "Aditi Sharan"], "venue": "Neural computing & applications (Print)", "abstract": null, "year": 2016, "publicationdate": "2016-02-02", "externalids": {"DOI": "10.1007/s00521-016-2207-x"}, "doi_lower": "10.1007/s00521-016-2207-x"}
{"paper_id": 254877046, "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "author_names": ["Luyu Gao", "Xueguang Ma", "Jimmy J. Lin", "Jamie Callan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.18653/v1/2023.acl-long.99"}, "doi_lower": "10.18653/v1/2023.acl-long.99"}
{"paper_id": 271244970, "title": "Crafting the Path: Robust Query Rewriting for Information Retrieval", "author_names": ["Ingeol Baek", "Jimin Lee", "Joonho Yang", "Hwanhee Lee"], "venue": "IEEE Access", "abstract": "Query rewriting aims to generate a new query that can complement the original query to improve the information retrieval system. Recent studies on query rewriting, such as query2doc, query2expand and querey2cot, rely on the internal knowledge of Large Language Models (LLMs) to generate a relevant passage to add information to the query. Nevertheless, the efficacy of these methodologies may markedly decline in instances where the requisite knowledge is not encapsulated within the model’s intrinsic parameters. In this paper, we propose a novel structured query rewriting method called Crafting The Path tailored for retrieval systems. Crafting The Path involves a three-step process that crafts query-related information necessary for finding the passages to be searched in each step. Specifically, the Crafting The Path begins with Query Concept Comprehension, proceeds to Query Type Identification, and finally conducts Expected Answer Extraction. Experimental results show that our method outperforms previous rewriting methods, especially in less familiar domains for LLMs. We demonstrate that our method is less dependent on the internal parameter knowledge of the model and generates queries with fewer factual inaccuracies. Furthermore, we observe that Crafting The Path demonstrates superior performance in the retrieval-augmented generation scenarios.", "year": 2024, "publicationdate": "2024-07-17", "externalids": {"DOI": "10.1109/ACCESS.2025.3538665"}, "doi_lower": "10.1109/access.2025.3538665"}
{"paper_id": 259123956, "title": "Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study", "author_names": ["An Exploratory Study", "L. Gallagher", "Marwah Alaofi", "M. Sanderson", "Falk Scholer"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need. Given a set of information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {"DOI": "10.1145/3539618.3591960"}, "doi_lower": "10.1145/3539618.3591960"}
{"paper_id": 265212774, "title": "Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?", "author_names": ["Minghan Li", "Honglei Zhuang", "Kai Hui", "Zhen Qin", "Jimmy Lin", "R. Jagerman", "Xuanhui Wang", "Michael Bendersky"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Query expansion has been widely used to improve the search results of first-stage retrievers, yet its influence on second-stage, cross-encoder rankers remains under-explored. A recent study shows that current expansion techniques benefit weaker models but harm stronger rankers. In this paper, we re-examine this conclusion and raise the following question: Can query expansion improve generalization of strong cross-encoder rankers? To answer this question, we first apply popular query expansion methods to different cross-encoder rankers and verify the deteriorated zero-shot effectiveness. We identify two vital steps in the experiment: high-quality keyword generation and minimally-disruptive query modification. We show that it is possible to improve the generalization of a strong neural ranker, by generating keywords through a reasoning chain and aggregating the ranking results of each expanded query via self-consistency, reciprocal rank weighting, and fusion. Experiments on BEIR and TREC Deep Learning 2019/2020 show that the nDCG@10 scores of both MonoT5 and RankT5 following these steps are improved, which points out a direction for applying query expansion to strong cross-encoder rankers.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.1145/3626772.3657979"}, "doi_lower": "10.1145/3626772.3657979"}
{"paper_id": 266163878, "title": "Query Rewriting in Retrieval-Augmented Large Language Models", "author_names": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.322"}, "doi_lower": "10.18653/v1/2023.emnlp-main.322"}
{"paper_id": 258676372, "title": "Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval", "author_names": ["Iain Mackie", "Shubham Chatterjee", "Jeffrey Stephen Dalton"], "venue": "arXiv.org", "abstract": "Pseudo-relevance feedback (PRF) is a classical approach to address lexical mismatch by enriching the query using first-pass retrieval. Moreover, recent work on generative-relevance feedback (GRF) shows that query expansion models using text generated from large language models can improve sparse retrieval without depending on first-pass retrieval effectiveness. This work extends GRF to dense and learned sparse retrieval paradigms with experiments over six standard document ranking benchmarks. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recall-oriented measures. Nonetheless, query analysis shows that GRF and PRF have contrasting benefits, with GRF providing external context not present in first-pass retrieval, whereas PRF grounds the query to the information contained within the target corpus. Thus, we propose combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes, which significantly increases recall over PRF methods on 95% of experiments.", "year": 2023, "publicationdate": "2023-05-12", "externalids": {"DOI": "10.48550/arXiv.2305.07477"}, "doi_lower": "10.48550/arxiv.2305.07477"}
{"paper_id": 259187509, "title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval", "author_names": ["Iain Mackie", "Ivan Sekulic", "Shubham Chatterjee", "Jeffrey Stephen Dalton", "F. Crestani"], "venue": "arXiv.org", "abstract": "Recent studies show that Generative Relevance Feedback (GRF), using text generated by Large Language Models (LLMs), can enhance the effectiveness of query expansion. However, LLMs can generate irrelevant information that harms retrieval effectiveness. To address this, we propose Generative Relevance Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, we identify similar real documents for each generated document and use a neural re-ranker to estimate their relevance. Experiments on three standard document ranking benchmarks show that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.48550/arXiv.2306.09938"}, "doi_lower": "10.48550/arxiv.2306.09938"}
{"paper_id": 266180782, "title": "Knowledge Refinement via Interaction Between Search Engines and Large Language Models", "author_names": ["Jiazhan Feng", "Chongyang Tao", "Xiubo Geng", "Tao Shen", "Can Xu", "Guodong Long", "Dongyan Zhao", "Daxin Jiang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.07402"}, "doi_lower": "10.48550/arxiv.2305.07402"}
{"paper_id": 271948010, "title": "Retrieval-Augmented Retrieval: Large Language Models are Strong Zero-Shot Retriever", "author_names": ["Tao Shen", "Guodong Long", "Xiubo Geng", "Chongyang Tao", "Yibin Lei", "Tianyi Zhou", "Michael Blumenstein", "Daxin Jiang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": ",", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.findings-acl.943"}, "doi_lower": "10.18653/v1/2024.findings-acl.943"}
{"paper_id": 258546701, "title": "Query Expansion by Prompting Large Language Models", "author_names": ["R. Jagerman", "Honglei Zhuang", "Zhen Qin", "Xuanhui Wang", "Michael Bendersky"], "venue": "arXiv.org", "abstract": "Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.", "year": 2023, "publicationdate": "2023-05-05", "externalids": {"DOI": "10.48550/arXiv.2305.03653"}, "doi_lower": "10.48550/arxiv.2305.03653"}
{"paper_id": 261395843, "title": "Context Aware Query Rewriting for Text Rankers using LLM", "author_names": ["Abhijit Anand", "V. Venktesh", "Vinay Setty", "Avishek Anand"], "venue": "arXiv.org", "abstract": "Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking. Queries are typically rewritten during query processing time for better query modelling for the downstream ranker. With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks. We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing. We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding. Firstly, we rewrite ambiguous training queries by context-aware prompting of LLMs, where we use only relevant documents as context.Unlike existing approaches, we use LLM-based query rewriting only during the training phase. Eventually, a ranker is fine-tuned on the rewritten queries instead of the original queries during training. In our extensive experiments, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to 33% on the passage ranking task and up to 28% on the document ranking task when compared to the baseline performance of using original queries.", "year": 2023, "publicationdate": "2023-08-31", "externalids": {"DOI": "10.48550/arXiv.2308.16753"}, "doi_lower": "10.48550/arxiv.2308.16753"}
{"paper_id": 269983189, "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "author_names": ["Shengyu Mao", "Yong Jiang", "Boli Chen", "Xiao Li", "Peng Wang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose ours, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that ours~can obtain better performance than baselines.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14431"}, "doi_lower": "10.48550/arxiv.2405.14431"}
{"paper_id": 257495903, "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search", "author_names": ["Kelong Mao", "Zhicheng Dou", "Haonan Chen", "Fengran Mo", "Hongjin Qian"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.", "year": 2023, "publicationdate": "2023-03-12", "externalids": {"DOI": "10.48550/arXiv.2303.06573"}, "doi_lower": "10.48550/arxiv.2303.06573"}
{"paper_id": 264145935, "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting", "author_names": ["Fanghua Ye", "Meng Fang", "Shenghui Li", "Emine Yilmaz"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a\"rewrite-then-edit\"process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.", "year": 2023, "publicationdate": "2023-10-15", "externalids": {"DOI": "10.48550/arXiv.2310.09716"}, "doi_lower": "10.48550/arxiv.2310.09716"}
{"paper_id": 256460881, "title": "ConvTrans: Transforming Web Search Sessions for Conversational Dense Retrieval", "author_names": ["Kelong Mao", "Zhicheng Dou", "Hongjin Qian", "Fengran Mo", "Xiaohua Cheng", "Zhao Cao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Conversational search provides users with a natural and convenient new search experience. Recently, conversational dense retrieval has shown to be a promising technique for realizing conversational search. However, as conversational search systems have not been widely deployed, it is hard to get large-scale real conversational search sessions and relevance labels to support the training of conversational dense retrieval. To tackle this data scarcity problem, previous methods focus on developing better few-shot learning approaches or generating pseudo relevance labels, but the data they use for training still heavily rely on manual generation.In this paper, we present ConvTrans, a data augmentation method that can automatically transform easily-accessible web search sessions into conversational search sessions to fundamentally alleviate the data scarcity problem for conversational dense retrieval. ConvTrans eliminates the gaps between these two types of sessions in terms of session quality and query form to achieve effective session transformation. Extensive evaluations on two widely used conversational search benchmarks, i.e., CAsT-19 and CAsT-20, demonstrate that the same model trained on the data generated by ConvTrans can achieve comparable retrieval performance as it trained on high-quality but expensive artificial conversational search data.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.190"}, "doi_lower": "10.18653/v1/2022.emnlp-main.190"}
{"paper_id": 271709437, "title": "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval", "author_names": ["Yanfei Chen", "Jinsung Yoon", "Devendra Singh Sachan", "Qingze Wang", "Vincent Cohen-Addad", "M. Bateni", "Chen-Yu Lee", "Tomas Pfister"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent advances in large language models (LLMs) have enabled autonomous agents with complex reasoning and task-fulfillment capabilities using a wide range of tools. However, effectively identifying the most relevant tools for a given task becomes a key bottleneck as the toolset size grows, hindering reliable tool utilization. To address this, we introduce Re-Invoke, an unsupervised tool retrieval method designed to scale effectively to large toolsets without training. Specifically, we first generate a diverse set of synthetic queries that comprehensively cover different aspects of the query space associated with each tool document during the tool indexing phase. Second, we leverage LLM's query understanding capabilities to extract key tool-related context and underlying intents from user queries during the inference phase. Finally, we employ a novel multi-view similarity ranking strategy based on intents to pinpoint the most relevant tools for each query. Our evaluation demonstrates that Re-Invoke significantly outperforms state-of-the-art alternatives in both single-tool and multi-tool scenarios, all within a fully unsupervised setting. Notably, on the ToolE datasets, we achieve a 20% relative improvement in nDCG@5 for single-tool retrieval and a 39% improvement for multi-tool retrieval.", "year": 2024, "publicationdate": "2024-08-03", "externalids": {"DOI": "10.48550/arXiv.2408.01875"}, "doi_lower": "10.48550/arxiv.2408.01875"}
{"paper_id": 271931445, "title": "RRNorm: A Novel Framework for Chinese Disease Diagnoses Normalization via LLM-Driven Terminology Component Recognition and Reconstruction", "author_names": ["Yongqi Fan", "Yansha Zhu", "Kui Xue", "Jingping Liu", "Tong Ruan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The Clinical Terminology Normalization aims 001 at finding standard terms from a given termbase 002 for mentions extracted from clinical texts. How-003 ever, we found that extracted mentions suffer 004 from the multi-implication problem, especially 005 disease diagnoses. The reason for this is that 006 physicians often use abbreviations, conjunc-007 tions, and juxtapositions when writing diag-008 noses, and it is difficult to manually decom-009 pose. To address this problem, we propose 010 a Terminology Component Recognition and 011 Reconstruction strategy that leverages the rea-012 soning capability of large language models 013 (LLMs) to recognize the components of terms, 014 enabling automated decomposition and trans-015 forming original mentions into multiple atomic 016 mentions. Furthermore, we adopt the main-017 stream “Recall and Rank” framework to apply 018 the benefits of the above strategy to the task 019 flow. By leveraging the LLM incorporating 020 the advanced sampling strategies, we design a 021 sampling algorithm for atomic mentions and 022 train the recall model using contrastive learn-023 ing. Besides the information about the compo-024 nents is also used as knowledge to guide the 025 final term ranking and selection. The experi-026 mental results show that our proposed strategy 027 effectively improves the performance of the ter-028 minology normalization task and our proposed 029 approach achieves state-of-the-art on the exper-030 imental dataset. 031", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.findings-acl.544"}, "doi_lower": "10.18653/v1/2024.findings-acl.544"}
{"paper_id": 265295442, "title": "An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback", "author_names": ["Kaustubh D. Dhole", "Ramraj Chandradevan", "Eugene Agichtein"], "venue": "arXiv.org", "abstract": "While search is the predominant method of accessing information, formulating effective queries remains a challenging task, especially for situations where the users are not familiar with a domain, or searching for documents in other languages, or looking for complex information such as events, which are not easily expressible as queries. Providing example documents or passages of interest, might be easier for a user, however, such query-by-example scenarios are prone to concept drift, and are highly sensitive to the query generation method. This demo illustrates complementary approaches of using LLMs interactively, assisting and enabling the user to provide edits and feedback at all stages of the query formulation process. The proposed Query Generation Assistant is a novel search interface which supports automatic and interactive query generation over a mono-linguial or multi-lingual document collection. Specifically, the proposed assistive interface enables the users to refine the queries generated by different LLMs, to provide feedback on the retrieved documents or passages, and is able to incorporate the users' feedback as prompts to generate more effective queries. The proposed interface is a valuable experimental tool for exploring fine-tuning and prompting of LLMs for query generation to qualitatively evaluate the effectiveness of retrieval and ranking models, and for conducting Human-in-the-Loop (HITL) experiments for complex search tasks where users struggle to formulate queries without such assistance.", "year": 2023, "publicationdate": "2023-11-19", "externalids": {"DOI": "10.48550/arXiv.2311.11226"}, "doi_lower": "10.48550/arxiv.2311.11226"}
{"paper_id": 276557765, "title": "Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs", "author_names": ["Raymond Wilson", "Chase Carter", "Cole Graham"], "venue": "arXiv.org", "abstract": "Conversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings. This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting. Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning. Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites. The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.", "year": 2025, "publicationdate": "2025-02-20", "externalids": {"DOI": "10.48550/arXiv.2502.15009"}, "doi_lower": "10.48550/arxiv.2502.15009"}
{"paper_id": 270845913, "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation", "author_names": ["Chenlong Deng", "Kelong Mao", "Zhicheng Dou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.", "year": 2024, "publicationdate": "2024-06-28", "externalids": {"DOI": "10.48550/arXiv.2406.19760"}, "doi_lower": "10.48550/arxiv.2406.19760"}
{"paper_id": 216867871, "title": "Zero-shot Neural Retrieval via Domain-targeted Synthetic Query Generation", "author_names": ["Ji Ma", "I. Korotkov", "Yinfei Yang", "Keith B. Hall", "Ryan T. McDonald"], "venue": "arXiv.org", "abstract": "Deep neural scoring models have recently been shown to improve ranking quality on a number of benchmarks (Guo et al., 2016; Daiet al., 2018; MacAvaney et al., 2019; Yanget al., 2019a). However, these methods rely on underlying ad-hoc retrieval systems to generate candidates for scoring, which are rarely neural themselves (Zamani et al., 2018). Re-cent work has shown that the performance of ad-hoc neural retrieval systems can be competitive with a number of baselines (Zamani et al.,2018), potentially leading the way to full end-to-end neural retrieval. A major road-block to the adoption of ad-hoc retrieval models is that they require large supervised training sets to surpass classic term-based techniques, which can be developed from raw corpora. Previous work shows weakly supervised data can yield competitive results, e.g., click data (Dehghaniet al., 2017; Borisov et al., 2016). Unfortunately for many domains, even weakly supervised data can be scarce. In this paper, we pro-pose an approach to zero-shot learning (Xianet al., 2018) for ad-hoc retrieval models that relies on synthetic query generation. Crucially, the query generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, query-document relevance pairs that are domain targeted. On a number of benchmarks, we show that this is an effective strategy for building neural retrieval models for specialised domains.", "year": 2020, "publicationdate": "2020-04-29", "externalids": {}, "doi_lower": null}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 252408513, "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "author_names": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.", "year": 2022, "publicationdate": "2022-09-21", "externalids": {}, "doi_lower": null}
{"paper_id": 86611921, "title": "Natural Questions: A Benchmark for Question Answering Research", "author_names": ["T. Kwiatkowski", "J. Palomaki", "Olivia Redfield", "Michael Collins", "Ankur P. Parikh", "Chris Alberti", "D. Epstein", "I. Polosukhin", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Llion Jones", "Matthew Kelcey", "Ming-Wei Chang", "Andrew M. Dai", "Jakob Uszkoreit", "Quoc V. Le", "Slav Petrov"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.1162/tacl_a_00276"}, "doi_lower": "10.1162/tacl_a_00276"}
{"paper_id": 272986661, "title": "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "author_names": ["Yike Wu", "Yi Huang", "Nan Hu", "Yuncheng Hua", "Guilin Qi", "Jiaoyan Chen", "Jeff Z. Pan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question’s semantics. To address them, we propose a novel rewriting method CoTKR, Chain- of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA.", "year": 2024, "publicationdate": "2024-09-29", "externalids": {"DOI": "10.48550/arXiv.2409.19753"}, "doi_lower": "10.48550/arxiv.2409.19753"}
{"paper_id": 271244970, "title": "Crafting the Path: Robust Query Rewriting for Information Retrieval", "author_names": ["Ingeol Baek", "Jimin Lee", "Joonho Yang", "Hwanhee Lee"], "venue": "IEEE Access", "abstract": "Query rewriting aims to generate a new query that can complement the original query to improve the information retrieval system. Recent studies on query rewriting, such as query2doc, query2expand and querey2cot, rely on the internal knowledge of Large Language Models (LLMs) to generate a relevant passage to add information to the query. Nevertheless, the efficacy of these methodologies may markedly decline in instances where the requisite knowledge is not encapsulated within the model’s intrinsic parameters. In this paper, we propose a novel structured query rewriting method called Crafting The Path tailored for retrieval systems. Crafting The Path involves a three-step process that crafts query-related information necessary for finding the passages to be searched in each step. Specifically, the Crafting The Path begins with Query Concept Comprehension, proceeds to Query Type Identification, and finally conducts Expected Answer Extraction. Experimental results show that our method outperforms previous rewriting methods, especially in less familiar domains for LLMs. We demonstrate that our method is less dependent on the internal parameter knowledge of the model and generates queries with fewer factual inaccuracies. Furthermore, we observe that Crafting The Path demonstrates superior performance in the retrieval-augmented generation scenarios.", "year": 2024, "publicationdate": "2024-07-17", "externalids": {"DOI": "10.1109/ACCESS.2025.3538665"}, "doi_lower": "10.1109/access.2025.3538665"}
{"paper_id": 275789950, "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "author_names": ["DeepSeek-AI", "Daya Guo", "Dejian Yang", "Haowei Zhang", "Jun-Mei Song", "Ruoyu Zhang", "R. Xu", "Qihao Zhu", "Shirong Ma", "Peiyi Wang", "Xiaoling Bi", "Xiaokang Zhang", "Xingkai Yu", "Yu Wu", "Z. F. Wu", "Zhibin Gou", "Zhihong Shao", "Zhuoshu Li", "Ziyi Gao", "A. Liu", "Bing Xue", "Bing-Li Wang", "Bochao Wu", "Bei Feng", "Chengda Lu", "Chenggang Zhao", "C. Deng", "Chenyu Zhang", "C. Ruan", "Damai Dai", "Deli Chen", "Dong-Li Ji", "Erhang Li", "Fangyun Lin", "Fucong Dai", "Fuli Luo", "Guangbo Hao", "Guanting Chen", "Guowei Li", "H. Zhang", "Han Bao", "Hanwei Xu", "Haocheng Wang", "Honghui Ding", "Huajian Xin", "Huazuo Gao", "Hui Qu", "Hui Li", "Jianzhong Guo", "Jiashi Li", "Jiawei Wang", "JingChang Chen", "Jingyang Yuan", "Junjie Qiu", "Junlong Li", "J. Cai", "J. Ni", "Jian Liang", "Jin Chen", "Kai Dong", "Kai Hu", "Kaige Gao", "Kang Guan", "Kexin Huang", "K. Yu", "Lean Wang", "Lecong Zhang", "Liang Zhao", "Litong Wang", "Liyue Zhang", "Lei Xu", "Leyi Xia", "Mingchuan Zhang", "Minghua Zhang", "M. Tang", "Meng Li", "Miaojun Wang", "Mingming Li", "Ning Tian", "Panpan Huang", "Peng Zhang", "Qiancheng Wang", "Qinyu Chen", "Qiushi Du", "Ruiqi Ge", "Ruisong Zhang", "Ruizhe Pan", "Runji Wang", "R. J. Chen", "R. Jin", "Ruyi Chen", "Shanghao Lu", "Shangyan Zhou", "Shanhuang Chen", "Shengfeng Ye", "Shiyu Wang", "Shuiping Yu", "Shunfeng Zhou", "Shuting Pan", "S. S. Li", "Shuang Zhou", "Shao-Kang Wu", "Tao Yun", "Tian Pei", "T. Sun", "T. Wang", "Wangding Zeng", "Wanjia Zhao", "Wen Liu", "W. Liang", "Wenjun Gao", "Wen-Xia Yu", "Wentao Zhang", "W. Xiao", "Wei An", "Xiaodong Liu", "Xiaohan Wang", "Xiaokang Chen", "X. Nie", "Xin Cheng", "Xin Liu", "Xin Xie", "Xingchao Liu", "Xinyu Yang", "Xinyuan Li", "Xuecheng Su", "Xuheng Lin", "X. Q. Li", "Xiangyu Jin", "Xi-Cheng Shen", "Xiaosha Chen", "Xiaowen Sun", "Xiaoxiang Wang", "Xinnan Song", "Xinyi Zhou", "Xianzu Wang", "Xinxia Shan", "Y. K. Li", "Y. Q. Wang", "Y. X. Wei", "Yang Zhang", "Yanhong Xu", "Yao Li", "Yao Zhao", "Yaofeng Sun", "Yaohui Wang", "Yi Yu", "Yichao Zhang", "Yifan Shi", "Yi Xiong", "Ying He", "Yishi Piao", "Yisong Wang", "Yixuan Tan", "Yiyang Ma", "Yiyuan Liu", "Yongqiang Guo", "Y. Ou", "Yuduan Wang", "Yue Gong", "Yu-Jing Zou", "Yujia He", "Yunfan Xiong", "Yu-Wei Luo", "Yu-mei You", "Yuxuan Liu", "Yuyang Zhou", "Y. X. Zhu", "Yanping Huang", "Yao Li", "Yi Zheng", "Yuchen Zhu", "Yunxiang Ma", "Ying Tang", "Y. Zha", "Yuting Yan", "Z. Ren", "Z. Ren", "Zhangli Sha", "Zhe Fu", "Zhean Xu", "Zhenda Xie", "Zhen-guo Zhang", "Zhewen Hao", "Zhicheng Ma", "Zhigang Yan", "Zhiyu Wu", "Zihui Gu", "Zijia Zhu", "Zijun Liu", "Zi-An Li", "Ziwei Xie", "Ziyang Song", "Zizheng Pan", "Zhen Huang", "Zhipeng Xu", "Zhongyu Zhang", "Zhen Zhang"], "venue": "arXiv.org", "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.", "year": 2025, "publicationdate": "2025-01-22", "externalids": {"DOI": "10.48550/arXiv.2501.12948"}, "doi_lower": "10.48550/arxiv.2501.12948"}
{"paper_id": 262012661, "title": "When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets", "author_names": ["Orion Weller", "Kyle Lo", "David Wadden", "Dawn J Lawrie", "Benjamin Van Durme", "Arman Cohan", "Luca Soldaini"], "venue": "Findings", "abstract": "Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear.", "year": 2023, "publicationdate": "2023-09-15", "externalids": {"DOI": "10.48550/arXiv.2309.08541"}, "doi_lower": "10.48550/arxiv.2309.08541"}
{"paper_id": 246705967, "title": "InPars: Data Augmentation for Information Retrieval using Large Language Models", "author_names": ["L. Bonifacio", "Hugo Abonizio", "Marzieh Fadaee", "Rodrigo Nogueira"], "venue": "arXiv.org", "abstract": "The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars .", "year": 2022, "publicationdate": "2022-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 260926325, "title": "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval", "author_names": ["Guangyuan Ma", "Xing Wu", "Peng Wang", "Zijia Lin", "Songlin Hu"], "venue": "arXiv.org", "abstract": "In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.", "year": 2023, "publicationdate": "2023-08-16", "externalids": {"DOI": "10.48550/arXiv.2308.08285"}, "doi_lower": "10.48550/arxiv.2308.08285"}
{"paper_id": 255440689, "title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval", "author_names": ["Vitor Jeronymo", "L. Bonifacio", "Hugo Abonizio", "Marzieh Fadaee", "R. Lotufo", "Jakub Zavrel", "Rodrigo Nogueira"], "venue": "arXiv.org", "abstract": "Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu", "year": 2023, "publicationdate": "2023-01-04", "externalids": {"DOI": "10.48550/arXiv.2301.01820"}, "doi_lower": "10.48550/arxiv.2301.01820"}
{"paper_id": 252519173, "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "author_names": ["Zhuyun Dai", "Vincent Zhao", "Ji Ma", "Yi Luan", "Jianmo Ni", "Jing Lu", "A. Bakalov", "Kelvin Guu", "Keith B. Hall", "Ming-Wei Chang"], "venue": "International Conference on Learning Representations", "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "year": 2022, "publicationdate": "2022-09-23", "externalids": {"DOI": "10.48550/arXiv.2209.11755"}, "doi_lower": "10.48550/arxiv.2209.11755"}
{"paper_id": 257405222, "title": "AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation", "author_names": ["Rui Meng", "Ye Liu", "Semih Yavuz", "Divyansh Agarwal", "Lifu Tu", "Ning Yu", "Jianguo Zhang", "Meghana Moorthy Bhat", "Yingbo Zhou"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 257279774, "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers", "author_names": ["Jon Saad-Falcon", "O. Khattab", "Keshav Santhanam", "Radu Florian", "M. Franz", "S. Roukos", "Avirup Sil", "Md Arafat Sultan", "Christopher Potts"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.", "year": 2023, "publicationdate": "2023-03-01", "externalids": {"DOI": "10.48550/arXiv.2303.00807"}, "doi_lower": "10.48550/arxiv.2303.00807"}
{"paper_id": 259937100, "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models", "author_names": ["Zhiyuan Peng", "Xuyang Wu", "Yihan Fang"], "venue": "Knowledge-Based Systems", "abstract": "Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.", "year": 2023, "publicationdate": "2023-07-17", "externalids": {"DOI": "10.48550/arXiv.2307.08303"}, "doi_lower": "10.48550/arxiv.2307.08303"}
{"paper_id": 249926985, "title": "Questions Are All You Need to Train a Dense Passage Retriever", "author_names": ["Devendra Singh Sachan", "M. Lewis", "Dani Yogatama", "Luke Zettlemoyer", "J. Pineau", "M. Zaheer"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: https://github.com/DevSinghSachan/art.", "year": 2022, "publicationdate": "2022-06-21", "externalids": {"DOI": "10.1162/tacl_a_00564"}, "doi_lower": "10.1162/tacl_a_00564"}
{"paper_id": 266693831, "title": "Improving Text Embeddings with Large Language Models", "author_names": ["Liang Wang", "Nan Yang", "Xiaolong Huang", "Linjun Yang", "Rangan Majumder", "Furu Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.", "year": 2023, "publicationdate": "2023-12-31", "externalids": {"DOI": "10.48550/arXiv.2401.00368"}, "doi_lower": "10.48550/arxiv.2401.00368"}
{"paper_id": 233296016, "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "author_names": ["Nandan Thakur", "Nils Reimers", "Andreas Ruckl'e", "Abhishek Srivastava", "Iryna Gurevych"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.", "year": 2021, "publicationdate": "2021-04-17", "externalids": {}, "doi_lower": null}
{"paper_id": 246275593, "title": "Text and Code Embeddings by Contrastive Pre-Training", "author_names": ["Arvind Neelakantan", "Tao Xu", "Raul Puri", "Alec Radford", "Jesse Michael Han", "Jerry Tworek", "Qiming Yuan", "N. Tezak", "Jong Wook Kim", "Chris Hallacy", "Johannes Heidecke", "Pranav Shyam", "Boris Power", "Tyna Eloundou Nekoul", "Girish Sastry", "Gretchen Krueger", "D. Schnurr", "F. Such", "K. Hsu", "Madeleine Thompson", "Tabarak Khan", "Toki Sherbakov", "Joanne Jang", "Peter Welinder", "Lilian Weng"], "venue": "arXiv.org", "abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.", "year": 2022, "publicationdate": "2022-01-24", "externalids": {}, "doi_lower": null}
{"paper_id": 245144556, "title": "Large Dual Encoders Are Generalizable Retrievers", "author_names": ["Jianmo Ni", "Chen Qu", "Jing Lu", "Zhuyun Dai", "Gustavo Hernández Abrego", "Ji Ma", "Vincent Zhao", "Yi Luan", "Keith B. Hall", "Ming-Wei Chang", "Yinfei Yang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.", "year": 2021, "publicationdate": "2021-12-15", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.669"}, "doi_lower": "10.18653/v1/2022.emnlp-main.669"}
{"paper_id": 257985497, "title": "Instruction Tuning with GPT-4", "author_names": ["Baolin Peng", "Chunyuan Li", "Pengcheng He", "Michel Galley", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {}, "doi_lower": null}
{"paper_id": 259203998, "title": "Textbooks Are All You Need", "author_names": ["Suriya Gunasekar", "Yi Zhang", "J. Aneja", "C. C. T. Mendes", "Allison Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "A. Salim", "S. Shah", "Harkirat Singh Behl", "Xin Wang", "Sébastien Bubeck", "Ronen Eldan", "A. Kalai", "Y. Lee", "Yuan-Fang Li"], "venue": "arXiv.org", "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\"data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "year": 2023, "publicationdate": "2023-06-20", "externalids": {}, "doi_lower": null}
{"paper_id": 263908865, "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval", "author_names": ["Xueguang Ma", "Liang Wang", "Nan Yang", "Furu Wei", "Jimmy Lin"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "While large language models (LLMs) have shown impressive NLP capabilities, existing IR applications mainly focus on prompting LLMs to generate query expansions or generating permutations for listwise reranking. In this study, we leverage LLMs directly to serve as components in the widely used multi-stage text ranking pipeline. Specifically, we fine-tune the open-source LLaMA-2 model as a dense retriever (repLLaMA) and a pointwise reranker (rankLLaMA). This is performed for both passage and document retrieval tasks using the MS MARCO training data. Our study shows that finetuned LLM retrieval models outperform smaller models. They are more effective and exhibit greater generalizability, requiring only a straightforward training strategy. Moreover, our pipeline allows for the fine-tuning of LLMs at each stage of a multi-stage retrieval pipeline. This demonstrates the strong potential for optimizing LLMs to enhance a variety of retrieval tasks. Furthermore, as LLMs are naturally pre-trained with longer contexts, they can directly represent longer documents. This eliminates the need for heuristic segmenting and pooling strategies to rank long documents. On the MS MARCO and BEIR datasets, our repLLaMA-rankLLaMA pipeline demonstrates a high level of effectiveness.", "year": 2023, "publicationdate": "2023-10-12", "externalids": {"DOI": "10.1145/3626772.3657951"}, "doi_lower": "10.1145/3626772.3657951"}
{"paper_id": 232417149, "title": "Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval", "author_names": ["Rui Zhao", "Kecheng Zheng", "Zhengjun Zha", "Hongtao Xie", "Jiebo Luo"], "venue": "arXiv.org", "abstract": "Cross-modal video-text retrieval, a challenging task in the field of vision and language, aims at retrieving corresponding instance giving sample from either modality. Existing approaches for this task all focus on how to design encoding model through a hard negative ranking loss, leaving two key problems unaddressed during this procedure. First, in the training stage, only a mini-batch of instance pairs is available in each iteration. Therefore, this kind of hard negatives is locally mined inside a mini-batch while ignoring the global negative samples among the dataset. Second, there are many text descriptions for one video and each text only describes certain local features of a video. Previous works for this task did not consider to fuse the multiply texts corresponding to a video during the training. In this paper, to solve the above two problems, we propose a novel memory enhanced embedding learning (MEEL) method for videotext retrieval. To be specific, we construct two kinds of memory banks respectively: cross-modal memory module and text center memory module. The cross-modal memory module is employed to record the instance embeddings of all the datasets for global negative mining. To avoid the fast evolving of the embedding in the memory bank during training, we utilize a momentum encoder to update the features by a moving-averaging strategy. The text center memory module is designed to record the center information of the multiple textual instances corresponding to a video, and aims at bridging these textual instances together. Extensive experimental results on two challenging benchmarks, i.e., MSR-VTT and VATEX, demonstrate the effectiveness of the proposed method.", "year": 2021, "publicationdate": "2021-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 274465107, "title": "Linq-Embed-Mistral Technical Report", "author_names": ["Chanyeol Choi", "Junseong Kim", "Seolhwa Lee", "Jihoon Kwon", "Sangmo Gu", "Yejin Kim", "Minkyung Cho", "Jy-yong Sohn"], "venue": "arXiv.org", "abstract": "This report explores the enhancement of text retrieval performance using advanced data refinement techniques. We develop Linq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}} by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on sophisticated data crafting, data filtering, and negative mining methods, which are highly tailored to each task, applied to both existing benchmark dataset and highly tailored synthetic dataset generated via large language models (LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024), achieving an average score of 68.2 across 56 datasets, and ranks 1st among all models for retrieval tasks on the MTEB leaderboard with a performance score of 60.2. This performance underscores its superior capability in enhancing search precision and reliability. Our contributions include advanced data refinement methods that significantly improve model performance on benchmark and synthetic datasets, techniques for homogeneous task ordering and mixed task fine-tuning to enhance model generalization and stability, and a streamlined evaluation process using 4-bit precision and a light retrieval evaluation set, which accelerates validation without sacrificing accuracy.", "year": 2024, "publicationdate": "2024-12-04", "externalids": {"DOI": "10.48550/arXiv.2412.03223"}, "doi_lower": "10.48550/arxiv.2412.03223"}
{"paper_id": 252907685, "title": "MTEB: Massive Text Embedding Benchmark", "author_names": ["Niklas Muennighoff", "Nouamane Tazi", "L. Magne", "Nils Reimers"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "year": 2022, "publicationdate": "2022-10-13", "externalids": {"DOI": "10.18653/v1/2023.eacl-main.148"}, "doi_lower": "10.18653/v1/2023.eacl-main.148"}
{"paper_id": 266551612, "title": "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval", "author_names": ["Chaofan Li", "Zheng Liu", "Shitao Xiao", "Yingxia Shao", "Defu Lian"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs'strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called Llama2Vec, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to reconstruct the input sentence and predict the next sentence based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model's fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.", "year": 2023, "publicationdate": "2023-12-24", "externalids": {"DOI": "10.18653/v1/2024.acl-long.191"}, "doi_lower": "10.18653/v1/2024.acl-long.191"}
{"paper_id": 270064259, "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models", "author_names": ["Chankyu Lee", "Rajarshi Roy", "Mengyao Xu", "Jonathan Raiman", "M. Shoeybi", "Bryan Catanzaro", "Wei Ping"], "venue": "International Conference on Learning Representations", "abstract": "Decoder-only LLM-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce NV-Embed, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the lasttoken embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across 56 tasks, demonstrating the sustained effectiveness of the proposed methods over time. It also achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. We further provide the analysis of model compression techniques for generalist embedding models.", "year": 2024, "publicationdate": "2024-05-27", "externalids": {"DOI": "10.48550/arXiv.2405.17428"}, "doi_lower": "10.48550/arxiv.2405.17428"}
{"paper_id": 254853816, "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings", "author_names": ["Hongjin Su", "Weijia Shi", "Jungo Kasai", "Yizhong Wang", "Yushi Hu", "Mari Ostendorf", "Wen-tau Yih", "Noah A. Smith", "Luke Zettlemoyer", "Tao Yu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.", "year": 2022, "publicationdate": "2022-12-19", "externalids": {"DOI": "10.48550/arXiv.2212.09741"}, "doi_lower": "10.48550/arxiv.2212.09741"}
{"paper_id": 271924082, "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "author_names": ["Kun Luo", "Minghao Qin", "Zheng Liu", "Shitao Xiao", "Jun Zhao", "Kang Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations—such as parameter sizes, pre-training duration, and alignment processes—on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.", "year": 2024, "publicationdate": "2024-08-22", "externalids": {"DOI": "10.48550/arXiv.2408.12194"}, "doi_lower": "10.48550/arxiv.2408.12194"}
{"paper_id": 269293051, "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "author_names": ["Kelong Mao", "Chenlong Deng", "Haonan Chen", "Fengran Mo", "Zheng Liu", "Tetsuya Sakai", "Zhicheng Dou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever significantly outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.", "year": 2024, "publicationdate": "2024-04-21", "externalids": {"DOI": "10.48550/arXiv.2404.13556"}, "doi_lower": "10.48550/arxiv.2404.13556"}
{"paper_id": 260334381, "title": "Scaling Sentence Embeddings with Large Language Models", "author_names": ["Ting Jiang", "Shaohan Huang", "Zhongzhi Luan", "Deqing Wang", "Fuzhen Zhuang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.", "year": 2023, "publicationdate": "2023-07-31", "externalids": {"DOI": "10.48550/arXiv.2307.16645"}, "doi_lower": "10.48550/arxiv.2307.16645"}
{"paper_id": 233864793, "title": "Rethinking Search: Making Experts out of Dilettantes", "author_names": ["Donald Metzler", "Yi Tay", "Dara Bahri", "Marc Najork"], "venue": "arXiv.org", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 255879096, "title": "DynamicRetriever: A Pre-trained Model-based IR System Without an Explicit Index", "author_names": ["Yujia Zhou", "Jing Yao", "Zhicheng Dou", "Ledell Yu Wu", "Ji-rong Wen"], "venue": "Machine Intelligence Research", "abstract": null, "year": 2023, "publicationdate": "2023-01-11", "externalids": {"DOI": "10.1007/s11633-022-1373-9"}, "doi_lower": "10.1007/s11633-022-1373-9"}
{"paper_id": 251594672, "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks", "author_names": ["Jiangui Chen", "Ruqing Zhang", "J. Guo", "Y. Liu", "Yixing Fan", "Xueqi Cheng"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Knowledge-intensive language tasks (KILT) usually require a large body of information to provide correct answers. A popular paradigm to solve this problem is to combine a search system with a machine reader, where the former retrieves supporting evidences and the latter examines them to produce answers. Recently, the reader component has witnessed significant advances with the help of large-scale pre-trained generative models. Meanwhile most existing solutions in the search component rely on the traditional \"index-retrieve-then-rank'' pipeline, which suffers from large memory footprint and difficulty in end-to-end optimization. Inspired by recent efforts in constructing model-based IR models, we propose to replace the traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the search process and be optimized in an end-to-end manner. We show that a strong generative retrieval model can be learned with a set of adequately designed pre-training tasks, and be adopted to improve a variety of downstream KILT tasks with further fine-tuning. We name the pre-trained generative retrieval model as CorpusBrain as all information about the corpus is encoded in its parameters without the need of constructing additional index. Empirical results show that CorpusBrain can significantly outperform strong baselines for the retrieval task on the KILT benchmark and establish new state-of-the-art downstream performances. We also show that CorpusBrain works well under zero- and low-resource settings.", "year": 2022, "publicationdate": "2022-08-16", "externalids": {"DOI": "10.1145/3511808.3557271"}, "doi_lower": "10.1145/3511808.3557271"}
{"paper_id": 246863488, "title": "Transformer Memory as a Differentiable Search Index", "author_names": ["Yi Tay", "Vinh Q. Tran", "Mostafa Dehghani", "Jianmo Ni", "Dara Bahri", "Harsh Mehta", "Zhen Qin", "Kai Hui", "Zhe Zhao", "Jai Gupta", "Tal Schuster", "William W. Cohen", "Donald Metzler"], "venue": "Neural Information Processing Systems", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "year": 2022, "publicationdate": "2022-02-14", "externalids": {}, "doi_lower": null}
{"paper_id": 249395549, "title": "A Neural Corpus Indexer for Document Retrieval", "author_names": ["Yujing Wang", "Ying Hou", "Hong Wang", "Ziming Miao", "Shibin Wu", "Hao Sun", "Qi Chen", "Yuqing Xia", "Chengmin Chi", "Guoshuai Zhao", "Zheng Liu", "Xing Xie", "Hao Sun", "Weiwei Deng", "Qi Zhang", "Mao Yang"], "venue": "Neural Information Processing Systems", "abstract": "Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method.", "year": 2022, "publicationdate": "2022-06-06", "externalids": {"DOI": "10.48550/arXiv.2206.02743"}, "doi_lower": "10.48550/arxiv.2206.02743"}
{"paper_id": 248366293, "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers", "author_names": ["Michele Bevilacqua", "G. Ottaviano", "Patrick Lewis", "Wen-tau Yih", "Sebastian Riedel", "F. Petroni"], "venue": "Neural Information Processing Systems", "abstract": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.", "year": 2022, "publicationdate": "2022-04-22", "externalids": {"DOI": "10.48550/arXiv.2204.10628"}, "doi_lower": "10.48550/arxiv.2204.10628"}
{"paper_id": 258822999, "title": "How Does Generative Retrieval Scale to Millions of Passages?", "author_names": ["Ronak Pradeep", "Kai Hui", "Jai Gupta", "Á. Lelkes", "Honglei Zhuang", "Jimmy J. Lin", "Donald Metzler", "Vinh Q. Tran"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Popularized by the Differentiable Search Index, the emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100k in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we find that generative retrieval is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge. We believe these findings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.48550/arXiv.2305.11841"}, "doi_lower": "10.48550/arxiv.2305.11841"}
{"paper_id": 267406766, "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks", "author_names": ["Xiaoxi Li", "Zhicheng Dou", "Yujia Zhou", "Fangchao Liu"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose CorpusLM, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.", "year": 2024, "publicationdate": "2024-02-02", "externalids": {"DOI": "10.1145/3626772.3657778"}, "doi_lower": "10.1145/3626772.3657778"}
{"paper_id": 233443837, "title": "Text-to-Text Multi-view Learning for Passage Re-ranking", "author_names": ["Jia-Huei Ju", "Jheng-Hong Yang", "Chuan-Ju Wang"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Recently, much progress in natural language processing has been driven by deep contextualized representations pretrained on large corpora. Typically, the fine-tuning on these pretrained models for a specific downstream task is based on single-view learning, which is however inadequate as a sentence can be interpreted differently from different perspectives. Therefore, in this work, we propose a text-to-text multi-view learning framework by incorporating an additional view---the text generation view---into a typical single-view passage ranking model. Empirically, the proposed approach is of help to the ranking performance compared to its single-view counterpart. Component analysis is also reported in the paper.", "year": 2021, "publicationdate": "2021-04-29", "externalids": {"DOI": "10.1145/3404835.3463048"}, "doi_lower": "10.1145/3404835.3463048"}
{"paper_id": 231603106, "title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models", "author_names": ["Ronak Pradeep", "Rodrigo Nogueira", "Jimmy J. Lin"], "venue": "arXiv.org", "abstract": "We propose a design pattern for tackling text ranking problems, dubbed\"Expando-Mono-Duo\", that has been empirically validated for a number of ad hoc retrieval tasks in different domains. At the core, our design relies on pretrained sequence-to-sequence models within a standard multi-stage ranking architecture.\"Expando\"refers to the use of document expansion techniques to enrich keyword representations of texts prior to inverted indexing.\"Mono\"and\"Duo\"refer to components in a reranking pipeline based on a pointwise model and a pairwise model that rerank initial candidates retrieved using keyword search. We present experimental results from the MS MARCO passage and document ranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge that validate our design. In all these tasks, we achieve effectiveness that is at or near the state of the art, in some cases using a zero-shot approach that does not exploit any training data from the target task. To support replicability, implementations of our design pattern are open-sourced in the Pyserini IR toolkit and PyGaggle neural reranking library.", "year": 2021, "publicationdate": "2021-01-14", "externalids": {}, "doi_lower": null}
{"paper_id": 252993059, "title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses", "author_names": ["Honglei Zhuang", "Zhen Qin", "R. Jagerman", "Kai Hui", "Ji Ma", "Jing Lu", "Jianmo Ni", "Xuanhui Wang", "Michael Bendersky"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Pretrained language models such as BERT have been shown to be exceptionally effective for text ranking. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as a classification problem and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each query-document pair, but also can be fine-tuned with pairwise or listwise ranking losses to optimize ranking performance. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, ranking models fine-tuned with listwise ranking losses have better zero-shot ranking performance on out-of-domain data than models fine-tuned with classification losses.", "year": 2022, "publicationdate": "2022-10-12", "externalids": {"DOI": "10.1145/3539618.3592047"}, "doi_lower": "10.1145/3539618.3592047"}
{"paper_id": 265466170, "title": "A Two-Stage Adaptation of Large Language Models for Text Ranking", "author_names": ["Longhui Zhang", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Min Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text ranking is a critical task in information retrieval. Recent advances in pre-trained language models (PLMs), especially large language models (LLMs), present new opportunities for applying them to text ranking. While supervised fine-tuning (SFT) with ranking data has been widely explored to better align PLMs with text ranking goals, previous studies have focused primarily on encoder-only and encoder-decoder PLMs. Research on leveraging decoder-only LLMs for text ranking remains scarce. An exception to this is RankLLaMA, which uses direct SFT to explore LLaMA's potential for text ranking. In this work, we propose a two-stage progressive paradigm to better adapt LLMs to text ranking. First, we conduct continual pre-training (CPT) of LLMs on a large weakly-supervised corpus. Second, we perform SFT, and propose an improved optimization strategy building upon RankLLaMA. Our experimental results on multiple benchmarks show that our approach outperforms previous methods in both in-domain and out-domain scenarios.", "year": 2023, "publicationdate": "2023-11-28", "externalids": {"DOI": "10.18653/v1/2024.findings-acl.706"}, "doi_lower": "10.18653/v1/2024.findings-acl.706"}
{"paper_id": 269005551, "title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models", "author_names": ["Zhiyuan Peng", "Xuyang Wu", "Qifan Wang", "Sravanthi Rajanala", "Yi Fang"], "venue": "arXiv.org", "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized in Large Language Models (LLMs) to improve the down-streaming tasks without the cost of fine-tuing the whole LLMs. Recent studies have shown how to effectively use PEFT for fine-tuning LLMs in ranking tasks with convincing performance; there are some limitations, including the learned prompt being fixed for different documents, overfitting to specific tasks, and low adaptation ability. In this paper, we introduce a query-dependent parameter efficient fine-tuning (Q-PEFT) approach for text reranking to leak the information of the true queries to LLMs and then make the generation of true queries from input documents much easier. Specifically, we utilize the query to extract the top-$k$ tokens from concatenated documents, serving as contextual clues. We further augment Q-PEFT by substituting the retrieval mechanism with a multi-head attention layer to achieve end-to-end training and cover all the tokens in the documents, guiding the LLMs to generate more document-specific synthetic queries, thereby further improving the reranking performance. Extensive experiments are conducted on four public datasets, demonstrating the effectiveness of our proposed approach.", "year": 2024, "publicationdate": "2024-04-06", "externalids": {"DOI": "10.48550/arXiv.2404.04522"}, "doi_lower": "10.48550/arxiv.2404.04522"}
{"paper_id": 265659512, "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models", "author_names": ["Xinyu Crystina Zhang", "Sebastian Hofstätter", "Patrick Lewis", "Raphael Tang", "Jimmy J. Lin"], "venue": "European Conference on Information Retrieval", "abstract": "Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art. However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general. In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT. Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4. Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers. Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources.", "year": 2023, "publicationdate": "2023-12-05", "externalids": {"DOI": "10.48550/arXiv.2312.02969"}, "doi_lower": "10.48550/arxiv.2312.02969"}
{"paper_id": 270688320, "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models", "author_names": ["Qi Liu", "Bo Wang", "Nan Wang", "Jiaxin Mao"], "venue": "The Web Conference", "abstract": "Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness. The code is available at https://github.com/liuqi6777/pe_rank", "year": 2024, "publicationdate": "2024-06-21", "externalids": {"DOI": "10.1145/3696410.3714554"}, "doi_lower": "10.1145/3696410.3714554"}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher Ré", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Yüksekgönül", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 269214676, "title": "Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers", "author_names": ["Fang Guo", "Wenyu Li", "Honglei Zhuang", "Yun Luo", "Yafu Li", "Le Yan", "Yue Zhang"], "venue": "arXiv.org", "abstract": "The most recent pointwise Large Language Model (LLM) rankers have achieved remarkable ranking results. However, these rankers are hindered by two major drawbacks: (1) they fail to follow a standardized comparison guidance during the ranking process, and (2) they struggle with comprehensive considerations when dealing with complicated passages. To address these shortcomings, we propose to build a ranker that generates ranking scores based on a set of criteria from various perspectives. These criteria are intended to direct each perspective in providing a distinct yet synergistic evaluation. Our research, which examines eight datasets from the BEIR benchmark demonstrates that incorporating this multi-perspective criteria ensemble approach markedly enhanced the performance of pointwise LLM rankers.", "year": 2024, "publicationdate": "2024-04-18", "externalids": {"DOI": "10.48550/arXiv.2404.11960"}, "doi_lower": "10.48550/arxiv.2404.11960"}
{"paper_id": 264406035, "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking", "author_names": ["Shengyao Zhuang", "Bing Liu", "B. Koopman", "G. Zuccon"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13243"}, "doi_lower": "10.48550/arxiv.2310.13243"}
{"paper_id": 270620088, "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers", "author_names": ["Shuoqi Sun", "Shengyao Zhuang", "Shuai Wang", "G. Zuccon"], "venue": "European Conference on Information Retrieval", "abstract": "We provide a systematic understanding of the impact of specific components and wordings used in prompts on the effectiveness of rankers based on zero-shot Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently been proposed. Among many aspects, methods differ across (1) the ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the use or not of role-definition (role-playing) and the actual words used to express this. It is currently unclear whether performance differences are due to the underlying ranking algorithm, or because of spurious factors such as better choice of words used in prompts. This confusion risks to undermine future research. Through our large-scale experimentation and analysis, we find that ranking algorithms do contribute to differences between methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even more importantly, the choice of prompt components and wordings affect the ranking. In fact, in our experiments, we find that, at times, these latter elements have more impact on the ranker's effectiveness than the actual ranking algorithms, and that differences among ranking methods become more blurred when prompt variations are considered.", "year": 2024, "publicationdate": "2024-06-20", "externalids": {"DOI": "10.1007/978-3-031-88711-6_12"}, "doi_lower": "10.1007/978-3-031-88711-6_12"}
{"paper_id": 270703185, "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task", "author_names": ["Wenhan Liu", "Yutao Zhu", "Zhicheng Dou"], "venue": "arXiv.org", "abstract": "Recently, there has been increasing interest in applying large language models (LLMs) as zero-shot passage rankers. However, few studies have explored how to select appropriate in-context demonstrations for the passage ranking task, which is the focus of this paper. Previous studies mainly use LLM's feedback to train a retriever for demonstration selection. These studies apply the LLM to score each demonstration independently, which ignores the dependencies between demonstrations (especially important in ranking task), leading to inferior performance of top-$k$ retrieved demonstrations. To mitigate this issue, we introduce a demonstration reranker to rerank the retrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL. However, generating training data for such reranker is quite challenging. On the one hand, different from demonstration retriever, the training samples of reranker need to incorporate demonstration dependencies. On the other hand, obtaining the gold ranking from the retrieved demonstrations is an NP-hard problem, which is hard to implement. To overcome these challenges, we propose a method to approximate the optimal demonstration list iteratively and utilize LLM to score demonstration lists of varying lengths. By doing so, the search space is greatly reduced and demonstration dependencies are considered. Based on these scored demonstration lists, we further design a list-pairwise training approach which compares a pair of lists that only differ in the last demonstration, to teach the reranker how to select the next demonstration given a previous sequence. In this paper, we propose a demonstration selection framework DemoRank for ranking task and conduct extensive experiments to prove its strong ability.", "year": 2024, "publicationdate": "2024-06-24", "externalids": {"DOI": "10.48550/arXiv.2406.16332"}, "doi_lower": "10.48550/arxiv.2406.16332"}
{"paper_id": 266166524, "title": "PaRaDe: Passage Ranking using Demonstrations with LLMs", "author_names": ["Andrew Drozdov", "Honglei Zhuang", "Zhuyun Dai", "Zhen Qin", "Razieh Rahimi", "Xuanhui Wang", "Dana Alon", "Mohit Iyyer", "Andrew McCallum", "Donald Metzler", "Kai Hui"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.950"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.950"}
{"paper_id": 274859423, "title": "Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models", "author_names": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Ziliang Zhao", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \\url{https://github.com/8421BCD/fullrank}.", "year": 2024, "publicationdate": "2024-12-19", "externalids": {"DOI": "10.48550/arXiv.2412.14574"}, "doi_lower": "10.48550/arxiv.2412.14574"}
{"paper_id": 277451550, "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "author_names": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \\textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\\%) while achieving even better effectiveness compared to using only the LLM listwise reranker.", "year": 2025, "publicationdate": "2025-03-30", "externalids": {"DOI": "10.48550/arXiv.2503.23427"}, "doi_lower": "10.48550/arxiv.2503.23427"}
{"paper_id": 258461030, "title": "Zero-Shot Listwise Document Reranking with a Large Language Model", "author_names": ["Xueguang Ma", "Xinyu Crystina Zhang", "Ronak Pradeep", "Jimmy J. Lin"], "venue": "arXiv.org", "abstract": "Supervised ranking methods based on bi-encoder or cross-encoder architectures have shown success in multi-stage text ranking tasks, but they require large amounts of relevance judgments as training data. In this work, we propose Listwise Reranker with a Large Language Model (LRL), which achieves strong reranking effectiveness without using any task-specific training data. Different from the existing pointwise ranking methods, where documents are scored independently and ranked according to the scores, LRL directly generates a reordered list of document identifiers given the candidate documents. Experiments on three TREC web search datasets demonstrate that LRL not only outperforms zero-shot pointwise methods when reranking first-stage retrieval results, but can also act as a final-stage reranker to improve the top-ranked results of a pointwise method for improved efficiency. Additionally, we apply our approach to subsets of MIRACL, a recent multilingual retrieval dataset, with results showing its potential to generalize across different languages.", "year": 2023, "publicationdate": "2023-05-03", "externalids": {"DOI": "10.48550/arXiv.2305.02156"}, "doi_lower": "10.48550/arxiv.2305.02156"}
{"paper_id": 270560753, "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy", "author_names": ["Yiqun Chen", "Qi Liu", "Yi Zhang", "Weiwei Sun", "Xinyu Ma", "Weiwei Yang", "Daiting Shi", "Jiaxin Mao", "Dawei Yin"], "venue": "The Web Conference", "abstract": "Large Language Models (LLMs) are increasingly employed in zero-shot documents ranking, yielding commendable results. However, several significant challenges still persist in LLMs for ranking: (1) LLMs are constrained by limited input length, precluding them from processing a large number of documents simultaneously; (2) The output document sequence is influenced by the input order of documents, resulting in inconsistent ranking outcomes; (3) Achieving a balance between cost and ranking performance is challenging. To tackle these issues, we introduce a novel documents ranking method called TourRank1. which is inspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1) overcome the limitation in input length and reduce the ranking latency by incorporating a multi-stage grouping strategy similar to the parallel group stage of sport tournaments; 2) improve the ranking performance and robustness to input orders by using a points system to ensemble multiple ranking results. We test TourRank with different LLMs on the TREC DL datasets and the BEIR benchmark. The experimental results demonstrate that TourRank delivers state-of-the-art performance at a modest cost.", "year": 2024, "publicationdate": "2024-06-17", "externalids": {"DOI": "10.1145/3696410.3714863"}, "doi_lower": "10.1145/3696410.3714863"}
{"paper_id": 269983636, "title": "Top-Down Partitioning for Efficient List-Wise Ranking", "author_names": ["Andrew Parry", "Sean MacAvaney", "Debasis Ganguly"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have significantly impacted many facets of natural language processing and information retrieval. Unlike previous encoder-based approaches, the enlarged context window of these generative models allows for ranking multiple documents at once, commonly called list-wise ranking. However, there are still limits to the number of documents that can be ranked in a single inference of the model, leading to the broad adoption of a sliding window approach to identify the k most relevant items in a ranked list. We argue that the sliding window approach is not well-suited for list-wise re-ranking because it (1) cannot be parallelized in its current form, (2) leads to redundant computational steps repeatedly re-scoring the best set of documents as it works its way up the initial ranking, and (3) prioritizes the lowest-ranked documents for scoring rather than the highest-ranked documents by taking a bottom-up approach. Motivated by these shortcomings and an initial study that shows list-wise rankers are biased towards relevant documents at the start of their context window, we propose a novel algorithm that partitions a ranking to depth k and processes documents top-down. Unlike sliding window approaches, our algorithm is inherently parallelizable due to the use of a pivot element, which can be compared to documents down to an arbitrary depth concurrently. In doing so, we reduce the number of expected inference calls by around 33% when ranking at depth 100 while matching the performance of prior approaches across multiple strong re-rankers.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14589"}, "doi_lower": "10.48550/arxiv.2405.14589"}
{"paper_id": 270619750, "title": "APEER : Automatic Prompt Engineering Enhances Large Language Model Reranking", "author_names": ["Can Jin", "Hongwu Peng", "Shiyu Zhao", "Zhenting Wang", "Wujiang Xu", "Ligong Han", "Jiahui Zhao", "Kai Zhong", "S. Rajasekaran", "Dimitris N. Metaxas"], "venue": "The Web Conference", "abstract": "Large Language Models (LLMs) have significantly enhanced Information Retrieval (IR) across various modules, such as reranking. Despite impressive performance, current zero-shot relevance ranking with LLMs heavily relies on human prompt engineering. Existing automatic prompt engineering algorithms primarily focus on language modeling and classification tasks, leaving the domain of IR, particularly reranking, underexplored. Directly applying current prompt engineering algorithms to relevance ranking is challenging due to the integration of query and long passage pairs in the input, where the ranking complexity surpasses classification tasks. To reduce human effort and unlock the potential of prompt optimization in reranking, we introduce a novel automatic prompt engineering algorithm named APEER. APEER iteratively generates refined prompts through feedback and preference optimization. Extensive experiments with four LLMs and ten datasets demonstrate the substantial performance improvement of APEER over existing state-of-the-art (SoTA) manual prompts. Furthermore, we find that the prompts generated by APEER exhibit better transferability across diverse tasks and LLMs.", "year": 2024, "publicationdate": "2024-06-20", "externalids": {"DOI": "10.1145/3701716.3717574"}, "doi_lower": "10.1145/3701716.3717574"}
{"paper_id": 259309299, "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting", "author_names": ["Zhen Qin", "R. Jagerman", "Kai Hui", "Honglei Zhuang", "Junru Wu", "Jiaming Shen", "Tianqi Liu", "Jialu Liu", "Donald Metzler", "Xuanhui Wang", "Michael Bendersky"], "venue": "NAACL-HLT", "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.", "year": 2023, "publicationdate": "2023-06-30", "externalids": {"DOI": "10.48550/arXiv.2306.17563"}, "doi_lower": "10.48550/arxiv.2306.17563"}
{"paper_id": 264146620, "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models", "author_names": ["Shengyao Zhuang", "Honglei Zhuang", "B. Koopman", "G. Zuccon"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at https://github.com/ielab/llm-rankers.", "year": 2023, "publicationdate": "2023-10-14", "externalids": {"DOI": "10.1145/3626772.3657813"}, "doi_lower": "10.1145/3626772.3657813"}
{"paper_id": 271923455, "title": "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking", "author_names": ["Jian Luo", "Xuanang Chen", "Ben He", "Le Sun"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pairwise Ranking Prompting (PRP) demon-001 strates impressive effectiveness in zero-shot 002 document re-ranking tasks with large language 003 models (LLMs). However, in the existing meth-004 ods, PRP only outputs the same label for the 005 comparison results of different confidence in-006 tervals without considering the uncertainty of 007 pairwise comparison, which implies an under-008 utilization of the generation probability infor-009 mation of LLMs. To bridge this gap, we pro-010 pose PRP-Graph, a novel pairwise re-ranking 011 approach, based on a refined scoring PRP unit 012 that exploits the output probabilities of target 013 labels to capture the degree of certainty of 014 the comparison results. Specifically, the PRP-015 Graph consists of two stages, namely ranking 016 graph construction and ranking graph aggre-017 gation. Extensive experiments conducted on 018 the BEIR benchmark demonstrate the superi-019 ority of our approach over existing PRP-based 020 methods. Comprehensive analysis reveals that 021 the PRP-Graph displays strong robustness to-022 wards the initial ranking order and delivers ex-023 ceptional re-ranking results with acceptable ef-024 ficiency. Our code and data will be available. 025", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.acl-long.313"}, "doi_lower": "10.18653/v1/2024.acl-long.313"}
{"paper_id": 259949767, "title": "ExaRanker: Synthetic Explanations Improve Neural Rankers", "author_names": ["Fernando Ferraretto", "Thiago Laitz", "R. Lotufo", "Rodrigo Nogueira"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Recent work has shown that incorporating explanations into the output generated by large language models (LLMs) can significantly enhance performance on a broad spectrum of reasoning tasks. Our study extends these findings by demonstrating the benefits of explanations for neural rankers. By utilizing LLMs such as GPT-3.5 to enrich retrieval datasets with explanations, we trained a sequence-to-sequence ranking model, dubbed ExaRanker, to generate relevance labels and explanations for query-document pairs. The ExaRanker model, finetuned on a limited number of examples and synthetic explanations, exhibits performance comparable to models finetuned on three times more examples, but without explanations. Moreover, incorporating explanations imposes no additional computational overhead into the reranking step and allows for on-demand explanation generation. The codebase and datasets used in this study will be available at https://github.com/unicamp-dl/ExaRanker", "year": 2023, "publicationdate": "2023-07-18", "externalids": {"DOI": "10.1145/3539618.3592067"}, "doi_lower": "10.1145/3539618.3592067"}
{"paper_id": 267617041, "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs", "author_names": ["Fernando Ferraretto", "Thiago Laitz", "R. Lotufo", "Rodrigo Nogueira"], "venue": "arXiv.org", "abstract": "ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0.6 nDCG@10 points in our study. To encourage further advancements by the research community, we have open-sourced both the code and datasets at https://github.com/unicamp-dl/ExaRanker.", "year": 2024, "publicationdate": "2024-02-09", "externalids": {"DOI": "10.48550/arXiv.2402.06334"}, "doi_lower": "10.48550/arxiv.2402.06334"}
{"paper_id": 255546584, "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers", "author_names": ["Leonid Boytsov", "Preksha Patel", "Vivek Sourabh", "Riddhi Nisar", "Sayan Kundu", "R. Ramanathan", "Eric Nyberg"], "venue": "Trans. Mach. Learn. Res.", "abstract": "We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/", "year": 2023, "publicationdate": "2023-01-08", "externalids": {"DOI": "10.48550/arXiv.2301.02998"}, "doi_lower": "10.48550/arxiv.2301.02998"}
{"paper_id": 266164016, "title": "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking", "author_names": ["Arian Askari", "Mohammad Aliannejadi", "Chuan Meng", "E. Kanoulas", "Suzan Verberne"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. Doc-Gen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance be-tween generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize Doc-Gen pipeline. Extensive experiments demonstrate that DocGen and DocGen-RL significantly outperform existing state-of-the-art data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area 1 .", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.623"}, "doi_lower": "10.18653/v1/2023.emnlp-main.623"}
{"paper_id": 262825475, "title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models", "author_names": ["Ronak Pradeep", "Sahel Sharifymoghaddam", "Jimmy Lin"], "venue": "arXiv.org", "abstract": "Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at https://github.com/castorini/rank_llm.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.48550/arXiv.2309.15088"}, "doi_lower": "10.48550/arxiv.2309.15088"}
{"paper_id": 265659387, "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!", "author_names": ["Ronak Pradeep", "Sahel Sharifymoghaddam", "Jimmy J. Lin"], "venue": "arXiv.org", "abstract": "In information retrieval, proprietary large language models (LLMs) such as GPT-4 and open-source counterparts such as LLaMA and Vicuna have played a vital role in reranking. However, the gap between open-source and closed models persists, with reliance on proprietary, non-transparent models constraining reproducibility. Addressing this gap, we introduce RankZephyr, a state-of-the-art, open-source LLM for listwise zero-shot reranking. RankZephyr not only bridges the effectiveness gap with GPT-4 but in some cases surpasses the proprietary model. Our comprehensive evaluations across several datasets (TREC Deep Learning Tracks; NEWS and COVID from BEIR) showcase this ability. RankZephyr benefits from strategic training choices and is resilient against variations in initial document ordering and the number of documents reranked. Additionally, our model outperforms GPT-4 on the NovelEval test set, comprising queries and passages past its training period, which addresses concerns about data contamination. To foster further research in this rapidly evolving field, we provide all code necessary to reproduce our results at https://github.com/castorini/rank_llm.", "year": 2023, "publicationdate": "2023-12-05", "externalids": {"DOI": "10.48550/arXiv.2312.02724"}, "doi_lower": "10.48550/arxiv.2312.02724"}
{"paper_id": 265019321, "title": "Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers", "author_names": ["Weiwei Sun", "Zheng Chen", "Xinyu Ma", "Lingyong Yan", "Shuaiqiang Wang", "Pengjie Ren", "Zhumin Chen", "Dawei Yin", "Zhaochun Ren"], "venue": "arXiv.org", "abstract": "Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performance of existing supervised methods like monoT5 and is on par with the state-of-the-art zero-shot methods. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.", "year": 2023, "publicationdate": "2023-11-02", "externalids": {"DOI": "10.48550/arXiv.2311.01555"}, "doi_lower": "10.48550/arxiv.2311.01555"}
{"paper_id": 276580242, "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval", "author_names": ["Orion Weller", "Kathryn Ricci", "Eugene Yang", "Andrew Yates", "Dawn J. Lawrie", "Benjamin Van Durme"], "venue": "arXiv.org", "abstract": "We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.", "year": 2025, "publicationdate": "2025-02-25", "externalids": {"DOI": "10.48550/arXiv.2502.18418"}, "doi_lower": "10.48550/arxiv.2502.18418"}
{"paper_id": 278769246, "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "author_names": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn J. Lawrie"], "venue": "arXiv.org", "abstract": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to make slow but effective rerankers efficient enough at query time by reducing the number of comparisons. Recent works in neural rerankers take advantage of large language models for their capability in reasoning between queries and passages and have achieved state-of-the-art retrieval effectiveness. However, such rerankers are resource-intensive, even after heavy optimization. In this work, we introduce Rank-K, a listwise passage reranking model that leverages the reasoning capability of the reasoning language model at query time that provides test time scalability to serve hard queries. We show that Rank-K improves retrieval effectiveness by 23\\% over the RankZephyr, the state-of-the-art listwise reranker, when reranking a BM25 initial ranked list and 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is inherently a multilingual model, we found that it ranks passages based on queries in different languages as effectively as it does in monolingual retrieval.", "year": 2025, "publicationdate": "2025-05-20", "externalids": {"DOI": "10.48550/arXiv.2505.14432"}, "doi_lower": "10.48550/arxiv.2505.14432"}
{"paper_id": 278910905, "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning", "author_names": ["Le Zhang", "Bo Wang", "Xipeng Qiu", "Siva Reddy", "Aishwarya Agrawal"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.", "year": 2025, "publicationdate": "2025-05-26", "externalids": {"DOI": "10.48550/arXiv.2505.20046"}, "doi_lower": "10.48550/arxiv.2505.20046"}
{"paper_id": 276903058, "title": "Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via Reinforcement Learning", "author_names": ["Shengyao Zhuang", "Xueguang Ma", "B. Koopman", "Jimmy Lin", "G. Zuccon"], "venue": "arXiv.org", "abstract": "In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a reinforcement learning algorithm along with only a small set of relevance labels (without any reasoning supervision) to enhance the reasoning ability of LLM-based rerankers. Our hypothesis is that adding reasoning capabilities to the rerankers can improve their relevance assessement and ranking capabilities. Our experiments on the TREC DL and BRIGHT datasets show that Rank-R1 is highly effective, especially for complex queries. In particular, we find that Rank-R1 achieves effectiveness on in-domain datasets at par with that of supervised fine-tuning methods, but utilizing only 18\\% of the training data used by the fine-tuning methods. We also find that the model largely outperforms zero-shot and supervised fine-tuning when applied to out-of-domain datasets featuring complex queries, especially when a 14B-size model is used. Finally, we qualitatively observe that Rank-R1's reasoning process improves the explainability of the ranking results, opening new opportunities for search engine results presentation and fruition.", "year": 2025, "publicationdate": "2025-03-08", "externalids": {"DOI": "10.48550/arXiv.2503.06034"}, "doi_lower": "10.48550/arxiv.2503.06034"}
{"paper_id": 280641833, "title": "TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking", "author_names": ["Yongqi Fan", "Xiaoyang Chen", "Dezhi Ye", "Jie Liu", "Haijin Liang", "Jin Ma", "Ben He", "Yingfei Sun", "Tong Ruan"], "venue": "arXiv.org", "abstract": "Reasoning-intensive ranking models built on Large Language Models (LLMs) have made notable progress. However, existing approaches often rely on large-scale LLMs and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational cost and latency that limit real-world use. To address this, we propose \\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale LLMs. To improve ranking performance, TFRank effectively integrates CoT data, fine-grained score supervision, and multi-task training. Furthermore, it achieves an efficient ``\\textbf{T}hink-\\textbf{F}ree\"reasoning capability by employing a ``think-mode switch''and pointwise format constraints. Specifically, this allows the model to leverage explicit reasoning during training while delivering precise relevance scores for complex queries at inference without generating any reasoning chains. Experiments show that TFRank achieves performance comparable to models with four times more parameters on the BRIGHT benchmark and demonstrates strong competitiveness on the BEIR benchmark. Further analysis shows that TFRank achieves an effective balance between performance and efficiency, providing a practical solution for integrating advanced reasoning into real-world systems. Our code and data are released in the repository: https://github.com/JOHNNY-fans/TFRank.", "year": 2025, "publicationdate": "2025-08-13", "externalids": {"DOI": "10.48550/arXiv.2508.09539"}, "doi_lower": "10.48550/arxiv.2508.09539"}
{"paper_id": 11168734, "title": "Learning to rank using gradient descent", "author_names": ["C. Burges", "T. Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Gregory N. Hullender"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2005, "publicationdate": "2005-08-07", "externalids": {"DOI": "10.1145/1102351.1102363"}, "doi_lower": "10.1145/1102351.1102363"}
{"paper_id": 258546764, "title": "Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing", "author_names": ["J. Baktash", "Mursal Dawodi"], "venue": "Journal of Electrical Electronics Engineering", "abstract": "Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation language model in the GPT series, developed by OpenAI, which promises significant advancements in the field of natural language processing (NLP). In this research article, we have discussed the features of GPT-4, its potential applications, and the challenges that it might face. We have also compared GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one trillion), better multilingual capabilities, improved contextual understanding, and reasoning capabilities than GPT-3. Some of the potential applications of GPT-4 include chatbots, personal assistants, language translation, text summarization, and question-answering. However, GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.", "year": 2023, "publicationdate": "2023-05-04", "externalids": {"DOI": "10.48550/arXiv.2305.03195"}, "doi_lower": "10.48550/arxiv.2305.03195"}
{"paper_id": 272911224, "title": "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "author_names": ["Nilanjan Sinhababu", "Andrew Parry", "Debasis Ganguly", "Debasis Samanta", "Pabitra Mitra"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "A supervised ranking model, despite its advantage of being effective, usually involves complex processing - typically multiple stages of task-specific pre-training and fine-tuning. This has motivated researchers to explore simpler pipelines leveraging large language models (LLMs) that are capable of working in a zero-shot manner. However, since zero-shot inference does not make use of a training set of pairs of queries and their relevant documents, its performance is mostly worse than that of supervised models, which are trained on such example pairs. Motivated by the existing findings that training examples generally improve zero-shot performance, in our work, we explore if this also applies to ranking models. More specifically, given a query and a pair of documents, the preference prediction task is improved by augmenting examples of preferences for similar queries from a training set. Our proposed pairwise few-shot ranker demonstrates consistent improvements over the zero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset) retrieval benchmarks. Our method also achieves a close performance to that of a supervised model without requiring any complex training pipeline.", "year": 2024, "publicationdate": "2024-09-26", "externalids": {"DOI": "10.18653/v1/2024.findings-emnlp.720"}, "doi_lower": "10.18653/v1/2024.findings-emnlp.720"}
{"paper_id": 271270735, "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval", "author_names": ["Hongjin Su", "Howard Yen", "Mengzhou Xia", "Weijia Shi", "Niklas Muennighoff", "Han-yu Wang", "Haisu Liu", "Quan Shi", "Zachary S. Siegel", "Michael Tang", "Ruoxi Sun", "Jinsung Yoon", "Sercan Ö. Arik", "Danqi Chen", "Tao Yu"], "venue": "International Conference on Learning Representations", "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,384 real-world queries spanning diverse domains, such as economics, psychology, mathematics, and coding. These queries are drawn from naturally occurring and carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023) SFR-Embedding-Mistral (Meng et al., 2024), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.3 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question-answering performance. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings.", "year": 2024, "publicationdate": "2024-07-16", "externalids": {"DOI": "10.48550/arXiv.2407.12883"}, "doi_lower": "10.48550/arxiv.2407.12883"}
{"paper_id": 278769828, "title": "R2MED: A Benchmark for Reasoning-Driven Medical Retrieval", "author_names": ["Lei Li", "Xiao Zhou", "Zheng Liu"], "venue": "arXiv.org", "abstract": "Current medical retrieval benchmarks primarily emphasize lexical or shallow semantic similarity, overlooking the reasoning-intensive demands that are central to clinical decision-making. In practice, physicians often retrieve authoritative medical evidence to support diagnostic hypotheses. Such evidence typically aligns with an inferred diagnosis rather than the surface form of a patient's symptoms, leading to low lexical or semantic overlap between queries and relevant documents. To address this gap, we introduce R2MED, the first benchmark explicitly designed for reasoning-driven medical retrieval. It comprises 876 queries spanning three tasks: Q&A reference retrieval, clinical evidence retrieval, and clinical case retrieval. These tasks are drawn from five representative medical scenarios and twelve body systems, capturing the complexity and diversity of real-world medical information needs. We evaluate 15 widely-used retrieval systems on R2MED and find that even the best model achieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical re-ranking and generation-augmented retrieval methods offer only modest improvements. Although large reasoning models improve performance via intermediate inference generation, the best results still peak at 41.4 nDCG@10. These findings underscore a substantial gap between current retrieval techniques and the reasoning demands of real clinical tasks. We release R2MED as a challenging benchmark to foster the development of next-generation medical retrieval systems with enhanced reasoning capabilities. Data and code are available at https://github.com/R2MED/R2MED", "year": 2025, "publicationdate": "2025-05-20", "externalids": {"DOI": "10.48550/arXiv.2505.14558"}, "doi_lower": "10.48550/arxiv.2505.14558"}
{"paper_id": 267740534, "title": "EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models", "author_names": ["Muhammad Shihab Rashid", "Jannat Ara Meem", "Yue Dong", "Vagelis Hristidis"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines.", "year": 2024, "publicationdate": "2024-02-16", "externalids": {"DOI": "10.48550/arXiv.2402.10866"}, "doi_lower": "10.48550/arxiv.2402.10866"}
{"paper_id": 273098593, "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers", "author_names": ["Shijie Chen", "Bernal Jim'enez Guti'errez", "Yu Su"], "venue": "International Conference on Learning Representations", "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more directly leverage such signals, we propose in-context re-ranking (ICR), a novel method that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two ($O(1)$) forward passes to re-rank $N$ documents, making it substantially more efficient than generative re-ranking methods that require at least $O(N)$ forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR's performance is specially strong on tasks that require more complex re-ranking signals. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.", "year": 2024, "publicationdate": "2024-10-03", "externalids": {"DOI": "10.48550/arXiv.2410.02642"}, "doi_lower": "10.48550/arxiv.2410.02642"}
{"paper_id": 269449617, "title": "Ranked List Truncation for Large Language Model-based Re-Ranking", "author_names": ["Chuan Meng", "Negar Arabzadeh", "Arian Askari", "Mohammad Aliannejadi", "M. de Rijke"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "We study ranked list truncation (RLT) from a novel retrieve-then-re-rank perspective, where we optimize re-ranking by truncating the retrieved list (i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can improve re-ranking efficiency by sending variable-length candidate lists to a re-ranker on a per-query basis. It also has the potential to improve re-ranking effectiveness. Despite its importance, there is limited research into applying RLT methods to this new perspective. To address this research gap, we reproduce existing RLT methods in the context of re-ranking, especially newly emerged large language model (LLM)-based re-ranking. In particular, we examine to what extent established findings on RLT for retrieval are generalizable to the \"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT methods in the context of LLM-based re-ranking with lexical first-stage retrieval, (ii) investigating the impact of different types of first-stage retrievers on RLT methods, and (iii) investigating the impact of different types of re-rankers on RLT methods. We perform experiments on the TREC 2019 and 2020 deep learning tracks, investigating 8 RLT methods for pipelines involving 3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the context of re-ranking.", "year": 2024, "publicationdate": "2024-04-28", "externalids": {"DOI": "10.1145/3626772.3657864"}, "doi_lower": "10.1145/3626772.3657864"}
{"paper_id": 51880268, "title": "Retrieval of the Best Counterargument without Prior Topic Knowledge", "author_names": ["Henning Wachsmuth", "S. Syed", "Benno Stein"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments’ premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.18653/v1/P18-1023"}, "doi_lower": "10.18653/v1/p18-1023"}
{"paper_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "author_names": ["Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang"], "venue": "International Conference on Machine Learning", "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.", "year": 2020, "publicationdate": "2020-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "author_names": ["Weijia Shi", "Sewon Min", "Michihiro Yasunaga", "Minjoon Seo", "Rich James", "M. Lewis", "Luke Zettlemoyer", "Wen-tau Yih"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12652"}, "doi_lower": "10.48550/arxiv.2301.12652"}
{"paper_id": 251371732, "title": "Few-shot Learning with Retrieval Augmented Language Models", "author_names": ["Gautier Izacard", "Patrick Lewis", "M. Lomeli", "Lucas Hosseini", "F. Petroni", "Timo Schick", "Jane A. Yu", "Armand Joulin", "Sebastian Riedel", "Edouard Grave"], "venue": "Journal of machine learning research", "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {}, "doi_lower": null}
{"paper_id": 255372320, "title": "Rethinking with Retrieval: Faithful Large Language Model Inference", "author_names": ["Hangfeng He", "Hongming Zhang", "D. Roth"], "venue": "arXiv.org", "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.", "year": 2022, "publicationdate": "2022-12-31", "externalids": {"DOI": "10.48550/arXiv.2301.00303"}, "doi_lower": "10.48550/arxiv.2301.00303"}
{"paper_id": 265212816, "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "author_names": ["W. Yu", "Hongming Zhang", "Xiaoman Pan", "Kaixin Ma", "Hongwei Wang", "Dong Yu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with “unknown” to avoid hallucination. In this paper, we introduces Chain-of-Note (CoN), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with CoN, outperforms the Chain-of-Thought approach. Besides, we utilized GPT-4 to create 10K CoN data, subsequently trained on smaller models like OPT and LLaMa-2. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CoN significantly outperform standard fine-tuned RALMs.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.48550/arXiv.2311.09210"}, "doi_lower": "10.48550/arxiv.2311.09210"}
{"paper_id": 258866037, "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy", "author_names": ["Zhihong Shao", "Yeyun Gong", "Yelong Shen", "Minlie Huang", "Nan Duan", "Weizhu Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15294"}, "doi_lower": "10.48550/arxiv.2305.15294"}
{"paper_id": 254877499, "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "author_names": ["H. Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10509"}, "doi_lower": "10.48550/arxiv.2212.10509"}
{"paper_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "author_names": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "venue": "International Conference on Learning Representations", "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {}, "doi_lower": null}
{"paper_id": 259108339, "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit", "author_names": ["Jiongnan Liu", "Jiajie Jin", "Zihan Wang", "Jiehan Cheng", "Zhicheng Dou", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05212"}, "doi_lower": "10.48550/arxiv.2306.05212"}
{"paper_id": 259360590, "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning", "author_names": ["Xiaozhong Lyu", "Stefan Grafberger", "Samantha Biegel", "Shaopeng Wei", "Meng Cao", "Sebastian Schelter", "Ce Zhang"], "venue": "arXiv.org", "abstract": "Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.48550/arXiv.2307.03027"}, "doi_lower": "10.48550/arxiv.2307.03027"}
{"paper_id": 270562583, "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation", "author_names": ["Shuting Wang", "Xin Xu", "Mang Wang", "Weipeng Chen", "Yutao Zhu", "Zhicheng Dou"], "venue": "International Conference on Computational Linguistics", "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.", "year": 2024, "publicationdate": "2024-06-18", "externalids": {"DOI": "10.48550/arXiv.2406.12566"}, "doi_lower": "10.48550/arxiv.2406.12566"}
{"paper_id": 278782226, "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization", "author_names": ["Yutao Zhu", "Jiajie Jin", "Hongjin Qian", "Zheng Liu", "Zhicheng Dou", "Ji-Rong Wen"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing studies have optimized retrieval-augmented generation (RAG) across various sub-tasks, such as query understanding and retrieval refinement, but integrating these optimizations into a unified framework remains challenging. To tackle this problem, this work proposes RoleRAG, a unified RAG framework that achieves efficient multi-task processing through role-specific token optimization. RoleRAG comprises six modules, each handling a specific sub-task within the RAG process. Additionally, we introduce a query graph to represent the decomposition of the query, which can be dynamically resolved according to the decomposing state. All modules are driven by the same underlying LLM, distinguished by task-specific role tokens that are individually optimized. This design allows RoleRAG to dynamically activate different modules within a single LLM instance, thereby streamlining deployment and reducing resource consumption. Experimental results on five open-domain question-answering datasets demonstrate the effectiveness, generalizability, and flexibility of our framework.", "year": 2025, "publicationdate": "2025-05-21", "externalids": {"DOI": "10.48550/arXiv.2505.15444"}, "doi_lower": "10.48550/arxiv.2505.15444"}
{"paper_id": 270688725, "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs", "author_names": ["Ziyan Jiang", "Xueguang Ma", "Wenhu Chen"], "venue": "arXiv.org", "abstract": "In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. In the two Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents. By increasing the unit size, we significantly reduce the total number of units. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5% on MultiFieldQA-en. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.", "year": 2024, "publicationdate": "2024-06-21", "externalids": {"DOI": "10.48550/arXiv.2406.15319"}, "doi_lower": "10.48550/arxiv.2406.15319"}
{"paper_id": 258865710, "title": "Enabling Large Language Models to Generate Text with Citations", "author_names": ["Tianyu Gao", "Howard Yen", "Jiatong Yu", "Danqi Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14627"}, "doi_lower": "10.48550/arxiv.2305.14627"}
{"paper_id": 267770471, "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning", "author_names": ["Zhipeng Xu", "Zhenghao Liu", "Yibin Liu", "Chenyan Xiong", "Yukun Yan", "Shuo Wang", "Shi Yu", "Zhiyuan Liu", "Ge Yu"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2402.13547"}, "doi_lower": "10.48550/arxiv.2402.13547"}
{"paper_id": 270062974, "title": "Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning", "author_names": ["Xun Liang", "Simin Niu", "Zhiyu Li", "Sensen Zhang", "Shichao Song", "Hanyu Wang", "Jiawei Yang", "Feiyu Xiong", "Bo Tang", "Chenyang Xi"], "venue": "arXiv.org", "abstract": "Retrieval-Augmented Generation (RAG) offers a cost-effective approach to injecting real-time knowledge into large language models (LLMs). Nevertheless, constructing and validating high-quality knowledge repositories require considerable effort. We propose a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students by providing them with abundant raw reading materials and encouraging them to engage in autonomous reading to record factual information in their own words. The resulting concise, well-organized mental indices are interconnected through common topics or complementary facts to form a pseudo-graph database. During the retrieval phase, PG-RAG mimics the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts. Adhering to the principle of the path taken by many is the best, it integrates highly corroborated fact paths to provide a structured and refined sub-graph assisting LLMs. We validated PG-RAG on three specialized question-answering datasets. In single-document tasks, PG-RAG significantly outperformed the current best baseline, KGP-LLaMA, across all key evaluation metrics, with an average overall performance improvement of 11.6%. Specifically, its BLEU score increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In multi-document scenarios, the average metrics of PG-RAG were at least 2.35% higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed stable improvements of around 7.55% and 12.75%, respectively. Our code: https://github.com/IAAR-Shanghai/PGRAG.", "year": 2024, "publicationdate": "2024-05-27", "externalids": {"DOI": "10.48550/arXiv.2405.16933"}, "doi_lower": "10.48550/arxiv.2405.16933"}
{"paper_id": 268032752, "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "author_names": ["Yuhao Wang", "Ruiyang Ren", "Junyi Li", "Wayne Xin Zhao", "Jing Liu", "Ji-Rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.", "year": 2024, "publicationdate": "2024-02-27", "externalids": {"DOI": "10.48550/arXiv.2402.17497"}, "doi_lower": "10.48550/arxiv.2402.17497"}
{"paper_id": 258865283, "title": "SAIL: Search-Augmented Instruction Learning", "author_names": ["Hongyin Luo", "Yung-Sung Chuang", "Yuan Gong", "Tianhua Zhang", "Yoon Kim", "Xixin Wu", "D. Fox", "H. Meng", "James R. Glass"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15225"}, "doi_lower": "10.48550/arxiv.2305.15225"}
{"paper_id": 263605962, "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "author_names": ["Xi Victoria Lin", "Xilun Chen", "Mingda Chen", "Weijia Shi", "Maria Lomeli", "Rich James", "Pedro Rodriguez", "Jacob Kahn", "Gergely Szilvasy", "Mike Lewis", "Luke S. Zettlemoyer", "Scott Yih"], "venue": "International Conference on Learning Representations", "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01352"}, "doi_lower": "10.48550/arxiv.2310.01352"}
{"paper_id": 268041643, "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation", "author_names": ["Shicheng Xu", "Liang Pang", "Mo Yu", "Fandong Meng", "Huawei Shen", "Xueqi Cheng", "Jie Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.", "year": 2024, "publicationdate": "2024-02-28", "externalids": {"DOI": "10.48550/arXiv.2402.18150"}, "doi_lower": "10.48550/arxiv.2402.18150"}
{"paper_id": 270123034, "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models", "author_names": ["Yutao Zhu", "Zhaoheng Huang", "Zhicheng Dou", "Ji-Rong Wen"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.", "year": 2024, "publicationdate": "2024-05-30", "externalids": {"DOI": "10.48550/arXiv.2405.19670"}, "doi_lower": "10.48550/arxiv.2405.19670"}
{"paper_id": 270620574, "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "author_names": ["Fuda Ye", "Shuangyin Li", "Yongqi Zhang", "Lei Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap.", "year": 2024, "publicationdate": "2024-06-19", "externalids": {"DOI": "10.48550/arXiv.2406.13249"}, "doi_lower": "10.48550/arxiv.2406.13249"}
{"paper_id": 270199429, "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "author_names": ["Feiteng Fang", "Yuelin Bai", "Shiwen Ni", "Min Yang", "Xiaojun Chen", "Ruifeng Xu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.", "year": 2024, "publicationdate": "2024-05-31", "externalids": {"DOI": "10.48550/arXiv.2405.20978"}, "doi_lower": "10.48550/arxiv.2405.20978"}
{"paper_id": 270067727, "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator", "author_names": ["Junda Zhu", "Lingyong Yan", "Haibo Shi", "Dawei Yin", "Lei Sha"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today’s Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system **(ATM)**. The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.", "year": 2024, "publicationdate": "2024-05-28", "externalids": {"DOI": "10.48550/arXiv.2405.18111"}, "doi_lower": "10.48550/arxiv.2405.18111"}
{"paper_id": 258841029, "title": "Improving Language Models via Plug-and-Play Retrieval Feedback", "author_names": ["W. Yu", "Zhihan Zhang", "Zhenwen Liang", "Meng Jiang", "Ashish Sabharwal"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. Moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. ReFeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner. Experiments on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14002"}, "doi_lower": "10.48550/arxiv.2305.14002"}
{"paper_id": 263830898, "title": "Retrieval-Generation Synergy Augmented Large Language Models", "author_names": ["Zhangyin Feng", "Xiaocheng Feng", "Dezhi Zhao", "Maojin Yang", "Bing Qin"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "abstract": "Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.1109/ICASSP48485.2024.10448015"}, "doi_lower": "10.1109/icassp48485.2024.10448015"}
{"paper_id": 270688739, "title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering", "author_names": ["Zhengliang Shi", "Shuo Zhang", "Weiwei Sun", "Shen Gao", "Pengjie Ren", "Zhumin Chen", "Zhaochun Ren"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method.", "year": 2024, "publicationdate": "2024-06-21", "externalids": {"DOI": "10.48550/arXiv.2406.14891"}, "doi_lower": "10.48550/arxiv.2406.14891"}
{"paper_id": 250451161, "title": "Language Models (Mostly) Know What They Know", "author_names": ["Saurav Kadavath", "Tom Conerly", "Amanda Askell", "T. Henighan", "Dawn Drain", "Ethan Perez", "Nicholas Schiefer", "Z. Dodds", "Nova Dassarma", "Eli Tran-Johnson", "Scott Johnston", "S. El-Showk", "Andy Jones", "Nelson Elhage", "Tristan Hume", "Anna Chen", "Yuntao Bai", "Sam Bowman", "Stanislav Fort", "Deep Ganguli", "Danny Hernandez", "Josh Jacobson", "John Kernion", "Shauna Kravec", "Liane Lovitt", "Kamal Ndousse", "Catherine Olsson", "Sam Ringer", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Nicholas Joseph", "Benjamin Mann", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability\"P(True)\"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict\"P(IK)\", the probability that\"I know\"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.", "year": 2022, "publicationdate": "2022-07-11", "externalids": {"DOI": "10.48550/arXiv.2207.05221"}, "doi_lower": "10.48550/arxiv.2207.05221"}
{"paper_id": 252762102, "title": "Measuring and Narrowing the Compositionality Gap in Language Models", "author_names": ["Ofir Press", "Muru Zhang", "Sewon Min", "Ludwig Schmidt", "Noah A. Smith", "M. Lewis"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.", "year": 2022, "publicationdate": "2022-10-07", "externalids": {"DOI": "10.48550/arXiv.2210.03350"}, "doi_lower": "10.48550/arxiv.2210.03350"}
{"paper_id": 255186555, "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP", "author_names": ["O. Khattab", "Keshav Santhanam", "Xiang Lisa Li", "David Leo Wright Hall", "Percy Liang", "Christopher Potts", "M. Zaharia"], "venue": "arXiv.org", "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp", "year": 2022, "publicationdate": "2022-12-28", "externalids": {"DOI": "10.48550/arXiv.2212.14024"}, "doi_lower": "10.48550/arxiv.2212.14024"}
{"paper_id": 270514562, "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers", "author_names": ["Myeonghwa Lee", "Seonho An", "Min-Soo Kim"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d_{best}, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.", "year": 2024, "publicationdate": "2024-06-18", "externalids": {"DOI": "10.48550/arXiv.2406.12430"}, "doi_lower": "10.48550/arxiv.2406.12430"}
{"paper_id": 258309779, "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought", "author_names": ["Ori Yoran", "Tomer Wolfson", "Ben Bogin", "Uri Katz", "Daniel Deutch", "Jonathan Berant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.", "year": 2023, "publicationdate": "2023-04-25", "externalids": {"DOI": "10.48550/arXiv.2304.13007"}, "doi_lower": "10.48550/arxiv.2304.13007"}
{"paper_id": 261531532, "title": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs", "author_names": ["Md. Adnan Arefeen", "Biplob Debnath", "S. Chakradhar"], "venue": "Natural Language Processing Journal", "abstract": "Question-answering (QA) is a significant application of Large Language Models (LLMs), shaping chatbot capabilities across healthcare, education, and customer service. However, widespread LLM integration presents a challenge for small businesses due to the high expenses of LLM API usage. Costs rise rapidly when domain-specific data (context) is used alongside queries for accurate domain-specific LLM responses. One option is to summarize the context by using LLMs and reduce the context. However, this can also filter out useful information that is necessary to answer some domain-specific queries. In this paper, we shift from human-oriented summarizers to AI model-friendly summaries. Our approach, LeanContext, efficiently extracts $k$ key sentences from the context that are closely aligned with the query. The choice of $k$ is neither static nor random; we introduce a reinforcement learning technique that dynamically determines $k$ based on the query and context. The rest of the less important sentences are reduced using a free open source text reduction method. We evaluate LeanContext against several recent query-aware and query-unaware context reduction approaches on prominent datasets (arxiv papers and BBC news articles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's ROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline that retains the entire context (no summarization). Additionally, if free pretrained LLM-based summarizers are used to reduce context (into human consumable summaries), LeanContext can further modify the reduced context to enhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.", "year": 2023, "publicationdate": "2023-09-02", "externalids": {"DOI": "10.48550/arXiv.2309.00841"}, "doi_lower": "10.48550/arxiv.2309.00841"}
{"paper_id": 265157538, "title": "Learning to Filter Context for Retrieval-Augmented Generation", "author_names": ["Zhiruo Wang", "Jun Araki", "Zhengbao Jiang", "Md. Rizwan Parvez", "Graham Neubig"], "venue": "arXiv.org", "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08377"}, "doi_lower": "10.48550/arxiv.2311.08377"}
{"paper_id": 264439519, "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction", "author_names": ["Junyi Liu", "Liangzhi Li", "Tong Xiang", "Bowen Wang", "Yiming Qian"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.", "year": 2023, "publicationdate": "2023-10-24", "externalids": {"DOI": "10.48550/arXiv.2310.15556"}, "doi_lower": "10.48550/arxiv.2310.15556"}
{"paper_id": 264590451, "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter", "author_names": ["Haoyan Yang", "Zhitao Li", "Yong Zhang", "Jianzong Wang", "Ning Cheng", "Ming Li", "Jing Xiao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA's effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.18347"}, "doi_lower": "10.48550/arxiv.2310.18347"}
{"paper_id": 267751485, "title": "BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence", "author_names": ["Jiajie Jin", "Yutao Zhu", "Yujia Zhou", "Zhicheng Dou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.", "year": 2024, "publicationdate": "2024-02-19", "externalids": {"DOI": "10.48550/arXiv.2402.12174"}, "doi_lower": "10.48550/arxiv.2402.12174"}
{"paper_id": 259360665, "title": "Lost in the Middle: How Language Models Use Long Contexts", "author_names": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "F. Petroni", "Percy Liang"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1162/tacl_a_00638"}, "doi_lower": "10.1162/tacl_a_00638"}
{"paper_id": 259991467, "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation", "author_names": ["Ruiyang Ren", "Yuhao Wang", "Yingqi Qu", "Wayne Xin Zhao", "J. Liu", "Hao Tian", "Huaqin Wu", "Ji-rong Wen", "Haifeng Wang"], "venue": "International Conference on Computational Linguistics", "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.11019"}, "doi_lower": "10.48550/arxiv.2307.11019"}
{"paper_id": 256826924, "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models", "author_names": ["Renat Aksitov", "Chung-Ching Chang", "D. Reitter", "Siamak Shakeri", "Yun-Hsuan Sung"], "venue": "arXiv.org", "abstract": "Despite recent progress, it has been difficult to prevent semantic hallucinations in generative Large Language Models. One common solution to this is augmenting LLMs with a retrieval system and making sure that the generated output is attributable to the retrieved information. Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency. Can scaling language models help? Here we examine the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings. Our experiments were implemented with a set of auto-metrics that are aligned with human preferences. They were used to evaluate a large set of generations, produced under varying parameters of LLMs and supplied context. We show that larger models tend to do much better in both fluency and attribution, and that (naively) using top-k retrieval versus top-1 retrieval improves attribution but hurts fluency. We next propose a recipe that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks.", "year": 2023, "publicationdate": "2023-02-11", "externalids": {"DOI": "10.48550/arXiv.2302.05578"}, "doi_lower": "10.48550/arxiv.2302.05578"}
{"paper_id": 254877603, "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories", "author_names": ["Alex Troy Mallen", "Akari Asai", "Victor Zhong", "Rajarshi Das", "Hannaneh Hajishirzi", "Daniel Khashabi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.18653/v1/2023.acl-long.546"}, "doi_lower": "10.18653/v1/2023.acl-long.546"}
{"paper_id": 278775002, "title": "Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions", "author_names": ["Hongru Wang", "Boyang Xue", "Baohang Zhou", "Tianhua Zhang", "Cunxiang Wang", "Huimin Wang", "Guanhua Chen", "Kam-Fai Wong"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2025.naacl-long.331"}, "doi_lower": "10.18653/v1/2025.naacl-long.331"}
{"paper_id": 267770347, "title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models", "author_names": ["Seiji Maekawa", "Hayate Iso", "Sairam Gurajada", "Nikita Bhutani"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity. Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.", "year": 2024, "publicationdate": "2024-02-21", "externalids": {"DOI": "10.48550/arXiv.2402.13492"}, "doi_lower": "10.48550/arxiv.2402.13492"}
{"paper_id": 263828724, "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models", "author_names": ["Yile Wang", "Peng Li", "Maosong Sun", "Yang Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.48550/arXiv.2310.05002"}, "doi_lower": "10.48550/arxiv.2310.05002"}
{"paper_id": 267750726, "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs", "author_names": ["Jiejun Tan", "Zhicheng Dou", "Yutao Zhu", "Peidong Guo", "Kun Fang", "Ji-Rong Wen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.", "year": 2024, "publicationdate": "2024-02-19", "externalids": {"DOI": "10.48550/arXiv.2402.12052"}, "doi_lower": "10.48550/arxiv.2402.12052"}
{"paper_id": 267782658, "title": "Tug-of-War between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models", "author_names": ["Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Xiaojian Jiang", "Jiexin Xu", "Qiuxia Li", "Jun Zhao"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently. Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory. To solve the challenge of knowledge conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding (CD2) to better calibrate the model’s confidence. Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.", "year": 2024, "publicationdate": "2024-02-22", "externalids": {"DOI": "10.48550/arXiv.2402.14409"}, "doi_lower": "10.48550/arxiv.2402.14409"}
{"paper_id": 269293062, "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "author_names": ["Sukmin Cho", "Soyeong Jeong", "Jeong-yeon Seo", "Taeho Hwang", "Jong C. Park"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.", "year": 2024, "publicationdate": "2024-04-22", "externalids": {"DOI": "10.48550/arXiv.2404.13948"}, "doi_lower": "10.48550/arxiv.2404.13948"}
{"paper_id": 270219294, "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models", "author_names": ["Jiaqi Xue", "Meng Zheng", "Yebowen Hu", "Fei Liu", "Xun Chen", "Qian Lou"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.", "year": 2024, "publicationdate": "2024-06-03", "externalids": {"DOI": "10.48550/arXiv.2406.00083"}, "doi_lower": "10.48550/arxiv.2406.00083"}
{"paper_id": 270199806, "title": "Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation", "author_names": ["Harsh Chaudhari", "Giorgio Severi", "John Abascal", "Matthew Jagielski", "Christopher A. Choquette-Choo", "Milad Nasr", "C. Nita-Rotaru", "Alina Oprea"], "venue": "", "abstract": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining. Despite their significant utility in various applications, RAG systems present new security risks. In this work, we propose a novel attack that allows an adversary to inject a single malicious document into a RAG system's knowledge base, and mount a backdoor poisoning attack. We design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model's output. First, the document is constructed to be retrieved only when a specific naturally occurring trigger sequence of tokens appears in the victim's queries. Second, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful behaviors.We demonstrate our attacks on multiple open-source LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to closed-source models such as GPT-3.5 Turbo and GPT-4. Finally, we successfully demonstrate our attack on an end-to-end black-box production RAG system: NVIDIA's\"Chat with RTX''.", "year": 2024, "publicationdate": "2024-05-30", "externalids": {}, "doi_lower": null}
{"paper_id": 270370889, "title": "DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation", "author_names": ["Shuting Wang", "Jiongnan Liu", "Jiehan Cheng", "Yuqi Fu", "Peidong Guo", "Kun Fang", "Yutao Zhu", "Zhicheng Dou"], "venue": "arXiv.org", "abstract": "Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.", "year": 2024, "publicationdate": "2024-06-09", "externalids": {"DOI": "10.48550/arXiv.2406.05654"}, "doi_lower": "10.48550/arxiv.2406.05654"}
{"paper_id": 270688478, "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems", "author_names": ["Florin Cuconasu", "Giovanni Trappolini", "Nicola Tonellotto", "Fabrizio Silvestri"], "venue": "arXiv.org", "abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".", "year": 2024, "publicationdate": "2024-06-21", "externalids": {"DOI": "10.48550/arXiv.2406.14972"}, "doi_lower": "10.48550/arxiv.2406.14972"}
{"paper_id": 261557644, "title": "Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering", "author_names": ["Yubo Wang", "Xueguang Ma", "Wenhu Chen"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2309.02233"}, "doi_lower": "10.48550/arxiv.2309.02233"}
{"paper_id": 265308533, "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science", "author_names": ["Sai Munikoti", "Anurag Acharya", "S. Wagle", "Sameera Horawalavithana"], "venue": "arXiv.org", "abstract": "Large language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy.", "year": 2023, "publicationdate": "2023-11-21", "externalids": {"DOI": "10.48550/arXiv.2311.12289"}, "doi_lower": "10.48550/arxiv.2311.12289"}
{"paper_id": 264487188, "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature", "author_names": ["A. Lozano", "Scott L. Fleming", "Chia-Chun Chiang", "Nigam H. Shah"], "venue": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing", "abstract": "The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.", "year": 2023, "publicationdate": "2023-10-24", "externalids": {"DOI": "10.48550/arXiv.2310.16146"}, "doi_lower": "10.48550/arxiv.2310.16146"}
{"paper_id": 263829356, "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models", "author_names": ["Boyu Zhang", "Hongyang Yang", "Tianyu Zhou", "Muhammad Ali Babar", "Xiao-Yang Liu"], "venue": "International Conference on AI in Finance", "abstract": "Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs’ sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score.", "year": 2023, "publicationdate": "2023-10-06", "externalids": {"DOI": "10.1145/3604237.3626866"}, "doi_lower": "10.1145/3604237.3626866"}
{"paper_id": 263310713, "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models", "author_names": ["Antoine Louis", "G. van Dijck", "Gerasimos Spanakis"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a \"retrieve-then-read\" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2309.17050"}, "doi_lower": "10.48550/arxiv.2309.17050"}
{"paper_id": 270062508, "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions", "author_names": ["Zheng Wang", "Shu Xian Teo", "Jieer Ouyang", "Yongjun Xu", "Wei Shi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.", "year": 2024, "publicationdate": "2024-05-26", "externalids": {"DOI": "10.48550/arXiv.2405.16420"}, "doi_lower": "10.48550/arxiv.2405.16420"}
{"paper_id": 265351819, "title": "Don’t forget private retrieval: distributed private similarity search for large language models", "author_names": ["Guy Zyskind", "Tobin South", "A. Pentland"], "venue": "PRIVATENLP", "abstract": "While the flexible capabilities of large language models (LLMs) allow them to answer a range of queries based on existing learned knowledge, information retrieval to augment generation is an important tool to allow LLMs to answer questions on information not included in pre-training data. Such private information is increasingly being generated in a wide array of distributed contexts by organizations and individuals. Performing such information retrieval using neural embeddings of queries and documents always leaked information about queries and database content unless both were stored locally. We present Private Retrieval Augmented Generation (PRAG), an approach that uses multi-party computation (MPC) to securely transmit queries to a distributed set of servers containing a privately constructed database to return top-k and approximate top-k documents. This is a first-of-its-kind approach to dense information retrieval that ensures no server observes a client’s query or can see the database content. The approach introduces a novel MPC friendly protocol for inverted file approximate search (IVF) that allows for fast document search over distributed and private data in sublinear communication complexity. This work presents new avenues through which data for use in LLMs can be accessed and used without needing to centralize or forgo privacy.", "year": 2023, "publicationdate": "2023-11-21", "externalids": {"DOI": "10.48550/arXiv.2311.12955"}, "doi_lower": "10.48550/arxiv.2311.12955"}
{"paper_id": 265995137, "title": "Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models", "author_names": ["Wenqi Jiang", "Marco Zeller", "R. Waleffe", "Torsten Hoefler", "Gustavo Alonso"], "venue": "Proceedings of the VLDB Endowment", "abstract": "A Retrieval-Augmented Language Model (RALM) combines a large language model (LLM) with a vector database to retrieve context-specific knowledge during text generation. This strategy facilitates impressive generation quality even with smaller models, thus reducing computational demands by orders of magnitude. To serve RALMs efficiently and flexibly, we propose\n Chameleon\n , a heterogeneous accelerator system integrating both LLM and vector search accelerators in a disaggregated architecture. The\n heterogeneity\n ensures efficient serving for both inference and retrieval, while the\n disaggregation\n allows independent scaling of LLM and vector search accelerators to fulfill diverse RALM requirements. Our Chameleon prototype implements vector search accelerators on FPGAs and assigns LLM inference to GPUs, with CPUs as cluster coordinators. Evaluated on various RALMs, Chameleon exhibits up to 2.16× reduction in latency and 3.18× speedup in throughput compared to the hybrid CPU-GPU architecture. The promising results pave the way for adopting heterogeneous accelerators for not only LLM inference but also vector search in future RALM systems.", "year": 2023, "publicationdate": "2023-10-15", "externalids": {"DOI": "10.48550/arXiv.2310.09949"}, "doi_lower": "10.48550/arxiv.2310.09949"}
{"paper_id": 261049520, "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models", "author_names": ["Yasuto Hoshi", "D. Miyashita", "Youyang Ng", "Kento Tatsuno", "Yasuhiro Morioka", "Osamu Torii", "J. Deguchi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.48550/arXiv.2308.10633"}, "doi_lower": "10.48550/arxiv.2308.10633"}
{"paper_id": 269982691, "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research", "author_names": ["Jiajie Jin", "Yutao Zhu", "Xinyu Yang", "Chenghao Zhang", "Zhicheng Dou"], "venue": "The Web Conference", "abstract": "With the advent of large language models (LLMs) and multimodal large language models (MLLMs), the potential of retrieval-augmented generation (RAG) has attracted considerable research attention. However, the absence of a standardized framework for implementation, coupled with the inherently complex RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. In response to this challenge, we develop FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing and comparing existing RAG methods and developing their own algorithms within a unified framework. Our toolkit has implemented 16 advanced RAG methods and gathered and organized 38 benchmark datasets. It has various features, including a customizable modular framework, a rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.", "year": 2024, "publicationdate": "2024-05-22", "externalids": {"DOI": "10.1145/3701716.3715313"}, "doi_lower": "10.1145/3701716.3715313"}
{"paper_id": 247627671, "title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion", "author_names": ["Kurt Shuster", "M. Komeili", "Leonard Adolphs", "Stephen Roller", "Arthur Szlam", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.", "year": 2022, "publicationdate": "2022-03-24", "externalids": {"DOI": "10.48550/arXiv.2203.13224"}, "doi_lower": "10.48550/arxiv.2203.13224"}
{"paper_id": 259144903, "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences", "author_names": ["Xiao Liu", "Hanyu Lai", "Hao Yu", "Yifan Xu", "Aohan Zeng", "Zhengxiao Du", "P. Zhang", "Yuxiao Dong", "Jie Tang"], "venue": "Knowledge Discovery and Data Mining", "abstract": "We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.", "year": 2023, "publicationdate": "2023-06-13", "externalids": {"DOI": "10.1145/3580305.3599931"}, "doi_lower": "10.1145/3580305.3599931"}
{"paper_id": 260126067, "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis", "author_names": ["Izzeddin Gur", "Hiroki Furuta", "Austin Huang", "Mustafa Safdari", "Yutaka Matsuo", "D. Eck", "Aleksandra Faust"], "venue": "International Conference on Learning Representations", "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.", "year": 2023, "publicationdate": "2023-07-24", "externalids": {"DOI": "10.48550/arXiv.2307.12856"}, "doi_lower": "10.48550/arxiv.2307.12856"}
{"paper_id": 273722475, "title": "Know where to go: Make LLM a relevant, responsible, and trustworthy searchers", "author_names": ["Xiang Shi", "Jiawei Liu", "Yinpeng Liu", "Qikai Cheng", "Wei Lu"], "venue": "Decision Support Systems", "abstract": null, "year": 2024, "publicationdate": "2024-10-01", "externalids": {"DOI": "10.1016/j.dss.2024.114354"}, "doi_lower": "10.1016/j.dss.2024.114354"}
{"paper_id": 267617266, "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models", "author_names": ["Peiyuan Gong", "Jiamian Li", "Jiaxin Mao"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo are accessible at https://github.com/pygongnlp/CoSearchAgent", "year": 2024, "publicationdate": "2024-02-09", "externalids": {"DOI": "10.1145/3626772.3657672"}, "doi_lower": "10.1145/3626772.3657672"}
{"paper_id": 276937772, "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "author_names": ["Bowen Jin", "Hansi Zeng", "Zhenrui Yue", "Dong Wang", "Hamed Zamani", "Jiawei Han"], "venue": "arXiv.org", "abstract": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.", "year": 2025, "publicationdate": "2025-03-12", "externalids": {"DOI": "10.48550/arXiv.2503.09516"}, "doi_lower": "10.48550/arxiv.2503.09516"}
{"paper_id": 277313597, "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "author_names": ["Mingyang Chen", "Tianpeng Li", "Haoze Sun", "Yijie Zhou", "Chenzheng Zhu", "Haofen Wang", "Jeff Z. Pan", "Wen Zhang", "Hua-zeng Chen", "Fan Yang", "Zenan Zhou", "Weipeng Chen"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.", "year": 2025, "publicationdate": "2025-03-25", "externalids": {"DOI": "10.48550/arXiv.2503.19470"}, "doi_lower": "10.48550/arxiv.2503.19470"}
{"paper_id": 276884818, "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning", "author_names": ["Huatong Song", "Jinhao Jiang", "Yingqian Min", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "venue": "arXiv.org", "abstract": "Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.", "year": 2025, "publicationdate": "2025-03-07", "externalids": {"DOI": "10.48550/arXiv.2503.05592"}, "doi_lower": "10.48550/arxiv.2503.05592"}
{"paper_id": 276813531, "title": "START: Self-taught Reasoner with Tools", "author_names": ["Chengpeng Li", "Mingfeng Xue", "Zhenru Zhang", "Jiaxin Yang", "Beichen Zhang", "Xiang Wang", "Bowen Yu", "Binyuan Hui", "Junyang Lin", "Dayiheng Liu"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", "year": 2025, "publicationdate": "2025-03-06", "externalids": {"DOI": "10.48550/arXiv.2503.04625"}, "doi_lower": "10.48550/arxiv.2503.04625"}
{"paper_id": 280677703, "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward", "author_names": ["Yong Deng", "Guoqing Wang", "Zhenzhe Ying", "Xiaofeng Wu", "Jinzhen Lin", "Wenwen Xiong", "Yuqin Dai", "Shuo Yang", "Zhanwei Zhang", "Qiwen Wang", "Yang Qin", "Yuan Wang", "Quanxing Zha", "Sunhao Dai", "Changhua Meng"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.", "year": 2025, "publicationdate": "2025-08-18", "externalids": {"DOI": "10.48550/arXiv.2508.12800"}, "doi_lower": "10.48550/arxiv.2508.12800"}
{"paper_id": 266149461, "title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models", "author_names": ["Haojie Pan", "Zepeng Zhai", "Hao Yuan", "Yaojia Lv", "Ruiji Fu", "Ming Liu", "Zhongyuan Wang", "Bing Qin"], "venue": "arXiv.org", "abstract": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.", "year": 2023, "publicationdate": "2023-12-08", "externalids": {"DOI": "10.48550/arXiv.2312.04889"}, "doi_lower": "10.48550/arxiv.2312.04889"}
{"paper_id": 280949856, "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning", "author_names": ["Lang Mei", "Zhihan Yang", "Chong Chen"], "venue": "arXiv.org", "abstract": "Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs'internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.", "year": 2025, "publicationdate": "2025-08-28", "externalids": {"DOI": "10.48550/arXiv.2508.20368"}, "doi_lower": "10.48550/arxiv.2508.20368"}
{"paper_id": 271534572, "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher", "author_names": ["Zehui Chen", "Kuikun Liu", "Qiuchen Wang", "Jiangning Liu", "Wenwei Zhang", "Kai Chen", "Feng Zhao"], "venue": "International Conference on Learning Representations", "abstract": "Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.", "year": 2024, "publicationdate": "2024-07-29", "externalids": {"DOI": "10.48550/arXiv.2407.20183"}, "doi_lower": "10.48550/arxiv.2407.20183"}
{"paper_id": 278911023, "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution", "author_names": ["Jiahao Qiu", "Xuan Qi", "Tongcheng Zhang", "Xinzhe Juan", "Jiacheng Guo", "Yifu Lu", "Yiming Wang", "Zixin Yao", "Qihan Ren", "Xun Jiang", "Xing Zhou", "Dongrui Liu", "Ling Yang", "Yue Wu", "Kaixuan Huang", "Shilong Liu", "Hongru Wang", "Mengdi Wang"], "venue": "arXiv.org", "abstract": "Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of\"Simplicity is the ultimate sophistication,\"enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at $\\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.", "year": 2025, "publicationdate": "2025-05-26", "externalids": {"DOI": "10.48550/arXiv.2505.20286"}, "doi_lower": "10.48550/arxiv.2505.20286"}
{"paper_id": 279071173, "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "author_names": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "venue": "arXiv.org", "abstract": "Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.", "year": 2025, "publicationdate": "2025-05-29", "externalids": {"DOI": "10.48550/arXiv.2505.23885"}, "doi_lower": "10.48550/arxiv.2505.23885"}
{"paper_id": 276422383, "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation", "author_names": ["Kaiyang Wan", "Honglin Mu", "Rui Hao", "Haoran Luo", "Tianle Gu", "Xiuying Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.", "year": 2025, "publicationdate": "2025-02-18", "externalids": {"DOI": "10.48550/arXiv.2502.12568"}, "doi_lower": "10.48550/arxiv.2502.12568"}
{"paper_id": 267617266, "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models", "author_names": ["Peiyuan Gong", "Jiamian Li", "Jiaxin Mao"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo are accessible at https://github.com/pygongnlp/CoSearchAgent", "year": 2024, "publicationdate": "2024-02-09", "externalids": {"DOI": "10.1145/3626772.3657672"}, "doi_lower": "10.1145/3626772.3657672"}
{"paper_id": 275358017, "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "author_names": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Zicheng Liu", "E. Barsoum"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.", "year": 2025, "publicationdate": "2025-01-08", "externalids": {"DOI": "10.48550/arXiv.2501.04227"}, "doi_lower": "10.48550/arxiv.2501.04227"}
{"paper_id": 271854887, "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery", "author_names": ["Chris Lu", "Cong Lu", "R. T. Lange", "J. Foerster", "Jeff Clune", "David Ha"], "venue": "arXiv.org", "abstract": "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist", "year": 2024, "publicationdate": "2024-08-12", "externalids": {}, "doi_lower": null}
{"paper_id": 274422541, "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models", "author_names": ["Tian Yu", "Shaolei Zhang", "Yang Feng"], "venue": "arXiv.org", "abstract": "Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\\footnote{Code is available at \\url{https://github.com/ictnlp/Auto-RAG}.", "year": 2024, "publicationdate": "2024-11-29", "externalids": {"DOI": "10.48550/arXiv.2411.19443"}, "doi_lower": "10.48550/arxiv.2411.19443"}
{"paper_id": 278789130, "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "author_names": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at https://github.com/RUCAIBox/SimpleDeepSearcher.", "year": 2025, "publicationdate": "2025-05-22", "externalids": {"DOI": "10.48550/arXiv.2505.16834"}, "doi_lower": "10.48550/arxiv.2505.16834"}
{"paper_id": 278789048, "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "author_names": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "venue": "arXiv.org", "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.", "year": 2025, "publicationdate": "2025-05-22", "externalids": {"DOI": "10.48550/arXiv.2505.16410"}, "doi_lower": "10.48550/arxiv.2505.16410"}
{"paper_id": 278367823, "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "author_names": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Yan Zhang", "Fei Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a novel RL framework that incentivizes the capabilities of LLMs to use a real search engine with simulated searches during training. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.", "year": 2025, "publicationdate": "2025-05-07", "externalids": {"DOI": "10.48550/arXiv.2505.04588"}, "doi_lower": "10.48550/arxiv.2505.04588"}
{"paper_id": 273026102, "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "author_names": ["Shayekh Bin Islam", "Md Asib Rahman", "K. S. M. T. Hossain", "Enamul Hoque", "Shafiq R. Joty", "Md. Rizwan Parvez"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/", "year": 2024, "publicationdate": "2024-10-02", "externalids": {"DOI": "10.48550/arXiv.2410.01782"}, "doi_lower": "10.48550/arxiv.2410.01782"}
{"paper_id": 276094069, "title": "DeepRAG: Thinking to Retrieve Step by Step for Large Language Models", "author_names": ["Xinyan Guan", "Jiali Zeng", "Fandong Meng", "Chunlei Xin", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Jie Zhou"], "venue": "", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities, while their practical applications are limited by severe factual hallucinations due to limitations in the timeliness, accuracy, and comprehensiveness of their parametric knowledge. Meanwhile, enhancing retrieval-augmented generation (RAG) with reasoning remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling reasonable and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency and boosts answer accuracy by 26.4%, demonstrating its effectiveness in enhancing retrieval-augmented reasoning.", "year": 2025, "publicationdate": "2025-02-03", "externalids": {}, "doi_lower": null}
{"paper_id": 275906944, "title": "Chain-of-Retrieval Augmented Generation", "author_names": ["Liang Wang", "Haonan Chen", "Nan Yang", "Xiaolong Huang", "Zhicheng Dou", "Furu Wei"], "venue": "arXiv.org", "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.", "year": 2025, "publicationdate": "2025-01-24", "externalids": {"DOI": "10.48550/arXiv.2501.14342"}, "doi_lower": "10.48550/arxiv.2501.14342"}
{"paper_id": 280323841, "title": "Agentic Reinforced Policy Optimization", "author_names": ["Guanting Dong", "Hangyu Mao", "Kai Ma", "Licheng Bao", "Yifei Chen", "Zhongyuan Wang", "Zhongxia Chen", "Jiazhen Du", "Huiyang Wang", "Fuzheng Zhang", "Guorui Zhou", "Yutao Zhu", "Ji-Rong Wen", "Zhicheng Dou"], "venue": "arXiv.org", "abstract": "Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models'intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO", "year": 2025, "publicationdate": "2025-07-26", "externalids": {"DOI": "10.48550/arXiv.2507.19849"}, "doi_lower": "10.48550/arxiv.2507.19849"}
{"paper_id": 278788476, "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "author_names": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.", "year": 2025, "publicationdate": "2025-05-22", "externalids": {"DOI": "10.48550/arXiv.2505.16421"}, "doi_lower": "10.48550/arxiv.2505.16421"}
{"paper_id": 277596185, "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments", "author_names": ["Yuxiang Zheng", "Dayuan Fu", "Xiangkun Hu", "Xiaojie Cai", "Lyumanshan Ye", "Pengrui Lu", "Pengfei Liu"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.", "year": 2025, "publicationdate": "2025-04-04", "externalids": {"DOI": "10.48550/arXiv.2504.03160"}, "doi_lower": "10.48550/arxiv.2504.03160"}
{"paper_id": 278207550, "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "author_names": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "venue": "arXiv.org", "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate among web pages, and draft reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.", "year": 2025, "publicationdate": "2025-04-30", "externalids": {"DOI": "10.48550/arXiv.2504.21776"}, "doi_lower": "10.48550/arxiv.2504.21776"}
{"paper_id": 278959248, "title": "WebDancer: Towards Autonomous Information Seeking Agency", "author_names": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.", "year": 2025, "publicationdate": "2025-05-28", "externalids": {"DOI": "10.48550/arXiv.2505.22648"}, "doi_lower": "10.48550/arxiv.2505.22648"}
{"paper_id": 280078605, "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "author_names": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents'performance and closing the capability gap.", "year": 2025, "publicationdate": "2025-07-03", "externalids": {"DOI": "10.48550/arXiv.2507.02592"}, "doi_lower": "10.48550/arxiv.2507.02592"}
{"paper_id": 280561766, "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent", "author_names": ["Xinyu Geng", "Peng Xia", "Zhen Zhang", "Xinyu Wang", "Qiuchen Wang", "Ruixue Ding", "Chenxi Wang", "Jialong Wu", "Yida Zhao", "Kuan Li", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.", "year": 2025, "publicationdate": "2025-08-07", "externalids": {"DOI": "10.48550/arXiv.2508.05748"}, "doi_lower": "10.48550/arxiv.2508.05748"}
{"paper_id": 26501419, "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "author_names": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.", "year": 2017, "publicationdate": "2017-05-01", "externalids": {"DOI": "10.18653/v1/P17-1147"}, "doi_lower": "10.18653/v1/p17-1147"}
{"paper_id": 254877603, "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories", "author_names": ["Alex Troy Mallen", "Akari Asai", "Victor Zhong", "Rajarshi Das", "Hannaneh Hajishirzi", "Daniel Khashabi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.18653/v1/2023.acl-long.546"}, "doi_lower": "10.18653/v1/2023.acl-long.546"}
{"paper_id": 52822214, "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "author_names": ["Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "R. Salakhutdinov", "Christopher D. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.", "year": 2018, "publicationdate": "2018-09-25", "externalids": {"DOI": "10.18653/v1/D18-1259"}, "doi_lower": "10.18653/v1/d18-1259"}
{"paper_id": 209848414, "title": "A B C D E F G H I J K L M N O P Q R S T U V W Y Z Inicio", "author_names": ["F. Scott"], "venue": "", "abstract": null, "year": 2001, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 277857238, "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents", "author_names": ["Jason Wei", "Zhiqing Sun", "Spencer Papay", "Scott McKinney", "Jeffrey Han", "Isa Fulford", "Hyung Won Chung", "Alexandre Passos", "W. Fedus", "Amelia Glaese"], "venue": "arXiv.org", "abstract": "We present BrowseComp, a simple yet challenging benchmark for measuring the ability for agents to browse the web. BrowseComp comprises 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. BrowseComp for browsing agents can be seen as analogous to how programming competitions are an incomplete but useful benchmark for coding agents. While BrowseComp sidesteps challenges of a true user query distribution, like generating long answers or resolving ambiguity, it measures the important core capability of exercising persistence and creativity in finding information. BrowseComp can be found at https://github.com/openai/simple-evals.", "year": 2025, "publicationdate": "2025-04-16", "externalids": {"DOI": "10.48550/arXiv.2504.12516"}, "doi_lower": "10.48550/arxiv.2504.12516"}
{"paper_id": 271745690, "title": "GAIA: a benchmark for General AI Assistants", "author_names": ["G. Mialon", "Clémentine Fourrier", "Thomas Wolf", "Yann LeCun", "Thomas Scialom"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 271328691, "title": "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "author_names": ["Ori Yoran", "S. Amouyal", "Chaitanya Malaviya", "Ben Bogin", "Ofir Press", "Jonathan Berant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.", "year": 2024, "publicationdate": "2024-07-22", "externalids": {"DOI": "10.48550/arXiv.2407.15711"}, "doi_lower": "10.48550/arxiv.2407.15711"}
{"paper_id": 273877854, "title": "Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks", "author_names": ["Adam Fourney", "Gagan Bansal", "Hussein Mozannar", "Cheng Tan", "Eduardo Salinas", "E. Zhu", "Friederike Niedtner", "Grace Proebsting", "Griffin Bassman", "Jack Gerrits", "Jacob Alber", "Peter Chang", "Ricky Loynd", "Robert West", "Victor Dibia", "Ahmed M. Awadallah", "Ece Kamar", "Rafah Hosn", "Saleema Amershi"], "venue": "arXiv.org", "abstract": "Modern AI agents, driven by advances in large foundation models, promise to enhance our productivity and transform our lives by augmenting our knowledge and capabilities. To achieve this vision, AI agents must effectively plan, perform multi-step reasoning and actions, respond to novel observations, and recover from errors, to successfully complete complex tasks across a wide range of scenarios. In this work, we introduce Magentic-One, a high-performing open-source agentic system for solving such tasks. Magentic-One uses a multi-agent architecture where a lead agent, the Orchestrator, plans, tracks progress, and re-plans to recover from errors. Throughout task execution, the Orchestrator directs other specialized agents to perform tasks as needed, such as operating a web browser, navigating local files, or writing and executing Python code. We show that Magentic-One achieves statistically competitive performance to the state-of-the-art on three diverse and challenging agentic benchmarks: GAIA, AssistantBench, and WebArena. Magentic-One achieves these results without modification to core agent capabilities or to how they collaborate, demonstrating progress towards generalist agentic systems. Moreover, Magentic-One's modular design allows agents to be added or removed from the team without additional prompt tuning or training, easing development and making it extensible to future scenarios. We provide an open-source implementation of Magentic-One, and we include AutoGenBench, a standalone tool for agentic evaluation. AutoGenBench provides built-in controls for repetition and isolation to run agentic benchmarks in a rigorous and contained manner -- which is important when agents' actions have side-effects. Magentic-One, AutoGenBench and detailed empirical performance evaluations of Magentic-One, including ablations and error analysis are available at https://aka.ms/magentic-one", "year": 2024, "publicationdate": "2024-11-07", "externalids": {"DOI": "10.48550/arXiv.2411.04468"}, "doi_lower": "10.48550/arxiv.2411.04468"}
{"paper_id": 263829697, "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "author_names": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "venue": "International Conference on Learning Representations", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06770"}, "doi_lower": "10.48550/arxiv.2310.06770"}
{"paper_id": 260886874, "title": "OctoPack: Instruction Tuning Code Large Language Models", "author_names": ["Niklas Muennighoff", "Qian Liu", "Qi Liu", "A. Zebaze", "Qinkai Zheng", "Binyuan Hui", "Terry Yue Zhuo", "Swayam Singh", "Xiangru Tang", "L. V. Werra", "S. Longpre"], "venue": "International Conference on Learning Representations", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {}, "doi_lower": null}
{"paper_id": 273233550, "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering", "author_names": ["Jun Shern Chan", "Neil Chowdhury", "Oliver Jaffe", "James Aung", "Dane Sherburn", "Evan Mays", "Giulio Starace", "Kevin Liu", "Leon Maksin", "Tejal Patwardhan", "Lilian Weng", "Aleksander Mkadry"], "venue": "arXiv.org", "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.", "year": 2024, "publicationdate": "2024-10-09", "externalids": {"DOI": "10.48550/arXiv.2410.07095"}, "doi_lower": "10.48550/arxiv.2410.07095"}
{"paper_id": 263671541, "title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation", "author_names": ["Qian Huang", "Jian Vora", "Percy Liang", "J. Leskovec"], "venue": "International Conference on Machine Learning", "abstract": "A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.", "year": 2023, "publicationdate": "2023-10-05", "externalids": {}, "doi_lower": null}
{"paper_id": 274192262, "title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts", "author_names": ["Hjalmar Wijk", "Tao Lin", "Joel Becker", "Sami Jawhar", "Neev Parikh", "Thomas Broadley", "Lawrence Chan", "Michael Chen", "Joshua Clymer", "Jai Dhyani", "Elena Ericheva", "Katharyn Garcia", "Brian Goodrich", "Nikola Jurkovic", "Megan Kinniment", "Aron Lajko", "Seraphina Nix", "L. Sato", "William Saunders", "Maksym Taran", "Ben West", "Elizabeth Barnes"], "venue": "arXiv.org", "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics -- e.g. an agent wrote a faster custom Triton kernel than any of our human experts' -- and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research.", "year": 2024, "publicationdate": "2024-11-22", "externalids": {"DOI": "10.48550/arXiv.2411.15114"}, "doi_lower": "10.48550/arxiv.2411.15114"}
{"paper_id": 274992362, "title": "ResearchTown: Simulator of Human Research Community", "author_names": ["Haofei Yu", "Zhaochen Hong", "Zirui Cheng", "Kunlun Zhu", "Keyang Xuan", "Jinwei Yao", "Tao Feng", "Jiaxuan You"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research community simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire pioneering research directions.", "year": 2024, "publicationdate": "2024-12-23", "externalids": {"DOI": "10.48550/arXiv.2412.17767"}, "doi_lower": "10.48550/arxiv.2412.17767"}
{"paper_id": 260164780, "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents", "author_names": ["Shuyan Zhou", "Frank F. Xu", "Hao Zhu", "Xuhui Zhou", "Robert Lo", "Abishek Sridhar", "Xianyi Cheng", "Yonatan Bisk", "Daniel Fried", "Uri Alon", "Graham Neubig"], "venue": "International Conference on Learning Representations", "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.", "year": 2023, "publicationdate": "2023-07-25", "externalids": {"DOI": "10.48550/arXiv.2307.13854"}, "doi_lower": "10.48550/arxiv.2307.13854"}
{"paper_id": 273502662, "title": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation", "author_names": ["Jingxuan Chen", "Derek Yuen", "Bin Xie", "Yuhao Yang", "Gongwei Chen", "Zhihao Wu", "Yixing Li", "Xurui Zhou", "Weiwen Liu", "Shuai Wang", "Kaiwen Zhou", "Rui Shao", "Liqiang Nie", "Yasheng Wang", "Jianye Hao", "Jun Wang", "Kun Shao"], "venue": "International Conference on Learning Representations", "abstract": "Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. SPA-Bench is available at https://ai-agents-2030.github.io/SPA-Bench/.", "year": 2024, "publicationdate": "2024-10-19", "externalids": {"DOI": "10.48550/arXiv.2410.15164"}, "doi_lower": "10.48550/arxiv.2410.15164"}
{"paper_id": 275471576, "title": "WebWalker: Benchmarking LLMs in Web Traversal", "author_names": ["Jialong Wu", "Wenbiao Yin", "Yong Jiang", "Zhenglin Wang", "Zekun Xi", "Runnan Fang", "Deyu Zhou", "Pengjun Xie", "Fei Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.", "year": 2025, "publicationdate": "2025-01-13", "externalids": {"DOI": "10.48550/arXiv.2501.07572"}, "doi_lower": "10.48550/arxiv.2501.07572"}
{"paper_id": 236956415, "title": "IntenT5: Search Result Diversification using Causal Language Models", "author_names": ["Sean MacAvaney", "Craig Macdonald", "R. Murray-Smith", "I. Ounis"], "venue": "arXiv.org", "abstract": "Search result diversification is a beneficial approach to overcome under-specified queries, such as those that are ambiguous or multi-faceted. Existing approaches often rely on massive query logs and interaction data to generate a variety of possible query intents, which then can be used to re-rank documents. However, relying on user interaction data is problematic because one first needs a massive user base to build a sufficient log; public query logs are insufficient on their own. Given the recent success of causal language models (such as the Text-To-Text Transformer (T5) model) at text generation tasks, we explore the capacity of these models to generate potential query intents. We find that to encourage diversity in the generated queries, it is beneficial to adapt the model by including a new Distributional Causal Language Modeling (DCLM) objective during fine-tuning and a representation replacement during inference. Across six standard evaluation benchmarks, we find that our method (which we call IntenT5) improves search result diversity and attains (and sometimes exceeds) the diversity obtained when using query suggestions based on a proprietary query log. Our analysis shows that our approach is most effective for multi-faceted queries and is able to generalize effectively to queries that were unseen in training data.", "year": 2021, "publicationdate": "2021-08-09", "externalids": {}, "doi_lower": null}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 46061205, "title": "Mean Reciprocal Rank", "author_names": ["Nick Craswell"], "venue": "Encyclopedia of Database Systems", "abstract": null, "year": 2009, "publicationdate": null, "externalids": {"DOI": "10.1007/978-0-387-39940-9_488"}, "doi_lower": "10.1007/978-0-387-39940-9_488"}
{"paper_id": 11080756, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author_names": ["Kishore Papineni", "Salim Roukos", "T. Ward", "Wei-Jing Zhu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "year": 2002, "publicationdate": "2002-07-06", "externalids": {"DOI": "10.3115/1073083.1073135"}, "doi_lower": "10.3115/1073083.1073135"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 258049316, "title": "WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus", "author_names": ["Hongjin Qian", "Yutao Zhu", "Zhicheng Dou", "Haoqi Gu", "Xinyu Zhang", "Zheng Liu", "Ruofei Lai", "Zhao Cao", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "In this paper, we introduce a new NLP task -- generating short factual articles with references for queries by mining supporting evidence from the Web. In this task, called WebBrain, the ultimate goal is to generate a fluent, informative, and factually-correct short article (e.g., a Wikipedia article) for a factual query unseen in Wikipedia. To enable experiments on WebBrain, we construct a large-scale dataset WebBrain-Raw by extracting English Wikipedia articles and their crawlable Wikipedia references. WebBrain-Raw is ten times larger than the previous biggest peer dataset, which can greatly benefit the research community. From WebBrain-Raw, we construct two task-specific datasets: WebBrain-R and WebBrain-G, which are used to train in-domain retriever and generator, respectively. Besides, we empirically analyze the performances of the current state-of-the-art NLP techniques on WebBrain and introduce a new framework ReGen, which enhances the generation factualness by improved evidence retrieval and task-specific pre-training for generation. Experiment results show that ReGen outperforms all baselines in both automatic and human evaluations.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04358"}, "doi_lower": "10.48550/arxiv.2304.04358"}
{"paper_id": 263834979, "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators", "author_names": ["Liang Chen", "Yang Deng", "Yatao Bian", "Zeyu Qin", "Bingzhe Wu", "Tat-Seng Chua", "Kam-Fai Wong"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07289"}, "doi_lower": "10.48550/arxiv.2310.07289"}
{"paper_id": 270687072, "title": "AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval", "author_names": ["Shicheng Xu", "Danyang Hou", "Liang Pang", "Jingcheng Deng", "Jun Xu", "Huawei Shen", "Xueqi Cheng"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2311.14084"}, "doi_lower": "10.48550/arxiv.2311.14084"}
{"paper_id": 272980542, "title": "LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts", "author_names": ["Sunhao Dai", "Yuqi Zhou", "Liang Pang", "Weihao Liu", "Xiaolin Hu", "Yong Liu", "Xiao Zhang", "Jun Xu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2310.20501"}, "doi_lower": "10.48550/arxiv.2310.20501"}
{"paper_id": 258040990, "title": "Generative Agents: Interactive Simulacra of Human Behavior", "author_names": ["J. Park", "Joseph C. O’Brien", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.1145/3586183.3606763"}, "doi_lower": "10.1145/3586183.3606763"}
