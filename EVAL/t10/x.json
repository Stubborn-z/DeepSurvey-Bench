{
  "survey": "Large Language Models (LLMs) have significantly transformed the landscape of information retrieval by enhancing semantic search capabilities through advanced transformer architectures. This survey systematically examines the evolution and application of LLMs, focusing on pre-training advancements and addressing inherent limitations in existing methods. Key findings highlight the impact of scaling on model performance and the demand for efficient foundation models, while exploring the role of GPT models in constructing listwise rerankers and the challenges of scientific reproducibility. The survey delves into the automation of real-world tasks using multi-agent systems and the necessity for novel approaches due to domain transferability challenges. It organizes research within the paradigm of prompt-based learning, discussing tuning strategies and applications, and introduces proactive capabilities in dialogue systems, distinct from traditional chatbots. Furthermore, it explores the integration of web search capabilities with LLMs to enhance question-answering systems, addressing limitations of previous methods like WebGPT. The survey aims to resolve issues in collaborative search systems and examines the factual knowledge boundaries of LLMs, exploring retrieval augmentation's influence on performance in open-domain question answering. By examining lightweight, state-of-the-art open models for improved language understanding and safety, the survey provides a comprehensive overview of LLMs' objectives and coverage in information retrieval, highlighting advancements in reasoning capabilities, relevance ranking, and the evolution of social chatbots. The exploration of large decoder transformers for semantic search and sentence embeddings addresses challenges related to their scale and the need for separate models for these tasks. Through these insights, the survey underscores the transformative potential of LLMs in revolutionizing information retrieval systems.\n\nIntroduction Purpose and Scope of the Survey The integration of large language models (LLMs) into information retrieval systems represents a pivotal advancement in processing user queries, particularly in capturing nuanced search intents within conversational contexts [1]. This survey systematically examines the evolution of LLMs, focusing on advancements in pre-training technologies and addressing the limitations of existing methods [2]. It underscores the significance of scaling model performance and the demand for efficient foundational models, while deliberately excluding unrelated NLP tasks [3]. A critical aspect is the reliance on GPT models for developing listwise rerankers, raising concerns regarding scientific reproducibility and the generalizability of findings across various LLMs [4]. The survey further investigates the automation of real-world tasks via multi-agent systems based on LLMs, highlighting the necessity for innovative approaches due to challenges in domain transferability [5]. Research is organized within the emerging paradigm of prompt-based learning in NLP, discussing pre-trained language models, tuning strategies, and their applications [6]. Proactive capabilities in dialogue systems are distinguished from traditional chatbots [7], and the transfer of knowledge from well-supervised tasks to diverse retrieval tasks with limited supervision is addressed [8]. The integration of web search capabilities with LLMs aims to enhance question-answering systems, revealing limitations of previous methods such as WebGPT [9]. The survey seeks to resolve challenges in implementing collaborative search systems that facilitate real-time multi-user interactions [10]. Additionally, it examines the factual knowledge boundaries of LLMs and the influence of retrieval augmentation on their performance in open-domain question answering [11]. Insights into the role of LLMs in improving information retrieval systems are provided, alongside efforts to develop agents capable of conducting independent scientific research [12]. The survey also addresses inadequate predictive performance in financial sentiment analysis using traditional NLP models and LLMs [13], and explores the balance between intrinsic long-horizon reasoning capabilities and proficiency in multi-turn tool interactions [14]. By reviewing lightweight, state-of-the-art open models for enhanced language understanding and safety [15], the survey delves into advancements in reasoning capabilities of LLMs [16], relevance ranking in IR tasks [17], and the evolution of social chatbots [18]. Furthermore, it discusses the performance limitations of pre-trained LLMs in autonomous web automation [19] and instruction tuning with code [20], offering a comprehensive overview of LLMs' objectives and coverage in information retrieval. The utilization of large decoder transformers for semantic search and sentence embeddings is also examined, addressing challenges related to their scale and the necessity for distinct models for these tasks [21]. Structure of the Survey This survey is meticulously structured to explore the role of large language models (LLMs) in information retrieval comprehensively. The initial section introduces the topic and elucidates the transformative impact of LLMs on enhancing semantic search capabilities. Following this, the survey delineates its purpose and scope, emphasizing the focus on transformer-based models and their application in capturing nuanced user queries [4]. The background section traces the evolution of information retrieval systems, highlighting the shift from traditional statistical methods to modern neural approaches, while discussing the limitations of conventional search technologies in grasping semantic nuances [18]. Subsequently, key terms and core concepts are defined to clarify the operational principles of LLMs, the concept of semantic search, and the role of natural language processing in information retrieval. In examining LLM applications, the survey highlights enhancements in query rewriting and reformulation processes, supported by case studies and benchmark evaluations. It delves into the architecture and components of transformer models, illustrating their role in enhancing semantic search by capturing complex language patterns. This includes the introduction of the Differentiable Search Index (DSI), which encodes corpus information within the model's parameters, enabling direct query answering and outperforming traditional models like dual encoders and BM25 in zero-shot setups. The Retrieval-Enhanced Transformer (RETRO) model exemplifies the potential of conditioning on retrieved document chunks from extensive corpora, achieving performance comparable to larger models like GPT-3 while utilizing significantly fewer parameters, thereby improving knowledge-intensive task performance through a combination of a frozen Bert retriever, a differentiable encoder, and a chunked cross-attention mechanism [22,23]. The survey critically analyzes challenges and limitations associated with LLMs, including computational complexity, data dependency, and ethical considerations. It concludes with a comprehensive discussion on future directions, emphasizing advancements aimed at optimizing model efficiency and expanding LLM capabilities across multilingual and multimodal contexts. The transformative impact of LLMs on information retrieval (IR) research, the development of benchmarks like ALCE for citation evaluation, and the potential for generating diverse query variants are highlighted. Additionally, the role of LLMs as backbone encoders in dense retrieval, their application in financial sentiment analysis through retrieval-augmentation frameworks, and the ongoing challenges of computational costs, credibility, and ethical considerations are explored [24,25,26,13,27]. By employing a structured methodology, the survey provides an in-depth analysis of how LLMs revolutionize information retrieval systems, enhancing text understanding, generation, and knowledge inference, thus facilitating generative retrieval and improving user-system interactions. It highlights the synergistic relationship between IR models, LLMs, and humans, forming a powerful new paradigm for information seeking. The transformative impact of LLMs on IR research is discussed, including a rethinking of core values and the mutual enhancement of LLMs and IR, while addressing challenges such as computational costs, credibility, and ethical considerations. The survey also underscores the potential of LLMs in specialized domains like legal case retrieval and financial sentiment analysis, demonstrating superior performance and robustness compared to traditional methods [13,28,26].The following sections are organized as shown in . Background Evolution of Information Retrieval Systems Information retrieval systems have evolved from traditional statistical methods to sophisticated neural language models. Early systems relied heavily on statistical techniques, which required complete redesigns and retraining for new domains, limiting adaptability and efficiency [2]. These methods often struggled with semantic depth and contextual nuances, prompting a shift towards more advanced models [29]. The advent of neural ranking models marked a transformative shift, utilizing dual encoders to enhance retrieval capabilities, though these models faced generalization challenges across diverse domains due to context-specific training [30]. This limitation led to the development of large pre-trained language models, significantly improving natural language understanding and generation [31]. Evaluations of models such as ChatGPT and GPT-4 in ranking passages for search tasks underscore the complexities in contemporary retrieval efforts [17]. Large language models represent a significant advancement, although they require extensive parameters for knowledge storage, complicating few-shot learning scenarios [32]. Inefficiencies in multi-step search pipelines for knowledge-intensive tasks highlight the need for end-to-end optimization to enhance retrieval processes [13]. Additionally, benchmarks addressing safety and factual grounding in dialog systems illustrate the complexity required in modern retrieval systems [33]. Despite these advancements, reliance on GPT models for listwise rerankers has restricted applicability and reproducibility, necessitating more robust methodologies [14]. Adapting decoder-only models for text ranking presents challenges in utilizing prompt-based learning with minimal labeled data [16]. The evolution of scientific research methodologies has exposed traditional methods' inefficiencies, often hindering timely discoveries, emphasizing the need for adaptive pre-trained language models capable of effective text generation. These developments have enabled information retrieval systems to better address the complexities of modern search technologies, enhancing performance and adaptability across contexts. The survey categorizes current methods into developmental stages, including early conversational agents, task-completion systems, intelligent personal assistants, and contemporary social chatbots, reflecting the dynamic evolution of these systems [18]. Limitations of Traditional Search Technologies Traditional search technologies face challenges in capturing semantic meaning due to outdated retrieval models and inefficient indexing methods. Sparse vector models like TF-IDF and BM25 have historically underpinned passage retrieval but frequently fail to retrieve semantically relevant passages, necessitating more advanced approaches [21]. Integrating pre-trained language models (PLMs) into existing frameworks complicates relevance modeling, particularly in processing dense vectors [16]. Traditional methods struggle with query disambiguation and retrieving pertinent information in sparse and dense retrieval contexts [34]. Existing benchmarks often depend on fine-tuning specific datasets, limiting generalization across diverse applications [28]. These benchmarks typically neglect the interplay of entities and relations in retrieval models, resulting in a gap in understanding their impact on language model performance [11]. Traditional search technologies inadequately simulate balancing fluency and attribution, leading to insufficient insights into retrieval strategies' effects on model performance [35]. The homogeneous nature of benchmarks further constrains understanding of retrieval models' generalization capabilities [36]. The inefficiency of listwise ranking methods is compounded by large language models' limitations in managing long contexts and associated latency [19]. Traditional methods often overlook knowledge interdependencies and reasoning steps, resulting in incomplete or inaccurate responses [12]. The scarcity of labeled data poses challenges in training effective models, as existing methods rely on specific datasets, making them less effective against new data distributions [18]. Difficulty in retrieving relevant information from complex conversational sessions further underscores traditional methods' limitations [28]. Moreover, traditional search technologies struggle to capture real-time information, critical in dynamic environments. This challenge is exacerbated by the need for high-quality, contextually relevant data retrieval, as highlighted by advancements in large language models (LLMs) and information retrieval (IR) systems. Despite these advancements, traditional methods grapple with computational costs, credibility concerns, and synthesizing information from multiple sources, underscoring the need for superior retrieval systems and leveraging LLMs for enhanced real-time information access [37,25,26,38,39]. The inconsistency between information structure and reasoning structure in agents hampers coherent question and answer generation. High computational costs, credibility concerns, domain-specific limitations, and ethical considerations further complicate implementing traditional search technologies, highlighting the need for sophisticated techniques to enhance accuracy and contextual relevance, especially in specialized domains like financial data, where existing benchmarks fail to capture specific needs and complexities. Definitions and Core Concepts The modern landscape of information retrieval is shaped by the synergy between advanced computational models and linguistic understanding. This section clarifies the foundational definitions and core concepts of large language models (LLMs) and their operational principles, emphasizing their role in enhancing traditional retrieval processes and introducing innovative methodologies that redefine user interactions with information systems. illustrates the hierarchical structure of information retrieval advancements, focusing on LLMs, semantic search, and Natural Language Processing (NLP) applications. This figure highlights the operational principles, applications, and challenges of LLMs, the advancements and impact of transformer models in semantic search, and the role of NLP in enhancing information retrieval and broader applications. By integrating these elements, we gain a comprehensive understanding of how LLMs are transforming the landscape of information retrieval. Large Language Models and Their Operational Principles Large language models (LLMs) have transformed natural language processing by enhancing information retrieval capabilities through sophisticated neural architectures. Central to LLMs is the transformer architecture, which uses self-attention mechanisms to discern complex dependencies and contextual relationships within extensive text sequences, producing coherent and contextually relevant outputs [21]. This framework advances semantic understanding and contextual relevance, integrating task instructions with input text to produce versatile embeddings applicable across various tasks [33]. As illustrated in , the operational principles, applications, and challenges of large language models are encapsulated, highlighting their transformer architecture, modular tasks, and integration with external knowledge sources. LLMs operate through modular tasks, including information search, knowledge generation, and response synthesis. Innovations such as SGPT use decoder transformers to create sentence embeddings suited for semantic search, employing prompting or fine-tuning techniques [21]. This exemplifies LLMs' transformative potential in optimizing retrieval processes for precise and contextually relevant information extraction. Integrating external knowledge sources into dialog generation, as demonstrated by models like LaMDA, introduces novel metrics for quantifying safety and factuality [33]. These mechanisms enhance LLMs' ability to produce accurate, fact-based responses, addressing issues of factual inaccuracies in long text generation [40]. Applications like WebAgent illustrate LLM-driven agents automating tasks on real websites, planning, summarizing, and generating executable Python code from natural language instructions [19]. In specialized domains, such as legal information retrieval, methods like KELLER leverage LLMs to enhance the retrieval of similar legal cases by accurately representing essential case information [28]. This adaptability underscores LLMs' effectiveness in contexts requiring nuanced understanding. Furthermore, the design of social chatbots that engage users emotionally while meeting their communication needs highlights LLMs' versatility across diverse applications [18]. Despite their advantages, LLMs face challenges such as overconfidence, where they provide answers despite lacking knowledge, leading to inaccuracies [40]. The Knowledge Boundary Awareness Enhancement (KBAE) method seeks to improve LLMs' awareness of their limitations. Additionally, LLMs often struggle with effectively accessing and utilizing external knowledge without costly retraining [21]. Techniques like prefix-tuning optimize task-specific vectors while keeping language model parameters frozen, enhancing adaptability to new tasks. By refining their operational principles, LLMs continue to revolutionize information retrieval, particularly in contexts requiring nuanced understanding. Their ability to dynamically generate user profiles through historical data and current queries significantly enhances personalized retrieval experiences. This adaptability is evident in specialized domains, such as personalized chatbots and search engines, where implicit user profiles are constructed from dialogue histories and query sequences. Advanced techniques like hierarchical recurrent neural networks with query-aware attention and retrieval-based models allow these systems to infer user preferences and language styles, resulting in more accurate and context-aware responses. The integration of LLM generators further enriches knowledge-intensive tasks, providing a novel approach to document generation that complements traditional retrieval methods. Extensive experiments demonstrate superior model performance, emphasizing their capability to personalize interactions based on nuanced user behaviors and preferences [38,41,7,42]. Thus, LLMs represent a significant advancement in the evolution of information retrieval systems, offering improved performance and adaptability across diverse contexts. Semantic Search and Transformer Models Semantic search signifies a paradigm shift in information retrieval, emphasizing meaning-based understanding over keyword matching. It leverages semantic and contextual relationships within text, facilitating the retrieval of information closely aligned with user intent. Transformer models, particularly BERT and its derivatives, have significantly advanced semantic search through dynamic pre-training techniques that capture deep bidirectional representations of language. Unlike earlier static models such as Word2Vec and GloVe, BERT conditions on both left and right context across all layers, effectively addressing polysemy and enhancing performance on various natural language processing tasks, achieving state-of-the-art results in question answering and language inference [2,43]. The landmark innovation of BERT (Bidirectional Encoder Representations from Transformers) lies in its ability to jointly condition on both left and right contexts across all layers, representing a significant departure from unidirectional models. This bidirectional approach enables BERT to capture complex dependencies within text, thereby enhancing its understanding of semantic nuances [43]. By attending to all tokens in a sequence simultaneously, BERT and similar transformer models can interpret text from multiple perspectives, improving the representation of polysemous and context-dependent words. Additionally, deep contextualized word representations, as proposed in models like ELMo, utilize the internal states of deep bidirectional language models to reflect the multifaceted nature of word usage [44]. Such representations significantly enhance semantic search effectiveness by capturing intricate language patterns and contextual meanings, allowing for more accurate retrieval of semantically relevant information. To facilitate task-specific adaptation without extensive retraining, methods like LoRA (Low-Rank Adaptation) have been introduced. LoRA integrates trainable low-rank matrices into each layer of a pre-trained transformer model while keeping the original model parameters frozen [45]. This approach enhances the flexibility and efficiency of transformers across diverse retrieval tasks, enabling them to adapt to new semantic search challenges with minimal computational overhead. Through these innovations, transformer models have fundamentally transformed the semantic search landscape, offering refined capabilities in understanding and retrieving contextually relevant information. Their ability to align retrieval processes with nuanced user intents signifies a major advancement in information retrieval, facilitating more precise and meaningful search outcomes. As these models evolve, they hold the potential to further revolutionize how information is accessed and understood across various domains. Natural Language Processing and Its Applications Natural Language Processing (NLP) is pivotal in advancing information retrieval systems, enabling the extraction, interpretation, and synthesis of human language data. NLP's transformative impact is evident in its ability to enhance the semantic understanding of user queries, thereby improving the precision and relevance of retrieved information [2]. Through sophisticated neural architectures, NLP facilitates the development of conversational agents and intelligent personal assistants that dynamically interact with users, delivering contextually relevant responses and personalized experiences [18]. In information retrieval, NLP techniques refine query processing capabilities. By employing methods such as query rewriting and reformulation, NLP systems adapt user inputs to align more closely with the semantic structures of target databases, enhancing retrieval accuracy [21]. NLP-driven models like ChatGPT and GPT-4 exhibit significant improvements in understanding complex language patterns, enabling more effective ranking of passages and retrieval of semantically pertinent information [17]. Beyond information retrieval, NLP applications span diverse domains, including sentiment analysis, which provides insights into public opinions and trends by processing vast amounts of textual data [13]. In legal information retrieval, NLP models such as KELLER enhance the accurate representation and retrieval of case information, demonstrating adaptability in specialized contexts [28]. Furthermore, NLP's integration into social chatbot design underscores its role in enhancing user engagement through emotionally intelligent interactions [18]. Despite advancements, NLP systems encounter challenges such as data dependency and quality, which can affect model performance and reliability [12]. Addressing these challenges necessitates continuous refinement of NLP techniques to ensure robust, scalable, and ethically sound applications. As NLP evolves, its role in information retrieval and related applications is likely to expand, presenting new opportunities for innovation and enhancement across various fields. Large Language Models in Information Retrieval Enhancements in Query Rewriting and Reformulation Large language models (LLMs) have significantly improved query rewriting and reformulation, aligning search results more closely with user intent. Techniques such as reasoning chains for keyword generation enhance ranker performance by employing self-consistency and reciprocal rank weighting, enabling context-aware query modifications [46]. Agentic Reinforced Policy Optimization (ARPO) refines multi-turn LLM-based agents by adapting to uncertainties during tool usage, thereby enhancing query reformulation [14]. The AI Scientist exemplifies LLMs' potential by autonomously generating research ideas and drafting scientific papers, showcasing their transformative impact on query processes [12]. Forward-Looking Active REtrieval augmented generation (FLARE) dynamically retrieves information based on future sentence predictions, improving adaptability and precision in text generation [40]. LaMDA models, with advanced transformer architectures, demonstrate LLMs' effectiveness in query rewriting through sophisticated language understanding [33]. The Gemma model highlights the importance of responsible model release for enhancing language model safety and innovation, reflecting LLMs' broader impact on query processes [15]. These advancements illustrate LLMs' capacity to provide precise, relevant, and contextually aware retrieval solutions, positioning them to further transform information retrieval through user-centric methodologies. Case Studies and Benchmark Evaluations Table presents a detailed summary of the benchmarks employed in evaluating the effectiveness of LLMs in enhancing retrieval and ranking capabilities across multiple domains. The integration of LLMs into search systems has been rigorously evaluated, revealing significant advancements in retrieval and ranking capabilities. DemoRank experiments showcased substantial improvements in demonstration selection for ranking tasks, highlighting LLMs' adaptability in complex ranking challenges [47]. Instruction tuning with LLaMA models demonstrated efficiency in managing retrieval tasks, outperforming previous state-of-the-art instruction-following data [48]. The BEQUE model's evaluation on Taobao indicated enhancements in gross merchandise volume (GMV), transaction numbers, and unique visitors for long-tail queries, illustrating LLMs' capability to manage complex search tasks without extensive fine-tuning [36]. Gemma models outperformed similarly sized open models on multiple benchmark tasks, showcasing versatility and robustness in diverse NLP applications, including improved information retrieval processes [15]. NovelEval findings highlighted competitive ranking performance with properly instructed LLMs [17]. In legal information retrieval, KELLER's extensive experiments demonstrated accuracy and robustness in handling complex queries [28]. SGPT model evaluations on the BEIR search benchmark showcased efficient parameter utilization for high performance with reduced computational resources [21]. Collectively, these evaluations underscore the successful integration of LLMs into search systems, yielding substantial improvements in retrieval accuracy, ranking precision, and user engagement across various domains. As LLMs evolve, their role in information retrieval is set to revolutionize search technologies, enhancing text understanding, generation, and knowledge inference. Despite challenges such as computational costs and credibility concerns, innovations like SlimPLM optimize knowledge acquisition processes. LLMs effectively generate query variants and improve citation accuracy, promising more powerful and efficient information-seeking systems [24,25,39,26]. Transformer Models and Semantic Search Architecture of Transformer Models Transformer models have revolutionized natural language processing by efficiently handling large-scale data through self-attention mechanisms, which process input sequences in parallel to improve computational efficiency and scalability [49]. This mechanism captures complex dependencies and contextual relationships within text, leading to more precise language generation. The architecture consists of multiple layers, each incorporating a self-attention mechanism and a feed-forward neural network. The self-attention mechanism computes attention scores to prioritize relevant information within the input sequence [19]. The feed-forward network processes these scores through linear transformations and a non-linear activation function, enabling hierarchical language representations and enhanced semantic understanding. Scalability is further supported by scaling laws that predict performance improvements based on parameters and computational effort, guiding the optimization of transformer architectures [50]. Techniques such as AttendOut dropout enhance model robustness by preventing overfitting and ensuring reliable performance across diverse tasks [49]. Transformer models, exemplified by WebGPT, demonstrate versatility in browsing the web and incorporating human feedback to improve answer quality for long-form questions [51]. The architectural design optimizes retrieval processes, as highlighted by dense retrieval studies focusing on architecture, training, indexing, and integration [52]. Through these innovations, transformer models have redefined information retrieval, enhancing semantic understanding, scalability, and integration. Their efficiency in processing large data volumes and generating contextually relevant outputs positions them as pivotal in advancing modern search technologies. As transformers evolve, their integration with dynamic pre-training technologies like BERT and large language models (LLMs) will drive significant innovations in information retrieval, improving text understanding, generation, and knowledge inference. Supported by domain-specific training data and synergistic relationships among information retrieval models, LLMs, and human evaluators, transformers form a powerful paradigm for information seeking. Despite challenges such as computational costs and domain-specific limitations, transformers' potential to enhance in-domain accuracy, zero-shot generalization, and multi-task learning underscores their critical role in advancing information retrieval [53,26,2,54,27]. Enhancing Semantic Search with Transformers Transformers have significantly advanced semantic search by capturing complex language patterns and aligning retrieval processes with user intent. Their architecture, characterized by self-attention mechanisms, enables parallel processing of input sequences, crucial for discerning intricate dependencies and contextual relationships within text [14]. This capability is essential for understanding nuanced meanings in user queries, facilitating accurate information retrieval. The Massive Text Embedding Benchmark (MTEB) provides a framework for evaluating text embeddings across various tasks, aiding in understanding how transformers enhance semantic search capabilities [55]. By leveraging sophisticated neural architectures, transformers generate embeddings that reflect semantic and contextual nuances, thus improving the precision and relevance of retrieved information. Techniques like Agentic Reinforced Policy Optimization (ARPO) refine decision-making in uncertain scenarios by dynamically adjusting sampling strategies, further improving semantic search processes [14]. Innovations such as the Differentiable Search Index (DSI), generate-then-read strategies, and retrieval-augmented transformers are redefining semantic search, enabling direct query-to-document retrieval, contextual document generation, and vast external knowledge integration. These advancements lead to more precise and contextually aware retrieval solutions in applications like open-domain question answering and financial sentiment analysis [7,22,23,38,13]. As transformers continue to evolve, they are poised to further enhance the landscape of information retrieval by providing more effective, user-centric solutions, as evidenced by diverse applications and methodologies explored in recent research. Challenges and Limitations Deploying large language models (LLMs) in information retrieval systems involves navigating computational complexity, scalability, data dependency, and ethical challenges. Understanding these issues is crucial for enhancing operational efficiency and effectiveness. Computational Complexity and Scalability LLMs pose significant computational challenges due to their intricate architectures and extensive data needs. Traditional fine-tuning methods require adjustment of all model parameters, increasing resource demands and limiting scalability. Techniques like QLoRA address these challenges by efficiently backpropagating gradients through a frozen, quantized pretrained language model into Low Rank Adapters (LoRA), reducing computational costs while improving scalability [56]. However, LLMs often fail to recognize their knowledge boundaries, leading to an over-reliance on retrieval methods [21]. Training strategies such as RocketQA demand careful tuning across various datasets, further complicating scalability. Similarly, methods like FLARE face limitations due to the computational burden of continuous retrieval and processing [40]. These challenges highlight the need for efficient methodologies that adapt to diverse retrieval contexts without incurring significant computational overhead. Prompt tuning has emerged as a solution to the computational demands of tuning all model parameters, minimizing costs while enhancing scalability [21]. Generative retrieval techniques, competitive on smaller datasets, require further research to improve performance on larger corpora, underscoring the challenges of scaling these methods [34]. Strategies that restrict retrieval to only the knowledge the LLM lacks streamline the knowledge acquisition process and enhance scalability. Addressing these computational and scalability issues requires ongoing innovation in architecture, parallel processing, and data management. Enhancing LLM configurations, including parameter sizes and pre-training durations, can significantly improve in-domain accuracy, data efficiency, and capabilities in zero-shot generalization and multi-task learning, paving the way for more effective information retrieval solutions [25,26,57,39,27]. Balancing computational demands with scalability is vital to fully harnessing the potential of LLMs in retrieval systems. Data Dependency and Quality The performance of LLMs in information retrieval systems is closely tied to the quality and diversity of the datasets used during training and operation. High-quality datasets are crucial for generating accurate and contextually relevant outputs, as intent extraction requires substantial data to enhance generation results [31]. The reliance on the quality of underlying models and retrieval systems, as seen in RaLLe, underscores the critical dependency that can significantly impact model efficacy [32]. A major challenge in optimizing LLM performance is the limited diversity and quantity of training data, which can impair reasoning capabilities in real-world scenarios [16]. The specificity of datasets, as noted in evaluations of models like ChatGPT, may not encompass all facets of real-world search scenarios, potentially constraining their applicability [17]. Moreover, reliance on annotated data, as demonstrated in LaMDA, may not capture the full spectrum of human values, thereby affecting response quality in complex situations [33]. In specialized domains such as legal information retrieval, the efficacy of models like KELLER depends on the quality and comprehensiveness of integrated legal knowledge, emphasizing the need for robust data strategies [28]. Additionally, challenges remain regarding the scalability of emotional engagement across diverse user demographics and contexts in NLP applications [18]. The inability to scale generative retrieval methods without performance compromise further highlights the limitations imposed by data dependencies [34]. Efficient fine-tuning on small, high-quality datasets, as evidenced by QLoRA, can yield state-of-the-art results even with smaller models, demonstrating the potential for optimizing performance through strategic data management [56]. However, dependence on specific data sources, such as Git commits in Octopack, may not encompass all coding scenarios, potentially affecting model generalization [20]. Addressing these challenges necessitates continuous innovation in data collection, preprocessing, and management. Ensuring high-quality datasets and refining data strategies are vital for optimizing LLM performance, especially in information retrieval, where contextual accuracy is paramount. Striking a balance between data dependency and quality is essential to fully leverage LLMs' potential in enhancing retrieval systems. This includes addressing discrepancies between LLMs' pre-training objectives and specific tasks, alongside improving their management of factual knowledge boundaries through retrieval augmentation. Incorporating instruction-tuned LLMs and retrieval-augmentation modules can yield significant performance gains by drawing additional context from reliable external sources. Enhancing LLMs' awareness of internal and external knowledge conflicts is also critical for optimizing their application in tasks like financial sentiment analysis and open-domain question answering [13,11]. Bias and Ethical Considerations Integrating LLMs in information retrieval systems raises significant concerns regarding bias and ethical implications. These models often reflect biases present in their training data, potentially leading to skewed or discriminatory outputs. Findings from R2MED reveal a substantial gap between current retrieval techniques and the reasoning demands of real clinical tasks, highlighting the need for improved, unbiased medical retrieval systems [58]. The TrojRAG method illustrates potential ethical risks by identifying vulnerabilities in the retrieval components of RAG systems, which can be exploited to manipulate LLMs and introduce biases [59]. Such vulnerabilities complicate the ethical landscape surrounding LLM deployment. Existing benchmarks, like Measurings, may not comprehensively address all aspects of language model performance, potentially limiting the understanding of how biases manifest in LLM outputs [60]. Additionally, unanswered questions regarding the capabilities of very large models and their ethical implications remain [61], necessitating ongoing scrutiny and ethical oversight in LLM development and application. The WebDancer method discusses limitations related to scaling in complex scenarios and ethical considerations tied to data quality [62]. Ensuring high-quality, unbiased data is crucial for mitigating ethical risks and guaranteeing that LLMs provide equitable and accurate information. Approaches like Canqueryex improve ranker generalization without sacrificing performance, yet they require careful ethical consideration to prevent unintended biases during query expansion [63]. Balancing performance enhancements with ethical responsibility is essential for the fair deployment of LLMs in information retrieval systems. Addressing bias and ethical considerations necessitates a multifaceted approach, including rigorous data management, continuous evaluation of model outputs, and adherence to ethical guidelines throughout LLM development and deployment. As information retrieval (IR) evolves, prioritizing ethical considerations will be critical to fully harness the capabilities of LLMs, which have shown exceptional potential in text understanding, generation, and knowledge inference. While LLMs enhance IR systems through generative retrieval and improved user interactions, challenges related to computational costs, credibility, and domain-specific limitations persist. Tackling ethical issues, including biases and hallucinations in LLM outputs, is essential for ensuring reliability and verifiability of information. The Chinese IR community's strategic workshop in April 2023 underscored the transformative impact of LLMs on IR research, emphasizing the need for a novel technical paradigm that synergistically integrates IR models, LLMs, and human evaluators. Advancements in retrieval augmentation techniques can further enhance LLMs' awareness of factual knowledge boundaries and improve their ability to synthesize information from multiple sources, thereby safeguarding against ethical pitfalls and maximizing their contributions to information retrieval [13,11,25,26]. Future Directions Optimizing Model Efficiency Optimizing large language models (LLMs) for efficiency remains a pivotal focus in current research, emphasizing computational refinement and innovative prompt designs to advance information retrieval. Integrating prompt tuning with fewer labeled examples emerges as a promising strategy, facilitating effective model operation with reduced data requirements [33]. Novel retrieval techniques are essential for enhancing LLMs' judgment capabilities, thereby refining retrieval mechanisms and overall efficiency [28]. Research should prioritize expanding proxy model capabilities across diverse LLM types and knowledge domains, improving adaptability and performance in various contexts [18]. Refining dataset construction and enhancing query rewriting effectiveness are crucial for optimizing retrieval processes [34]. Advancements in autonomous reading processes and utilizing diverse datasets will bolster frameworks like PG-RAG, optimizing model efficiency [20]. Enhancing reward scheduling processes could significantly improve LLM adaptability and efficiency in dynamic retrieval tasks [19]. Improving feedback mechanisms and integrating advanced LLMs are vital for refining query generation processes, enabling precise and contextually relevant information retrieval [21]. Expanding model capabilities to manage larger datasets and exploring integration architectures are crucial for optimizing retrieval mechanisms [56]. Refining retrieval mechanisms, broadening dataset sources, and optimizing LLMs for superior performance are essential areas for future research [46]. The adaptive mechanisms of ARPO should be refined and applied beyond tested benchmarks to enhance model efficiency and effectiveness [40]. Expanding benchmarks to encompass a wider range of tasks and conducting additional safety evaluations will ensure robust LLM performance. Developing robust learning-to-reason frameworks, exploring new training methodologies, and addressing identified gaps are essential for enhancing LLM reasoning capabilities [40]. Research should focus on improving creativity in generated ideas and expanding scientific fields covered by models like The AI Scientist [21]. Enhancements to LRL, including adaptations for different retrieval tasks and performance improvements across languages, are vital for optimizing model efficiency in multilingual contexts [20]. Recent advancements position LLMs to offer enhanced efficiencies in information retrieval by excelling in text understanding, generation, and knowledge inference. These capabilities support generative retrieval, improve user understanding, and refine user-system interactions. Integrating LLMs with information retrieval (IR) systems creates a powerful paradigm where IR models deliver real-time, relevant data, LLMs contribute extensive internal knowledge, and users serve as demanders and evaluators. Challenges such as computational costs and credibility concerns persist. Efforts to enhance factual correctness and verifiability, including integrating citation generation and developing benchmarks for citation quality evaluation, highlight the potential for enhancing utility across diverse applications [25,26]. Exploration of Multilingual and Multimodal Capabilities Exploring multilingual and multimodal capabilities in large language models (LLMs) represents a significant frontier in advancing information retrieval systems. Emerging trends emphasize enhancing dynamic pre-training techniques, crucial for bridging gaps across diverse languages and contexts [2]. This approach optimizes LLMs for multilingual environments, ensuring effective processing and generation of content in multiple languages, thereby expanding applicability in global contexts. There is a growing emphasis on multimodal interactions, integrating various data formats such as text, images, and audio. This necessitates developing robust models capable of understanding and generating content across modalities, enhancing LLM versatility [31]. Leveraging multimodal data, LLMs can provide enriched information retrieval experiences aligning closely with user needs and preferences. Efforts to expand LLM capabilities across languages and modalities are crucial for enhancing modern information retrieval systems, as these models significantly improve text understanding, generation, and knowledge inference. Integrating LLMs with IR models can create a powerful technical paradigm addressing diverse user information needs, facilitating generative retrieval, and improving user-system interactions. Challenges such as computational costs, credibility concerns, and domain-specific limitations must be addressed. Enabling LLMs to generate text with citations is essential for improving factual correctness and verifiability, as current models often lack adequate citation support. Future directions include developing better retrievers, advancing long-context LLMs, and enhancing the synthesis of information from multiple sources [25,26]. As research progresses, focusing on multilingual and multimodal advancements will be essential for driving innovations and enhancing LLM effectiveness across various applications. Conclusion The exploration of large language models (LLMs) within the domain of information retrieval reveals transformative advancements, particularly through the utilization of transformer architectures. These models have significantly improved semantic search capabilities, demonstrating substantial efficacy across diverse natural language processing tasks. The incorporation of techniques like AttendOut has notably enhanced model performance, further establishing the transformative impact of LLMs on the field. Additionally, retrieval-augmented generation methods, exemplified by FlashRAG, provide a robust framework for the evaluation and comparison of LLM approaches, fostering continued research and development. A pivotal finding of this survey is the role of pre-trained language models (PLMs) in advancing dense retrieval processes, effectively modeling semantic relevance and improving retrieval accuracy. The integration of retrieval-augmented frameworks with LLMs has resulted in marked performance enhancements, with significant increases in accuracy and F1 scores when compared to traditional models. This highlights the transformative potential of these models in reshaping information retrieval systems. These developments have redefined the landscape of information retrieval by enhancing semantic understanding, scalability, and integration across multilingual and multimodal contexts. As research continues to focus on optimizing model efficiency and expanding the capabilities of LLMs, these models are poised to drive further innovations in information retrieval, underscoring their critical role in the evolution of sophisticated natural language processing applications.",
  "reference": {
    "1": "2502.15009v1",
    "2": "2003.08271v4",
    "3": "2504.03160v4",
    "4": "2305.03195v1",
    "5": "2310.03214v2",
    "6": "2504.21776v2",
    "7": "2402.13492v3",
    "8": "2412.14574v1",
    "9": "2306.07906v1",
    "10": "2402.06360v1",
    "11": "2307.11019v3",
    "12": "2408.06292v3",
    "13": "2310.04027v2",
    "14": "2507.19849v1",
    "15": "2403.08295v4",
    "16": "2212.10403v2",
    "17": "2304.09542v3",
    "18": "1801.01957v2",
    "19": "2307.12856v4",
    "20": "2308.07124v2",
    "21": "2202.08904v5",
    "22": "2112.04426v3",
    "23": "2202.06991v3",
    "24": "2501.17981v1",
    "25": "2305.14627v2",
    "26": "2307.09751v2",
    "27": "2408.12194v2",
    "28": "2406.19760v1",
    "29": "2206.10658v4",
    "30": "2405.16933v1",
    "31": "2303.04226v1",
    "32": "2308.10633v2",
    "33": "2201.08239v3",
    "34": "2305.11841v1",
    "35": "2507.02592v1",
    "36": "2311.03758v3",
    "37": "2505.16834v3",
    "38": "2209.10063v3",
    "39": "2402.12052v3",
    "40": "2305.06983v2",
    "41": "1908.07600v1",
    "42": "2108.07935v1",
    "43": "1810.04805v2",
    "44": "1802.05365v2",
    "45": "2106.09685v2",
    "46": "2502.18418v2",
    "47": "2406.16332v2",
    "48": "2304.03277v1",
    "49": "1706.03762v7",
    "50": "2202.01169v2",
    "51": "2112.09332v3",
    "52": "2211.14876v1",
    "53": "1705.01509v1",
    "54": "2202.05144v1",
    "55": "2210.07316v3",
    "56": "2305.14314v1",
    "57": "2305.09612v1",
    "58": "2505.14558v1",
    "59": "2406.00083v2",
    "60": "2411.04368v1",
    "61": "2303.18223v16",
    "62": "2505.22648v3",
    "63": "2311.09175v2"
  },
  "chooseref": {
    "1": "2303.04226v1",
    "2": "1711.08611v1",
    "3": "2206.02743v3",
    "4": "2307.12856v4",
    "5": "2310.09497v2",
    "6": "2303.18223v16",
    "7": "2406.14972v1",
    "8": "2311.16720v3",
    "9": "2406.14449v2",
    "10": "2503.02197v2",
    "11": "2305.06983v2",
    "12": "2501.04227v2",
    "13": "2507.19849v1",
    "14": "2311.11226v1",
    "15": "2007.00808v2",
    "16": "2407.15711v2",
    "17": "2208.03299v3",
    "18": "2508.12800v3",
    "19": "1706.03762v7",
    "20": "2411.19443v1",
    "21": "2204.10628v1",
    "22": "1910.13461v1",
    "23": "2104.08663v4",
    "24": "1810.04805v2",
    "25": "2406.00083v2",
    "26": "2303.17564v3",
    "27": "2504.12516v1",
    "28": "2501.17981v1",
    "29": "2311.09175v2",
    "30": "2311.09210v2",
    "31": "2501.14342v3",
    "32": "2310.09949v4",
    "33": "2302.05578v2",
    "34": "2404.13556v1",
    "35": "2310.16146v1",
    "36": "2404.11791v1",
    "37": "2011.01060v2",
    "38": "2308.16753v1",
    "39": "2502.15009v1",
    "40": "2108.10510v1",
    "41": "2503.23427v3",
    "42": "2208.07652v1",
    "43": "2402.01176v2",
    "44": "2402.06360v1",
    "45": "2409.19753v4",
    "46": "2407.12529v2",
    "47": "2311.00587v2",
    "48": "1802.05365v2",
    "49": "2502.01142v2",
    "50": "2504.03160v4",
    "51": "2503.00223v3",
    "52": "2501.12948v1",
    "53": "2402.03300v3",
    "54": "2406.16332v2",
    "55": "2004.04906v3",
    "56": "2211.14876v1",
    "57": "2205.09073v2",
    "58": "2305.18290v3",
    "59": "2305.13729v1",
    "60": "2003.06713v1",
    "61": "2311.12955v1",
    "62": "2206.07682v2",
    "63": "2405.16933v1",
    "64": "2305.14627v2",
    "65": "2310.09716v2",
    "66": "2310.04027v2",
    "67": "2305.15294v2",
    "68": "2402.06334v1",
    "69": "2406.15657v1",
    "70": "2409.17745v3",
    "71": "2405.13576v2",
    "72": "2310.07712v2",
    "73": "2310.03214v2",
    "74": "1801.01957v2",
    "75": "2306.09938v1",
    "76": "2403.20327v1",
    "77": "2403.08295v4",
    "78": "2209.10063v3",
    "79": "2305.07477v1",
    "80": "2305.03195v1",
    "81": "2211.09110v2",
    "82": "1809.09600v1",
    "83": "2305.11841v1",
    "84": "2112.04426v3",
    "85": "2204.07496v4",
    "86": "2401.00368v3",
    "87": "2307.09751v2",
    "88": "2301.01820v4",
    "89": "2202.05144v1",
    "90": "2311.01555v1",
    "91": "2304.03277v1",
    "92": "2212.10509v2",
    "93": "2203.05115v2",
    "94": "2307.11019v3",
    "95": "2304.09542v3",
    "96": "2201.08239v3",
    "97": "2005.14165v4",
    "98": "2203.13224v2",
    "99": "2112.07899v1",
    "100": "2311.03758v3",
    "101": "2305.09612v1",
    "102": "2306.17563v2",
    "103": "2408.12194v2",
    "104": "2303.06573v2",
    "105": "2108.07935v1",
    "106": "2406.19760v1",
    "107": "2406.14848v2",
    "108": "2402.15838v3",
    "109": "2302.13971v1",
    "110": "2406.15319v3",
    "111": "2106.09685v2",
    "112": "2307.03172v3",
    "113": "2405.16420v1",
    "114": "2210.07316v3",
    "115": "2411.04468v1",
    "116": "2411.04368v1",
    "117": "2407.20183v2",
    "118": "2410.07095v6",
    "119": "1705.01509v1",
    "120": "2405.17428v3",
    "121": "2505.23885v2",
    "122": "2308.07124v2",
    "123": "2212.09741v3",
    "124": "1908.07600v1",
    "125": "2107.13586v1",
    "126": "2003.08271v4",
    "127": "2212.10496v1",
    "128": "2101.00190v1",
    "129": "2201.05273v4",
    "130": "2107.08329v1",
    "131": "2209.11755v1",
    "132": "2305.14314v1",
    "133": "2305.03653v1",
    "134": "2303.07678v2",
    "135": "2206.10658v4",
    "136": "2503.05592v2",
    "137": "2505.14558v1",
    "138": "2505.20046v1",
    "139": "2301.12652v4",
    "140": "2306.05212v1",
    "141": "2405.14431v1",
    "142": "2308.10633v2",
    "143": "2505.14432v1",
    "144": "2503.06034v1",
    "145": "2312.02969v1",
    "146": "2502.18418v2",
    "147": "2404.18185v1",
    "148": "2309.15088v1",
    "149": "2408.01875v2",
    "150": "2508.07050v2",
    "151": "2305.07001v1",
    "152": "2307.02046v6",
    "153": "2503.19470v3",
    "154": "2301.00303v1",
    "155": "2002.08909v1",
    "156": "2402.13492v3",
    "157": "2010.08191v2",
    "158": "2202.08904v5",
    "159": "2503.04625v2",
    "160": "2307.16645v1",
    "161": "2503.09516v5",
    "162": "2310.05002v1",
    "163": "2310.11511v1",
    "164": "1612.01627v2",
    "165": "2505.16834v3",
    "166": "2505.15444v1",
    "167": "2412.14574v1",
    "168": "2402.12052v3",
    "169": "2307.08303v5",
    "170": "2310.06770v3",
    "171": "2211.09260v2",
    "172": "2203.11147v1",
    "173": "2201.10005v1",
    "174": "2104.14133v2",
    "175": "2306.11644v2",
    "176": "2508.09539v3",
    "177": "2408.06292v3",
    "178": "2104.08691v2",
    "179": "2505.16410v1",
    "180": "2405.14589v1",
    "181": "2406.11678v2",
    "182": "2306.10933v4",
    "183": "2212.10403v2",
    "184": "2202.06991v3",
    "185": "1705.03551v2",
    "186": "2303.00807v3",
    "187": "2202.01169v2",
    "188": "2505.22648v3",
    "189": "2306.07906v1",
    "190": "2112.09332v3",
    "191": "2507.02592v1",
    "192": "2507.15061v1",
    "193": "2504.21776v2",
    "194": "2508.05748v3",
    "195": "2402.11457v2",
    "196": "2305.02156v1",
    "197": "2505.04588v2"
  }
}