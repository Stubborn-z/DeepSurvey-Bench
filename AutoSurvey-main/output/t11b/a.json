{
    "survey": "# Diffusion Model-Based Image Editing: A Comprehensive Survey\n\n## 1 Introduction\n\n### 1.1 Evolution and Rise of Diffusion Models\n\nThe evolution and rise of diffusion models represent one of the most significant advancements in generative artificial intelligence (AI), offering a paradigm shift from traditional generative adversarial networks (GANs) and variational autoencoders (VAEs). This subsection traces their development, highlighting key innovations that established diffusion models as a leading framework for high-fidelity image synthesis and editing.\n\nThe foundations of diffusion models emerged from principles in non-equilibrium thermodynamics, Markov chains, and stochastic differential equations (SDEs). Early work formalized the denoising process through Denoising Diffusion Probabilistic Models (DDPM) [1], which introduced a systematic approach to corrupting data with Gaussian noise and learning its reversal via iterative refinement. This framework was later optimized by Denoising Diffusion Implicit Models (DDIM) [1], which reduced sampling time by redefining the reverse process as a deterministic trajectory. These innovations addressed critical limitations of GANs—such as mode collapse and training instability [2]—and VAEs, which often produced blurry outputs due to their fixed prior distribution [3]. Diffusion models surpassed both in sample quality and diversity, as evidenced by their superior FID scores in tasks like unconditional and conditional generation [4].\n\nA pivotal advancement was the introduction of latent diffusion models (LDMs) [5], which operated in a compressed latent space to reduce computational overhead while preserving perceptual quality. This enabled high-resolution synthesis, exemplified by text-to-image models like Stable Diffusion [5], which leveraged textual prompts for controlled generation. Unlike GANs, diffusion models inherently supported multimodal conditioning, allowing precise alignment of text and images [6] and seamless integration of masks or other inputs for editing [7]. Such flexibility proved invaluable in applications ranging from medical imaging [8] to 3D scene generation [9].\n\nTheoretical robustness further distinguished diffusion models. Grounded in a probabilistic framework [10], they avoided the adversarial vulnerabilities of GANs and ensured reliable inversion and translation [11]. This reversibility enabled tasks like inpainting and super-resolution with exceptional contextual consistency [12]. Computational efficiency also improved dramatically: techniques like progressive distillation [13] and feature caching [14] reduced sampling steps without sacrificing quality, making diffusion models practical for real-time use.\n\nBeyond general synthesis, diffusion models excelled in specialized domains. In medical imaging, they generated anatomically plausible data while preserving spatial context [15], outperforming GANs in tasks like histopathology image synthesis [16]. Their adaptability extended to 3D generation [17] and controllable scene modeling [18], underscoring their versatility.\n\nIn summary, diffusion models have risen to prominence through theoretical elegance, superior sample quality, and unmatched controllability. Their ability to integrate diverse conditioning signals, coupled with computational optimizations, has solidified their role as the leading generative framework—setting the stage for the next section’s exploration of their core principles in image editing. As research addresses challenges like efficiency and ethics [19], diffusion models continue to push the boundaries of generative AI.\n\n### 1.2 Core Principles of Diffusion Models\n\nDiffusion models have emerged as a transformative framework for generative AI, building upon their theoretical foundations (discussed in the previous section) to enable unprecedented control in image editing tasks. Their unique iterative denoising approach—comprising forward and reverse processes, noise scheduling, and latent representations—provides the structural backbone for high-fidelity generation and manipulation. These core principles directly enable the advanced editing applications explored in subsequent sections, from inpainting to 3D scene composition.\n\nThe **forward process** systematically corrupts input data through a Markov chain of Gaussian noise additions, following a predefined variance schedule. This deterministic transformation gradually maps the data distribution to a tractable noise distribution (typically isotropic Gaussian), creating a structured trajectory for the reverse process to learn. In image editing, this noise injection serves as a foundational operation: [20] demonstrates how the forward process generates editable latent representations that support object manipulation while preserving spatial relationships. The process's incremental nature ensures smooth transitions between editing states, avoiding the abrupt artifacts common in GAN-based approaches.\n\nThe learned **reverse process** acts as the creative engine, where a neural network (typically a U-Net) iteratively denoises data by predicting and removing noise at each timestep. This denoising trajectory becomes highly adaptable for editing through conditional guidance—whether via text prompts, masks, or other control signals. For instance, [21] introduces a rectifier module that dynamically adjusts the reverse process to maintain original image fidelity during edits, while [22] optimizes text embeddings to steer attention mechanisms for realistic modifications. The reverse process's step-wise refinement is particularly suited for editing tasks requiring precise local adjustments, such as object insertion or texture replacement.\n\nCentral to both processes is the **noise schedule**, which governs the rate and magnitude of noise addition/removal. Empirical studies like [23] reveal that schedule design directly impacts output quality, especially for high-resolution editing. Adaptive schedules can prioritize coarse structural edits in early timesteps and fine details later—a principle exploited by [24] to enable pixel-level control over edit strength. This temporal granularity allows diffusion models to outperform traditional methods in preserving context during complex edits.\n\nLatent space representations further enhance editing efficiency and quality. By operating in a compressed latent space (as pioneered by [5]), diffusion models reduce computational overhead while maintaining perceptual fidelity. The structured nature of this space enables intuitive edits: [11] shows how latent codes from disparate models can align naturally for cross-domain editing, while [25] introduces a noise-space formulation supporting direct attribute manipulation. Theoretical insights from [26] reveal that semantic edits can be achieved by traversing geometrically meaningful latent directions—enabling intuitive control over features like lighting or pose without retraining.\n\nThe reversibility inherent in diffusion processes also enables novel editing paradigms. [27] extends this concept through distribution interpolation, outperforming traditional methods in image translation tasks. Meanwhile, [28] demonstrates that pretrained models inherently encode editable semantic attributes, eliminating the need for task-specific fine-tuning.\n\nThese principles collectively establish diffusion models as a unified framework for generative editing. Their noise-based formulation provides inherent robustness against mode collapse, while the latent space structure enables both global and local manipulations—qualities that will be further exemplified in the following sections covering applications from medical imaging [29] to video editing. As research continues to refine these core mechanisms, the boundary between generation and editing will further dissolve, enabling ever more intuitive and powerful creative tools.\n\n### 1.3 Applications in Image Editing\n\nDiffusion models have rapidly evolved from their initial role as powerful generative tools to becoming indispensable frameworks for a wide array of image editing tasks. This transition has been driven by their iterative denoising process, which enables precise control over both global and local modifications, as well as their ability to leverage latent representations and multimodal conditioning—principles established in the core foundations of diffusion models. The versatility of diffusion models has been demonstrated across numerous applications, including inpainting, style transfer, object manipulation, and beyond, making them a cornerstone of modern image editing pipelines. These capabilities align with the key advantages of diffusion-based editing, such as high-fidelity outputs and robustness to complex data distributions, which are further explored in subsequent sections.\n\n### Inpainting  \nOne of the most prominent applications of diffusion models is **inpainting**, where missing or corrupted regions are filled in a semantically coherent manner. Traditional methods often struggled with realistic texture synthesis and context preservation, but diffusion models excel due to their iterative denoising process. For instance, [30] introduces a cascaded diffusion model fine-tuned for inpainting, leveraging object detectors to propose masks during training, ensuring alignment with text prompts. Similarly, [31] automates mask generation by contrasting predictions from diffusion models conditioned on different prompts, enabling precise inpainting without manual annotation. These advancements highlight how diffusion models combine automation with high-quality output, a strength further elaborated in the discussion of multimodal conditioning.\n\n### Style Transfer  \nAnother critical application is **style transfer**, where the aesthetic style of one image is applied to another while preserving content. Early methods relied on pixel-level optimization, often resulting in artifacts. Diffusion models, however, leverage latent representations for more nuanced transfers. For example, [32] manipulates self-attention layers to substitute key and value features from style images, preserving structure while transferring textures. Additionally, [33] integrates text prompts and style images to guide generation, adhering to semantic and stylistic constraints. These methods demonstrate high-quality style transfer without extensive fine-tuning, showcasing the inherent stochasticity and flexibility of diffusion models.\n\n### Object Manipulation  \n**Object manipulation**—such as removal, insertion, and rearrangement—benefits from diffusion models' ability to understand and modify complex scenes. For instance, [34] fine-tunes on a counterfactual dataset to synthesize edits that maintain physical consistency (e.g., shadows and reflections). Similarly, [35] optimizes layered scene representations during sampling, enabling flexible object rearrangement. These techniques underscore the models' capacity for intricate object-level edits, a capability rooted in their robust handling of multimodal distributions.\n\n### Interactive and Point-Based Editing  \nDiffusion models have also enabled **interactive and point-based editing**, bridging generative power with user control. [36] extends DragGAN to diffusion models, optimizing latent codes for precise object positioning using UNet features. [37] introduces feature-space rotation to handle in-plane rotations, a challenging scenario. These methods highlight the potential of diffusion models to combine high-fidelity generation with intuitive interfaces.\n\n### Video and 3D Scene Editing  \nThe application of diffusion models extends to **video editing**, where temporal consistency is critical. [38] combines low-resolution spatio-temporal information with high-resolution synthesis for text-based motion and appearance editing. [39] ensures inter-frame consistency for edited objects. In **3D scene editing**, [40] edits NeRFs without retraining, while [41] generates 3D-consistent views using depth-conditioned diffusion models. These innovations demonstrate the scalability of diffusion models across domains.\n\n### Cross-Domain Compositing  \nFinally, diffusion models excel in **cross-domain compositing**, seamlessly integrating elements from different domains. [42] iteratively refines injected objects with contextual information, eliminating domain-specific training. [43] treats images as object amalgamations, enabling fine-grained control over structure and appearance. These applications underscore the adaptability of diffusion models to complex editing scenarios.\n\nIn summary, the transition of diffusion models from pure generation to advanced editing tasks has been marked by their ability to handle diverse applications with unprecedented fidelity and flexibility. From inpainting and style transfer to object manipulation and video editing, diffusion models have set new benchmarks in quality and usability, building on their core principles and advantages. As research continues, their role in image editing is poised to expand further, unlocking new creative and practical possibilities.\n\n### 1.4 Key Advantages of Diffusion-Based Editing\n\n### 1.4 Key Advantages of Diffusion-Based Editing  \n\nThe widespread adoption of diffusion models in image editing, as demonstrated by their diverse applications in Section 1.3, stems from their unique advantages over traditional generative frameworks like GANs and VAEs. These strengths—ranging from high-fidelity outputs to multimodal conditioning and inherent stochasticity—address longstanding challenges in generative editing while enabling new creative possibilities. Below, we systematically analyze these advantages, grounding our discussion in empirical research and highlighting their implications for the field.  \n\n#### **High-Fidelity Outputs**  \nDiffusion models excel in generating and editing images with exceptional detail and realism, overcoming key limitations of GANs, such as mode collapse and artifacts. Their iterative denoising process progressively refines noisy inputs into high-quality outputs, preserving fine-grained textures and structural coherence. For instance, [21] introduces a rectifier module to correct errors during denoising, demonstrating that larger inversion steps enhance reconstruction fidelity. However, the study also notes that conditional Markovian processes can accumulate errors, which may limit editing precision.  \n\nFurther, [44] reveals that diffusion models achieve robust performance across diverse configurations (e.g., noise schedules, samplers) due to their underlying dynamics and architecture. This consistency ensures reliable high-quality outputs for tasks like inpainting and style transfer, as evidenced in Section 1.3.  \n\n#### **Multimodal Conditioning**  \nA defining feature of diffusion models is their ability to integrate diverse conditioning signals—text, sketches, masks, or reference images—to guide edits with precision. This capability aligns with their applications in text-guided inpainting (e.g., [31]) and style transfer (e.g., [32]). For example, [45] addresses self-conditioning degradation during training, ensuring tighter alignment between text prompts and generated images. Similarly, [7] combines text and mask inputs to edit facial attributes while preserving structural constraints, showcasing the flexibility of multimodal control.  \n\n#### **Inherent Stochasticity for Diverse Edits**  \nThe stochastic nature of diffusion models enables diverse, non-deterministic edits from a single input, a critical advantage for creative applications. This variability arises from the iterative noise-addition process, which allows multiple plausible solutions for tasks like inpainting or style transfer. [46] demonstrates that this stochasticity remains consistent across frameworks, ensuring reproducibility without sacrificing diversity. [47] further validates this property in reinforcement learning, where diffusion models generate varied policies from a learned latent space.  \n\n#### **Robustness to Complex Data Distributions**  \nUnlike GANs and VAEs, which struggle with multimodal data due to Lipschitz constraints, diffusion models inherently handle complex distributions. [48] contrasts diffusion models with push-forward methods, showing their superior ability to capture diverse modes. [27] extends this advantage to image translation, using diffusion bridges to interpolate between paired distributions while maintaining stability.  \n\n#### **Efficiency and Scalability**  \nDespite their computational demands, recent optimizations have improved the practicality of diffusion models. [49] reduces FLOPs by 32% through step reuse, while [50] employs distillation to achieve high-quality outputs in fewer steps. These advancements address scalability concerns raised in Section 1.5, though challenges remain in balancing speed and fidelity.  \n\n#### **Ethical and Creative Flexibility**  \nDiffusion models offer nuanced control, enabling both ethical mitigation of biases and creative exploration. [51] uses counterfactual generation to reduce dataset biases, while [20] introduces intuitive spatial editing tools. However, as Section 1.5 discusses, ethical risks like misuse and bias amplification require ongoing attention.  \n\n#### **Conclusion**  \nThe advantages of diffusion-based editing—high fidelity, multimodal conditioning, stochasticity, and robustness—underpin their transformative impact across applications (Section 1.3) while informing solutions to computational and ethical challenges (Section 1.5). These strengths are empirically validated, from artistic synthesis to medical imaging, positioning diffusion models as a cornerstone of modern generative AI. Future advancements will likely expand their capabilities, but current research already demonstrates their unparalleled versatility and potential.\n\n### 1.5 Challenges and Motivation for the Survey\n\n### 1.5 Challenges and Motivation for the Survey  \n\nWhile diffusion models have demonstrated transformative potential in image editing, as highlighted in Section 1.4, their adoption is impeded by significant computational, ethical, and methodological challenges. These limitations underscore the need for a comprehensive survey to consolidate research efforts and guide the field toward sustainable and equitable advancements.  \n\n#### **Computational Challenges**  \nThe resource-intensive nature of diffusion models poses a major barrier to accessibility and scalability. Training and inference often demand extensive GPU resources, limiting their use for researchers and practitioners with constrained infrastructure [52]. Large-scale models like Stable Diffusion or DALL-E require thousands of GPU hours, raising environmental concerns due to their substantial carbon footprint [53; 54]. This issue is exacerbated by the trend toward increasingly larger models, which prioritize performance over sustainability [55].  \n\nAlthough optimization techniques such as distillation and sparse inference are emerging, they remain nascent. Methods like Denoising Diffusion Implicit Models (DDIM) improve sampling speed but often sacrifice edit fidelity. The absence of standardized benchmarks to evaluate these trade-offs further hinders progress.  \n\n#### **Bias and Fairness Concerns**  \nDiffusion models risk perpetuating and amplifying biases present in their training data, leading to ethically problematic outputs. For instance, text-guided editing systems may reinforce stereotypes or marginalize underrepresented groups due to skewed datasets [56; 57]. Such biases are particularly consequential in sensitive applications like face editing or medical imaging [58; 59].  \n\nCurrent debiasing approaches often focus on post-hoc corrections rather than addressing root causes in data collection and model design [60]. Fairness metrics like demographic parity are frequently misapplied without contextual nuance, highlighting the need for a unified framework for bias mitigation in diffusion-based editing [61].  \n\n#### **Ethical and Societal Risks**  \nThe democratization of diffusion models introduces ethical dilemmas, including misuse for deepfake generation, non-consensual image manipulation, and copyright infringement [62; 63]. Tools like MidJourney or Stable Diffusion have blurred the line between creative expression and exploitation [64].  \n\nExisting ethical guidelines, such as those in the EU AI Act, lack enforceability and specificity for diffusion models [65]. Additionally, model opacity complicates accountability, as users struggle to trace the provenance of edited outputs [66]. The exclusion of marginalized communities from model development further exacerbates these issues [67].  \n\n#### **Methodological Fragmentation**  \nThe rapid advancement of diffusion-based editing has resulted in a fragmented research landscape. Techniques like latent space manipulation and attention mechanisms are often developed in isolation, limiting cross-disciplinary integration. For example, text-guided editing methods using CLIP may not align with spatial conditioning frameworks, hindering reproducibility [68].  \n\nBenchmarking inconsistencies further stifle progress. While metrics like Fréchet Inception Distance (FID) or CLIP scores are widely adopted, they fail to capture domain-specific nuances in editing tasks [69].  \n\n#### **Motivation for the Survey**  \nThis survey addresses these challenges by synthesizing disparate methodologies, identifying gaps, and proposing actionable solutions. It consolidates optimization strategies to bridge the efficiency gap, emphasizing innovations like adaptive sampling and hardware acceleration. Additionally, it critiques bias mitigation frameworks, advocating for inclusive data practices and interdisciplinary collaboration [61; 70].  \n\nEthically, the survey highlights the need for transparent governance and participatory design [71]. By documenting misuse cases and regulatory responses, it provides a roadmap for responsible deployment [65]. Methodologically, it advocates for unified evaluation protocols and task-specific benchmarks.  \n\nUltimately, this survey aims to foster a paradigm shift toward sustainable, equitable, and reproducible diffusion-based editing. By framing future directions—such as real-time editing and multimodal fusion—as collaborative opportunities, it encourages a cohesive vision for the field [72].\n\n### 1.6 Scope and Organization of the Survey\n\n---\n1.6 Scope and Organization of the Survey  \n\nBuilding upon the challenges and motivations outlined in Section 1.5, this survey provides a structured synthesis of diffusion model-based image editing, bridging foundational principles, methodological advancements, and domain-specific applications. The organization is designed to guide readers through the technical evolution of diffusion models while addressing critical gaps in efficiency, fairness, and reproducibility. Below, we detail the survey’s structure, thematic categorization, and open problems to establish a cohesive roadmap for the field.  \n\n### Survey Structure and Themes  \n\nThe survey is organized into ten interconnected sections, progressing from theoretical foundations to emerging trends:  \n\n1. **Introduction (Section 1)**: Contextualizes the rise of diffusion models, their advantages over GANs and VAEs, and their transition to editing tasks, while framing the survey’s scope and objectives.  \n\n2. **Fundamentals of Diffusion Models (Section 2)**: Explores core principles, including forward/reverse processes, noise scheduling, and architectures like DDPM and DDIM [73], laying the groundwork for subsequent techniques.  \n\n3. **Techniques for Diffusion-Based Image Editing (Section 3)**: Surveys methodologies such as text-guided editing, latent space manipulation, and hybrid approaches, emphasizing real-time control and interactivity [74].  \n\n4. **Controllable and Conditional Editing (Section 4)**: Examines fine-grained control via spatial conditioning, semantic guidance, and multi-modal integration [75], addressing alignment with user intent.  \n\n5. **Applications in Domain-Specific Editing (Section 5)**: Highlights practical uses in medical imaging, artistic style transfer, and video manipulation [76], showcasing adaptability to real-world needs.  \n\n6. **Efficiency and Optimization Strategies (Section 6)**: Discusses distillation, sparse inference, and hardware acceleration [77], targeting scalability for resource-constrained settings.  \n\n7. **Challenges and Limitations (Section 7)**: Critically analyzes computational costs, bias amplification, and ethical risks, echoing concerns raised in Section 1.5.  \n\n8. **Comparative Analysis and Benchmarking (Section 8)**: Evaluates diffusion models against other generative frameworks using metrics like FID and CLIP scores [78], addressing domain-specific evaluation gaps.  \n\n9. **Future Directions and Open Problems (Section 9)**: Explores emerging trends (e.g., real-time editing, multimodal fusion) and unresolved challenges in consistency and sustainability [79].  \n\n10. **Conclusion (Section 10)**: Synthesizes key insights and calls for interdisciplinary collaboration to advance the field.  \n\n### Categorization of Techniques and Applications  \n\nTo clarify the landscape, we group techniques into three themes:  \n\n1. **Input-Driven Editing**: Leverages external inputs (e.g., text prompts) for semantic control [80].  \n2. **Structure-Aware Editing**: Preserves/modifies structural elements via attention or spatial conditioning [81].  \n3. **Interactive Editing**: Enables real-time user control through dynamic conditioning.  \n\nApplications are categorized by domain:  \n- **Creative**: Artistic style transfer, photorealistic synthesis.  \n- **Scientific**: Medical imaging, computational pathology.  \n- **Dynamic**: Video and 3D scene editing.  \n\n### Open Problems and Research Gaps  \n\nKey unresolved challenges include:  \n1. **Efficiency**: Real-time high-resolution editing remains elusive despite optimization advances.  \n2. **Fairness**: Bias mitigation requires robust, fairness-aware training frameworks.  \n3. **Consistency**: Long-term coherence in sequential edits (e.g., video) is underexplored.  \n4. **Sustainability**: The environmental impact of large-scale training demands greener alternatives.  \n\n### Reader’s Guide  \n\nThis survey caters to diverse audiences:  \n- **Beginners**: Focus on Sections 1–3 for foundations.  \n- **Advanced Researchers**: Prioritize Sections 4–9 for state-of-the-art methods.  \n- **Practitioners**: Refer to Sections 5–6 for domain-specific optimizations.  \n\nBy unifying fragmented methodologies and emphasizing interdisciplinary solutions, this survey aims to accelerate progress toward sustainable, equitable, and reproducible diffusion-based editing.  \n\n---\n\n## 2 Fundamentals of Diffusion Models\n\n### 2.1 Forward and Reverse Processes\n\n### 2.1 Forward and Reverse Processes in Diffusion Models  \n\nThe forward and reverse processes form the core operational framework of diffusion models, enabling their powerful generative capabilities. These processes are inspired by thermodynamic principles and stochastic dynamics, where data undergoes systematic corruption (forward process) followed by iterative reconstruction (reverse process). This subsection provides a comprehensive analysis of these processes, including their mathematical foundations, practical implementations, and their critical role in high-quality image synthesis.  \n\n#### Forward Process: Gradual Noise Corruption  \nThe forward process is defined as a Markov chain that incrementally adds Gaussian noise to an initial data sample \\( x_0 \\) (e.g., an image) over \\( T \\) timesteps, producing a sequence of increasingly noisy variants \\( x_1, x_2, \\ldots, x_T \\). At each step \\( t \\), the noise injection follows a predefined schedule \\( \\beta_t \\), which governs the noise variance. The transition is modeled as:  \n\n\\[\nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}),\n\\]  \n\nwhere \\( \\mathcal{N} \\) denotes a Gaussian distribution with mean \\( \\sqrt{1 - \\beta_t} x_{t-1} \\) and covariance \\( \\beta_t \\mathbf{I} \\). This ensures isotropic noise addition while preserving the underlying data structure. Crucially, the forward process admits a closed-form expression for any timestep \\( t \\):  \n\n\\[\nq(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}),\n\\]  \n\nwhere \\( \\bar{\\alpha}_t = \\prod_{s=1}^t (1 - \\beta_s) \\). This property enables efficient training by allowing direct sampling of intermediate noisy states without simulating the entire chain [1].  \n\nThe forward process is designed to be irreversible, asymptotically transforming the data distribution \\( q(x_t) \\) into a standard Gaussian \\( \\mathcal{N}(0, \\mathbf{I}) \\) as \\( t \\to T \\). This guarantees that the reverse process can begin from pure noise and progressively denoise it to generate realistic samples. The choice of noise schedule \\( \\beta_t \\)—whether linear, cosine, or learned—significantly impacts training stability and output quality [5].  \n\n#### Reverse Process: Learned Denoising  \nThe reverse process is the generative phase, where a neural network learns to approximate the true posterior \\( q(x_{t-1} | x_t) \\) by iteratively denoising the data. Starting from \\( x_T \\sim \\mathcal{N}(0, \\mathbf{I}) \\), the model reconstructs the sample through a learned Markov chain:  \n\n\\[\np_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),\n\\]  \n\nHere, \\( \\mu_\\theta \\) and \\( \\Sigma_\\theta \\) are predicted by a neural network parameterized by \\( \\theta \\). In practice, the network is often trained to predict the noise \\( \\epsilon \\) added at each step, simplifying the objective to:  \n\n\\[\n\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[82].\n\\]  \n\nThis formulation aligns with denoising score matching and ensures efficient training [83]. The reverse process refines samples through sequential denoising steps, with quality heavily dependent on the noise prediction network’s accuracy. Techniques like classifier-free guidance and dynamic thresholding further enhance fidelity and controllability [84].  \n\n#### Theoretical Foundations  \nThe forward and reverse processes are intrinsically linked through stochastic differential equations (SDEs). The forward process discretizes a continuous-time SDE that corrupts data with noise, while the reverse process solves the corresponding reverse-time SDE, requiring estimation of the score function \\( \\nabla_{x_t} \\log q(x_t) \\). This perspective unifies diffusion models with score-based generative models, providing a rigorous theoretical framework [10].  \n\nThe reversibility of the diffusion process is pivotal: by learning to invert noise addition, the model captures the data distribution and generates coherent samples. However, the reverse process is computationally demanding, necessitating multiple network evaluations per timestep. Innovations like progressive distillation and latent diffusion mitigate this cost while preserving quality [13; 85].  \n\n#### Practical Implementation  \nKey optimizations enhance the efficiency and scalability of these processes:  \n- **Noise Scheduling**: The schedule \\( \\beta_t \\) balances noise addition rate with training stability. For example, cosine schedules slow noise injection mid-process to improve detail generation [5].  \n- **Sampling Acceleration**: Methods like DDIM (Denoising Diffusion Implicit Models) reduce sampling steps without quality loss [11].  \n- **Architecture Design**: U-Nets are widely adopted for noise prediction due to their multi-scale feature capture. Enhancements like cross-attention layers enable complex conditioning (e.g., text or class labels) [6].  \n\n#### Conclusion  \nThe forward and reverse processes are the mechanistic backbone of diffusion models, enabling high-fidelity generation through systematic noise corruption and learned denoising. Grounded in stochastic theory and refined via architectural and algorithmic innovations, these processes underpin the versatility of diffusion models in generative tasks [86]. Their interplay with noise scheduling and latent spaces (discussed next) further amplifies their potential in applications like image editing and synthesis.\n\n### 2.2 Noise Scheduling and Latent Spaces\n\n### 2.2 Noise Scheduling and Latent Spaces  \n\nBuilding upon the forward and reverse processes described in Section 2.1, noise scheduling and latent spaces emerge as critical mechanisms that govern the efficiency and quality of diffusion models. These components work synergistically: noise scheduling dictates the progression of corruption and denoising, while latent spaces provide a compressed yet expressive representation for computational tractability. Together, they enable diffusion models to achieve high-fidelity image generation and editing, forming a bridge to the architectural innovations discussed in Section 2.3.  \n\n#### Noise Scheduling Strategies  \n\nThe noise schedule determines the trajectory of the diffusion process, balancing the rate of noise addition (forward process) and removal (reverse process). Its design directly impacts training stability, sampling speed, and output quality.  \n\n1. **Linear and Cosine Schedules**: Early diffusion models adopted linear schedules, where noise variance increases uniformly across timesteps. However, this approach often neglects the varying sensitivity of image features to noise at different stages. Cosine schedules address this by slowing the noise injection rate mid-process, allowing the model to prioritize fine details during later denoising steps [5]. This adjustment aligns with the coarse-to-fine generation behavior intrinsic to diffusion models.  \n\n2. **Adaptive and Learned Schedules**: Recent advances introduce dynamic scheduling to optimize noise levels for specific tasks or datasets. For instance, [23] shows that larger images benefit from noisier schedules due to spatial redundancy. Learned schedules automate this adaptation by optimizing noise levels during training [87], outperforming fixed schedules through task-specific tailoring.  \n\n3. **Time-Dependent Modulation**: Techniques like \"resolution chromatography\" [88] analyze how different image resolutions emerge over timesteps. This insight informs schedules that emphasize global structures early and local details later, mirroring human visual perception. Such modulation enhances the interpretability and controllability of the generation process.  \n\n4. **High-Resolution Optimization**: Scaling to larger resolutions requires careful noise calibration. [89] proposes attenuating less significant components early, reducing computational overhead while preserving quality. This strategy is particularly relevant for editing tasks requiring precise spatial control.  \n\n#### Latent Spaces: Efficiency and Controllability  \n\nLatent spaces compress high-dimensional data into lower-dimensional representations, enabling efficient training and inference without sacrificing perceptual quality. Their structure also influences the model’s ability to perform nuanced edits.  \n\n1. **Latent Diffusion Models (LDMs)**: By operating in the latent space of a pretrained autoencoder, LDMs decouple perceptual compression from generative modeling [5]. This separation drastically reduces computational costs, making high-resolution synthesis and editing feasible—a principle leveraged in architectures like DDIM (Section 2.3).  \n\n2. **Disentangled Representations**: Disentangled latent codes isolate attributes such as shape or texture, enabling precise manipulation. For example, [90] demonstrates how such representations facilitate targeted edits like style transfer or object reconfiguration.  \n\n3. **Inversion and Editing**: Real-image editing requires accurate inversion into the latent space. [91] achieves this through coupled noise vectors, ensuring high-fidelity reconstruction. Similarly, [25] designs a noise space that preserves semantic structure, simplifying operations like color adjustments.  \n\n4. **Cross-Modal Alignment**: Latent spaces from related domains (e.g., natural images and art) often exhibit inherent alignment. [11] exploits this property for unpaired image translation, suggesting broader applicability in multi-modal tasks.  \n\n#### Impact on Generation and Editing  \n\nThe interplay between noise scheduling and latent spaces defines key trade-offs:  \n- **Coarse-to-Fine Generation**: Both components reinforce the model’s ability to first establish global structure and then refine details. [92] links this behavior to noise schedules, while [26] traces its roots to latent space dynamics.  \n- **Efficiency-Quality Balance**: Techniques like computation reuse [49] and layer caching [93] optimize this trade-off, enabling faster inference without quality loss.  \n- **Robustness**: Diffusion models exhibit resilience to perturbations, partly due to latent space properties. [94] finds they outperform GANs and VAEs in handling adversarial noise.  \n\nIn summary, noise scheduling and latent spaces are pivotal in shaping the behavior of diffusion models. Their optimization continues to advance applications like real-time editing and high-resolution synthesis, while maintaining strong theoretical ties to the core processes outlined in Section 2.1 and the architectures explored in Section 2.3. Future directions may include dynamic schedule learning and hierarchical latent representations to further enhance controllability and efficiency.\n\n### 2.3 Key Architectures: DDPM and DDIM\n\n### 2.3 Key Architectures: DDPM and DDIM  \n\nThe foundational architectures of Denoising Diffusion Probabilistic Models (DDPM) and Denoising Diffusion Implicit Models (DDIM) have established the theoretical and practical framework for modern diffusion-based image generation and editing. Building upon the noise scheduling and latent space principles discussed in Section 2.2, these architectures demonstrate how diffusion processes can be optimized for quality, efficiency, and controllability—properties that are further extended by the reversibility concepts explored in Section 2.4. This subsection provides a comprehensive analysis of DDPM and DDIM, examining their architectural innovations, comparative advantages, and implications for image editing tasks.  \n\n#### Denoising Diffusion Probabilistic Models (DDPM)  \n\nDDPM formalizes the diffusion process as a Markov chain of gradual noise addition and denoising steps, leveraging the noise scheduling strategies introduced in Section 2.2. The forward process incrementally corrupts an input image through a fixed Gaussian noise schedule, while the reverse process learns to denoise by predicting the noise component at each timestep. This noise-prediction objective, coupled with a simplified variational lower bound (VLB), stabilizes training and enables high-fidelity generation.  \n\nThe forward process is governed by a predefined variance schedule, which determines the weighting between the original image and added noise at each step. The reverse process, typically implemented with a U-Net architecture, iteratively refines the noisy latent by removing predicted noise. While DDPM excels in generating diverse, high-quality samples [95], its reliance on lengthy Markov chains (e.g., 1000 steps) for sampling limits real-time applicability. This limitation motivated the development of faster alternatives like DDIM, which bridges to the reversibility principles discussed in Section 2.4.  \n\n#### Denoising Diffusion Implicit Models (DDIM)  \n\nDDIM addresses DDPM's computational inefficiency by reparameterizing the diffusion process as a non-Markovian trajectory, enabling deterministic sampling through an ordinary differential equation (ODE) formulation. This innovation reduces sampling steps by an order of magnitude (e.g., 50 steps) while preserving quality, making it practical for interactive editing tasks. The ODE-based reverse process aligns with the theoretical foundations of reversibility explored in Section 2.4, where deterministic trajectories ensure consistent edits.  \n\nDDIM also introduces latent interpolation, enabling smooth transitions between images by manipulating their latent representations. This property is exploited in [24] for pixel-level editing control and in [20] for efficient interactive manipulation. The deterministic nature of DDIM further ensures temporal coherence in video editing [96], a critical requirement for multi-frame consistency.  \n\n#### Comparative Analysis and Hybrid Architectures  \n\nDDPM and DDIM represent complementary approaches: DDPM's Markovian framework provides theoretical robustness, while DDIM's non-Markovian design prioritizes speed. Hybrid architectures have emerged to combine their strengths. For example, [21] integrates DDIM's sampling efficiency with DDPM's noise-prediction framework, and [97] enhances both architectures with attention mechanisms for precise text-guided editing.  \n\n#### Practical Implications and Future Directions  \n\nDDPM's iterative refinement suits high-precision tasks like inpainting or style transfer, whereas DDIM's speed enables real-time applications such as point-based editing [36] and video manipulation [38]. Future research may explore:  \n1. **Scalability**: Distillation techniques like [50] to bridge quality-efficiency gaps.  \n2. **Generalization**: Hybrid architectures as proposed in [98].  \n3. **Controllability**: Extensions like [99] for complex multi-object edits.  \n\nIn summary, DDPM and DDIM form the backbone of diffusion-based image editing, each excelling in distinct scenarios. Their evolution continues to drive advances in efficiency and controllability, as seen in recent works like [100], while maintaining strong connections to the broader theoretical framework of diffusion models.\n\n### 2.4 Theoretical Foundations of Reversibility\n\n### 2.4 Theoretical Foundations of Reversibility  \n\nThe theoretical foundations of reversibility in diffusion models bridge the architectural innovations of DDPM and DDIM (Section 2.3) with the efficiency optimizations explored in Section 2.5. At its core, reversibility enables diffusion models to invert the noising process, transforming noisy inputs back into coherent samples—a property formalized through stochastic differential equations (SDEs) and ordinary differential equations (ODEs). This section examines how reversibility underpins computational efficiency and editability, while addressing open challenges in the field.  \n\n#### **Reversibility in Stochastic and Deterministic Processes**  \nThe forward process in diffusion models gradually corrupts data via a predefined noise schedule, while the learned reverse process denoises it iteratively. This invertibility is theoretically grounded in continuous-time SDEs, where the reverse SDE is derived using the score function—a neural network approximating the gradient of the data distribution’s log-density [46]. The deterministic counterpart, modeled as an ODE, enables faster sampling by defining a unique trajectory from noise to data [101]. These frameworks ensure that generated samples faithfully reconstruct the original distribution, as demonstrated in [102], which incorporates conditional information without violating reversibility.  \n\n#### **Computational Efficiency and Reversibility**  \nReversibility directly impacts efficiency by enabling reduced sampling steps. For instance, consistency models exploit reversibility to map noise to data in a single step [103], while [49] reuses computations across timesteps. Theoretical studies like [104] show that accurate score learning permits shorter diffusion times, further optimized by techniques such as balanced loss scaling [105]. These advances align with the efficiency goals discussed in Section 2.5, where distillation and sparse inference mitigate computational costs.  \n\n#### **Reversibility and Editability**  \nFine-grained control in image editing relies on manipulating the reverse process. Text prompts, for example, steer denoising trajectories via conditioned score functions [45], while [20] encodes images into modifiable latent elements. The robustness of reversibility is highlighted in [106], where corrupted inputs are inverted through regularized score functions. Multi-modal editing further benefits from collaborative reverse processes [7], ensuring coherent outputs.  \n\n#### **Challenges and Open Problems**  \nDespite its theoretical elegance, reversibility faces practical limitations:  \n1. **Error Accumulation**: Imperfect score matching degrades sample quality, addressed partially by contractive DPMs [107].  \n2. **Diversity Trade-offs**: Excessive contraction in the reverse process may reduce output variability, as analyzed in [108].  \n3. **Multimodal Distributions**: Push-forward models struggle with complex distributions, suggesting diffusion models as a viable alternative [48].  \n\nFuture directions include refining reverse-process approximations [109] and quantifying trade-offs between reversibility, efficiency, and control. These efforts will further unify the theoretical framework with practical applications, as explored in subsequent sections on optimization (Section 2.5) and advanced editing techniques.\n\n### 2.5 Efficiency and Optimization\n\n### 2.5 Efficiency and Optimization  \n\nThe computational intensity of diffusion models poses significant challenges for real-world image editing applications, where efficiency is often as critical as output quality. Building on the theoretical foundations of reversibility discussed earlier—which enable the denoising process to be inverted—this subsection explores practical techniques to optimize diffusion models for editing tasks. These methods aim to reduce inference time, minimize memory overhead, and maintain high-fidelity outputs, addressing the gap between theoretical capabilities and practical deployment [110].  \n\n#### **1. Distillation and Model Compression**  \nModel distillation has emerged as a key strategy for improving efficiency while preserving the editability enabled by reversibility. By training a smaller student model to replicate the behavior of a larger teacher diffusion model, distillation reduces the number of sampling steps required during inference. Progressive distillation, for instance, iteratively refines the student model to match the teacher's outputs, enabling fewer steps without sacrificing edit fidelity [111]. Complementary techniques like quantization (reducing numerical precision) and low-rank approximations (decomposing weight matrices) further compress diffusion models, making them viable for edge devices [112].  \n\n#### **2. Sparse Inference and Adaptive Sampling**  \nTo avoid redundant computations, sparse inference techniques focus computational resources on regions requiring significant edits. For example, sparsified attention mechanisms prioritize salient features, while adaptive sampling dynamically adjusts the number of denoising steps based on edit complexity. Simple edits (e.g., color adjustments) may require fewer steps than structural changes (e.g., object insertion), aligning computational effort with task demands [110].  \n\n#### **3. Hardware Acceleration and Parallelization**  \nThe iterative nature of diffusion models benefits from hardware optimizations. GPUs and TPUs parallelize denoising steps, but specialized architectures (e.g., neuromorphic chips or FPGAs) offer further gains [113]. Mixed-precision training and inference leverage hardware-native support for lower precision arithmetic, while distributed computing frameworks partition workloads across nodes—though synchronization overhead remains a challenge [112].  \n\n#### **4. Latent Space Optimization**  \nOperating in compressed latent spaces—enabled by techniques like VAEs or hierarchical representations—reduces memory and computational costs. Disentangled latent spaces further improve efficiency by allowing targeted edits (e.g., modifying style without altering content), minimizing unnecessary computations [61].  \n\n#### **5. Hybrid and Conditional Architectures**  \nHybrid architectures combine diffusion models with other generative frameworks (e.g., GANs or VAEs) to leverage their complementary strengths. For instance, a GAN can generate an initial rough edit, which a diffusion model refines with fewer denoising steps. Conditional diffusion models incorporate user inputs (e.g., masks or text prompts) early in the process, focusing computations on relevant regions [114].  \n\n#### **6. Benchmarking and Trade-offs**  \nEvaluating efficiency-accuracy trade-offs requires domain-specific metrics. While FID and CLIP scores assess edit quality, FLOPs and inference time measure computational efficiency. Medical imaging, for example, demands higher fidelity than artistic style transfer, necessitating tailored optimizations. Human evaluations remain critical to capture perceptual nuances beyond automated metrics.  \n\n#### **7. Future Directions**  \nEmerging research explores energy-efficient training paradigms (e.g., green AI techniques) and dynamic architectures that adapt to input complexity [53]. Interdisciplinary collaboration is essential to address ethical and environmental implications, ensuring diffusion models are both practical and responsible [62].  \n\nIn summary, optimizing diffusion models for editing requires balancing algorithmic innovations, hardware advancements, and ethical considerations. While progress has been made in distillation, sparse inference, and latent space manipulation, challenges like real-time performance and energy sustainability persist [65]. Future work must continue to bridge the gap between theoretical reversibility and practical efficiency [71].\n\n## 3 Techniques for Diffusion-Based Image Editing\n\n### 3.1 Text-Guided Image Editing\n\n---\nText-guided image editing has emerged as a dominant application of diffusion models, enabling intuitive image manipulation through natural language prompts. This paradigm leverages the unique ability of diffusion models to condition generation on textual inputs, offering precise semantic control over edited outputs. The success of these methods stems from sophisticated alignment between text and image representations, often mediated by vision-language models like CLIP. This subsection systematically examines key methodologies in text-guided editing, including CLIP-based approaches, semantic alignment techniques, hybrid frameworks, and attention mechanisms, while discussing current challenges and future directions.\n\n### CLIP-Guided Editing Frameworks\nThe integration of CLIP embeddings has proven transformative for text-guided editing, creating a shared semantic space where images and text can interact. [6] demonstrates how CLIP-based alignment enhances cross-attention maps in diffusion models, yielding edits that better match textual descriptions. This approach excels in style transfer tasks, where prompts specify artistic or photorealistic styles. The work further shows that automatically generated captions can improve alignment, enabling more accurate interpretation of user intent. These CLIP-guided techniques establish a foundation for semantic-preserving edits without requiring structural modifications to the diffusion architecture.\n\n### Semantic Alignment and Structure Preservation\nBeyond basic CLIP integration, advanced semantic alignment techniques ensure edited images maintain structural coherence with the original content. [115] approaches editing through an inpainting paradigm, where diffusion models fill masked regions according to text prompts while preserving unmasked areas. This method demonstrates exceptional performance in both image inpainting and video editing applications, highlighting the importance of context-aware generation. The semantic alignment is achieved through careful conditioning of the denoising process, ensuring generated content respects spatial relationships and object boundaries present in the source image.\n\n### Hybrid Architectures for Enhanced Control\nRecent work has explored hybrid frameworks that combine diffusion models with other generative approaches to overcome inherent limitations. [116] introduces a novel method leveraging GANs' disentangled latent spaces to guide diffusion models. By transferring interpretable directions from pre-trained GANs, this approach achieves precise edits while maintaining the generative quality of diffusion models. The hybrid architecture addresses the lack of structured latent space in pure diffusion models, enabling more controlled and semantically meaningful modifications.\n\n### The Role of Attention Mechanisms\nCross-attention layers have become indispensable for text-guided editing, dynamically weighting the relevance of text tokens to image regions. [5] showcases how integrating cross-attention into latent diffusion models (LDMs) enables fine-grained control. Operating in compressed latent space, LDMs maintain high-resolution synthesis while supporting diverse conditioning signals including text prompts and bounding boxes. This work underscores how attention mechanisms provide the architectural foundation for precise, text-driven edits by establishing direct relationships between linguistic concepts and visual features.\n\n### Multi-Modal Extensions and Consistency\nText-guided editing has expanded to incorporate additional modalities like sketches or reference images. [7] presents a framework where uni-modal diffusion models collaborate for multi-modal control. A dynamic diffuser adaptively combines denoising steps from different models, enabling seamless integration of textual descriptions with visual cues like masks or sketches. This approach proves particularly effective for portrait editing, where multiple input modalities can specify complementary aspects of the desired output.\n\n### Current Challenges and Limitations\nDespite significant progress, text-guided editing faces several persistent challenges:\n1. **Prompt Ambiguity**: Handling vague or contradictory text instructions remains difficult\n2. **Adversarial Robustness**: Models may be vulnerable to carefully crafted adversarial prompts\n3. **Authenticity Verification**: As shown in [117], the high quality of diffusion outputs complicates detection of edited content\n4. **Computational Efficiency**: Real-time applications remain challenging due to iterative denoising\n\n### Future Directions and Optimizations\nEmerging solutions point toward several promising research avenues:\n- **Efficiency Improvements**: Techniques like those in [14] demonstrate how feature caching can significantly reduce computational overhead\n- **Multi-Modal Expansion**: Further integration of diverse input modalities beyond text\n- **Consistency Mechanisms**: Enhanced methods for maintaining spatial and temporal coherence in edits\n- **Interactive Systems**: Development of real-time editing interfaces for practical applications\n\nIn summary, text-guided image editing represents a rapidly evolving frontier in diffusion model applications. Through innovations in CLIP integration, semantic alignment, hybrid architectures, and attention mechanisms, researchers have created powerful tools for semantic image manipulation. While challenges remain in prompt interpretation, robustness, and efficiency, ongoing advances in model architectures and optimization techniques continue to expand the capabilities and accessibility of these systems.\n\n### 3.2 Latent Space Manipulation\n\n---\n\nLatent space manipulation has emerged as a powerful paradigm for diffusion-based image editing, building on the text-guided approaches discussed previously while laying the foundation for the attention mechanisms explored in subsequent sections. This technique enables fine-grained control over generated content by modifying intermediate representations, offering advantages over pixel-space editing in preserving semantic coherence. The structured and hierarchical nature of learned latent representations facilitates more intuitive and disentangled edits, as we systematically examine through key techniques including disentangled direction discovery, inversion methods, and geometric interpretations.\n\n### Foundations of Latent Space Editing\nThe latent space of diffusion models encodes rich semantic information that naturally propagates through the denoising process, creating a bridge between the text-conditioned generation discussed earlier and the localized attention controls to follow. [26] revealed the geometric structure of latent space $\\mathbf{x}_t$, where local basis vectors correspond to interpretable image attributes across timesteps. This structural understanding enables semantic edits like facial expression changes through targeted perturbations, complementing the text-guided approaches while offering more direct control. Similarly, [28] introduced the h-space concept, demonstrating how homogeneous linear subspaces can optimize reverse process trajectories for robust attribute manipulation.\n\n### Disentangled Latent Directions\nThe quest for disentangled directions mirrors the semantic alignment challenges in text-guided editing while providing more precise control over specific attributes (pose, color, texture). [25] established an edit-friendly noise space where linear operations on noise maps achieve transformations, preserving structure through timestep-aware disentanglement. This approach parallels the hybrid architectures discussed earlier but operates purely in latent space. [91] advanced this through coupled noise vectors that maintain invertibility while isolating content and style factors, offering an alternative to the attention-based localization methods that follow.\n\nClassifier-guided techniques further bridge latent manipulation with multimodal control. [11] used CLIP embeddings to align latent codes across domains, creating a unified framework that connects with the text-alignment approaches from previous sections. Similarly, [118] identified semantic boundaries in unconditional models, enabling pixel-aware edits that anticipate the spatial precision of attention mechanisms.\n\n### Inversion and Reconstruction\nThe inversion challenge—critical for editing real images—relates to the fidelity preservation issues in text-guided editing while informing subsequent attention-based methods. [119] revealed cross-model latent compatibility, a property exploited in [11] for transferring edits between frameworks. [120] addressed fidelity loss through decoupled diffusion paths, achieving reconstruction quality that supports the precise local edits enabled by attention mechanisms.\n\n### Geometric and Hierarchical Properties\nThe latent space's geometric structure provides the foundation for both global text-guided edits and local attention-based modifications. [26] demonstrated how timestep-specific edits correspond to coarse or fine attributes, informing the multi-scale control later achieved through attention. [88] quantified this hierarchy, showing how noise schedules prioritize low-frequency components—insights crucial for both latent manipulation and the attention-guided approaches that follow.\n\n### Applications and Limitations\nWhile excelling in semantic-precise tasks like face editing and style transfer, latent manipulation faces challenges in edit propagation and complex scenes—limitations that subsequent attention mechanisms aim to address. [20] demonstrated element-based editing that maintains realism, while [44] revealed architectural impacts on editability that inform both latent and attention-based approaches.\n\n### Future Directions\nEmerging techniques increasingly blend latent manipulation with other paradigms. [24] and [121] combine latent control with attention mechanisms, pointing toward hybrid solutions. Future work may explore dynamic latent scheduling or unified spaces, as suggested in [122], while [123] frames editing as posterior optimization—concepts that could enhance both latent and attention-based methods.\n\nIn summary, latent space manipulation forms a crucial link between text-guided generation and attention-based control, leveraging diffusion models' structured representations for precise editing. The geometric insights and disentanglement techniques developed here directly enable and inform the localized attention mechanisms discussed next, while building upon the semantic foundations established by text-guided approaches.\n\n### 3.3 Attention Mechanisms for Localized Editing\n\n---\n### 3.3 Attention Mechanisms for Localized Editing  \n\nBuilding upon the latent space manipulation techniques discussed earlier, attention mechanisms provide a complementary approach for achieving precise spatial control in diffusion-based image editing. These mechanisms enable localized modifications while preserving the integrity of unedited regions, addressing a key limitation of global latent space operations. This subsection analyzes the role of attention in localized editing, covering cross-attention guidance, self-attention regularization, and their integration into modern editing pipelines, while highlighting connections to both preceding and subsequent approaches.  \n\n#### Cross-Attention Guidance for Semantic Alignment  \nCross-attention layers in diffusion models establish a mapping between textual prompts and spatial features, making them ideal for text-guided localized editing. By manipulating cross-attention maps, users can direct edits to specific regions corresponding to semantic concepts in the prompt. [124] disentangles cross-attention interactions into item-prompt associations, enabling precise control over individual objects in an image. This approach optimizes cross-attention weights to link specific image regions to learned prompts, ensuring that edits are confined to target areas without affecting unrelated content. Similarly, [97] identifies leakage in cross-attention maps as a primary cause of unintended edits in background regions, proposing leakage repairment losses to dynamically adjust attention weights.  \n\nThe effectiveness of cross-attention guidance extends to multi-modal editing scenarios that bridge to the hybrid approaches discussed in the following subsection. [7] leverages cross-attention to harmonize text-driven and mask-driven edits, with its dynamic diffuser module predicting spatial-temporal influence functions for pre-trained uni-modal models. This synergy enables complex edits while preserving identity, demonstrating how attention mechanisms can complement conditional frameworks.  \n\n#### Self-Attention Regularization for Structural Consistency  \nWhile cross-attention aligns edits with external conditions, self-attention layers maintain internal structural coherence—a property that builds upon the geometric latent space properties discussed earlier. [37] exploits self-attention maps to track handle points during object rotation, comparing features between original and rotated views to enforce consistency in non-target regions. This technique demonstrates how self-attention can preserve the hierarchical relationships discovered in latent space analysis.  \n\nFor temporal editing tasks, [31] uses self-attention to propagate edits coherently across video frames. By contrasting self-attention maps from source and target denoising paths, the method minimizes artifacts in adjacent frames, showcasing how attention mechanisms can maintain consistency across multiple dimensions.  \n\n#### Hybrid Attention Mechanisms for Multi-Granular Control  \nThe integration of cross-attention and self-attention enables multi-granular control that combines the strengths of both latent space manipulation and attention guidance. [121] introduces a hierarchical attention mechanism where cross-attention guides coarse-level edits while self-attention refines fine details, mirroring the multi-scale capabilities observed in latent space operations.  \n\n[99] advances this concept by employing foreground masks to spatially constrain cross-attention updates while optimizing self-attention for background preservation. This approach achieves high-fidelity multi-object edits in a single pass, demonstrating how attention mechanisms can work in concert with spatial conditioning techniques that will be explored further in hybrid approaches.  \n\n#### Challenges and Innovations  \nAttention-based methods face challenges in handling complex scenes—a limitation shared with some latent space manipulation techniques. [22] addresses this through a Balanced Attention Module that dynamically adjusts the trade-off between textual guidance and image semantics, complementing similar efforts in latent space disentanglement.  \n\nThe computational overhead of attention operations, particularly at high resolutions, presents another challenge. [125] mitigates this by applying attention-guided edits at low resolutions before upscaling—an optimization strategy that parallels the efficiency improvements seen in hybrid frameworks.  \n\n#### Future Directions  \nFuture work could explore adaptive attention sparsification to reduce computational overhead, potentially drawing inspiration from the efficiency gains demonstrated in hybrid approaches. The integration of 3D-aware attention for scene-level editing [41] and combination with physical constraints [34] represent promising avenues that could bridge attention mechanisms with emerging conditional techniques.  \n\nIn summary, attention mechanisms serve as a vital complement to latent space manipulation, offering precise spatial control that maintains structural coherence. Their ability to interface with both text-based conditions and spatial masks creates natural connections to the hybrid and conditional approaches discussed next, while building upon the geometric foundations established in latent space analysis. As evidenced by [95], these techniques are rapidly evolving to address the complex demands of modern image editing workflows.\n---\n\n### 3.4 Hybrid and Conditional Approaches\n\n---\n### 3.4 Hybrid and Conditional Approaches  \n\nBuilding upon the attention mechanisms discussed in Section 3.3, hybrid and conditional approaches in diffusion models integrate auxiliary networks or explicit guidance to achieve more precise and efficient editing. These methods address key limitations of standalone diffusion models while preserving their generative capabilities, creating a natural transition to the interactive techniques explored in Section 3.5. This subsection examines frameworks that combine diffusion with other architectures and conditional editing techniques that leverage masks, references, or task-specific guidance.  \n\n#### Hybrid Frameworks: Augmenting Diffusion with Auxiliary Networks  \nHybrid approaches often enhance diffusion models by integrating complementary architectures, improving efficiency and control. For instance, [126] introduces a time-step-adaptive framework that dynamically allocates convolutional and self-attention operations based on frequency characteristics. This design reduces computational overhead by 3.3x while maintaining quality, demonstrating how hybrid architectures can optimize the trade-off between performance and efficiency—a challenge also noted in attention-based methods.  \n\nThe integration of reinforcement learning (RL) further expands diffusion capabilities. [127] reformulates denoising as a multi-step RL problem via Denoising Diffusion Policy Optimization (DDPO), enabling fine-grained control over outputs. This synergy between generative modeling and RL mirrors the precision achieved by attention mechanisms while introducing goal-directed optimization.  \n\n#### Conditional Editing: Precision Through Explicit Guidance  \nConditional techniques leverage masks, references, or spatial constraints to guide edits, building upon the localized control enabled by attention mechanisms. [20] encodes images into manipulable \"elements\" that users can spatially rearrange, with diffusion models decoding these edits into coherent outputs. Similarly, [21] introduces a rectifier module to correct error propagation during denoising, ensuring high-fidelity reconstruction—a challenge paralleled in attention-based temporal editing.  \n\nReference-based conditioning offers another dimension of control. [128] uses cluster-derived score functions to maintain character consistency across prompts, addressing mode collapse while preserving diversity. These methods exemplify how conditional mechanisms can complement the semantic alignment achieved through cross-attention.  \n\n#### Task-Specific and Compositional Applications  \nHybrid and conditional approaches excel in specialized scenarios, such as multi-task reinforcement learning or style transfer. [129] integrates Transformer backbones for generative planning, while [7] combines text, masks, and style codes for photorealistic edits. These applications demonstrate the versatility of conditional frameworks, bridging the gap between high-level semantic control and pixel-level precision—a theme further developed in interactive editing.  \n\n#### Challenges and Future Directions  \nDespite their advantages, these approaches face challenges in computational efficiency and conditional alignment. [49] reduces FLOPs by reusing denoising computations, while [130] mitigates bias in conditional generation. Future work could explore lightweight hybrid architectures [48] or advanced multimodal fusion [131], potentially drawing inspiration from efficiency gains in attention-based methods.  \n\nIn summary, hybrid and conditional approaches extend the capabilities of diffusion models by combining their strengths with auxiliary networks or explicit guidance. These methods not only address limitations in efficiency and control but also create a foundation for the interactive paradigms discussed next, marking a critical evolution in diffusion-based editing workflows.\n\n### 3.5 Interactive and Point-Based Editing\n\n### 3.5 Interactive and Point-Based Editing  \n\nBuilding upon the hybrid and conditional approaches discussed in Section 3.4, interactive and point-based editing techniques in diffusion models offer a more intuitive and precise paradigm for user-guided image manipulation. These methods bridge the gap between high-level semantic control and pixel-level precision, enabling real-time modifications through intuitive interactions like points, strokes, or region-based selections. This subsection explores how these techniques complement existing conditional approaches while addressing unique challenges in localized editing, setting the stage for the multi-modal techniques covered in Section 3.6.  \n\n#### Foundations of Interactive Editing  \nInteractive editing capitalizes on the iterative refinement capability of diffusion models, allowing dynamic incorporation of user inputs during the denoising process. Unlike the auxiliary network integrations discussed in Section 3.4, interactive methods often employ attention mechanisms or spatial conditioning to focus edits on user-specified regions while preserving global coherence. For instance, cross-attention layers can align sparse user inputs with latent representations, enabling precise control over object manipulation or attribute adjustment. This approach is particularly effective for tasks requiring localized changes, such as portrait retouching or object repositioning, where traditional text-guided methods may lack precision.  \n\n#### Point-Based and Region-Specific Control  \nPoint-based editing extends this paradigm by using sparse annotations as \"handles\" for geometric or semantic transformations. In facial editing, for example, marking keypoints allows natural expression manipulation by leveraging the model's understanding of facial structure—a capability that aligns with the conditional face generation techniques in [7]. Similarly, region-specific editing through masks enables targeted inpainting or object insertion, building on the spatial conditioning principles introduced in [20]. These methods demonstrate how diffusion models can infer complex edits from minimal user input while maintaining contextual consistency.  \n\n#### Challenges in Real-Time Implementation  \nDespite their advantages, interactive methods face significant hurdles in computational efficiency. The iterative nature of diffusion models often conflicts with the low-latency requirements of interactive tools. Recent solutions, such as latent space optimization or model distillation, aim to reduce computational overhead while preserving edit fidelity—a challenge also noted in hybrid frameworks like [49]. User experience design further complicates this, necessitating progressive rendering or intuitive interfaces to balance responsiveness and quality [132].  \n\n#### Ethical and Future Considerations  \nThe democratization of precise editing tools raises ethical concerns parallel to those in conditional generation, particularly regarding misuse for deepfakes or unauthorized alterations [133]. Future advancements may focus on three directions: (1) optimizing real-time performance through hardware-aware architectures, (2) developing more natural interfaces (e.g., gesture or voice control) [70], and (3) establishing ethical guidelines akin to those proposed for conditional methods [63].  \n\nIn summary, interactive and point-based editing techniques expand the versatility of diffusion models by integrating user creativity directly into the generative process. By addressing efficiency and ethical challenges, these methods pave the way for more accessible and responsible applications, from digital art to medical imaging. Their development reflects a broader trend in diffusion-based editing—toward increasingly intuitive and human-centered control mechanisms.\n\n### 3.6 Multi-Modal and Style Transfer Techniques\n\n### 3.6 Multi-Modal and Style Transfer Techniques  \n\nBuilding on the interactive and point-based editing methods discussed in Section 3.5, multi-modal and style transfer techniques in diffusion-based image editing enable richer creative control by integrating diverse input modalities—such as text, image exemplars, and style codes—to achieve sophisticated transformations. These approaches leverage the flexibility of diffusion models to manipulate visual content while preserving semantic coherence, bridging the gap between precise user-guided edits and the broader generative capabilities explored in Section 3.7 for 3D and video domains.  \n\n#### **Multi-Modal Conditioning in Diffusion Models**  \nMulti-modal conditioning expands the control mechanisms of diffusion models by incorporating textual descriptions, reference images, or other auxiliary inputs. Text-guided editing, for instance, allows users to specify edits through natural language prompts, which are encoded into the diffusion process. This aligns with the interactive paradigms of Section 3.5 but extends precision to semantic-level adjustments, such as altering artistic styles or object attributes [74].  \n\nKey to this advancement is the integration of pre-trained language models (e.g., CLIP) to bridge textual and visual representations. By aligning text embeddings with latent image features through cross-attention mechanisms, models like Stable Diffusion achieve fine-grained control over generated content [78]. Similarly, image exemplars enable style transfer by matching features in latent space, allowing users to apply color palettes, textures, or brushstrokes from a reference image to a target [134]. Hierarchical techniques further refine this by separating global and local stylistic attributes for selective application [135].  \n\n#### **Style Transfer Techniques**  \nDiffusion-based style transfer builds on neural style transfer but leverages the iterative denoising process to produce higher-quality and more diverse results. Unlike single-pass optimization methods, diffusion models refine outputs progressively, enabling nuanced stylistic adaptations [136].  \n\nA common approach involves conditioning the model on style codes extracted from reference images using encoders like VGG or ResNet. These codes are injected at specific timesteps, allowing control over the strength and locality of the transfer [80]. Innovations such as \"style interpolation\" further enhance creativity by blending multiple style codes to generate hybrid artistic effects, offering smooth transitions between stylistic extremes [137].  \n\n#### **Compositional Editing with Multi-Modal Inputs**  \nCompositional editing integrates multiple operations—style transfer, object insertion, background modification—into a single coherent output. Diffusion models excel here by combining complex conditioning signals, such as text prompts, style references, and spatial masks [138].  \n\nA critical challenge is maintaining consistency across edits. Attention mechanisms dynamically balance the influence of different modalities per region, while adversarial or perceptual losses enforce semantic alignment between edited and original content [139]. These solutions mirror the hybrid approaches of Section 3.4 and anticipate the temporal coherence requirements of video editing in Section 3.7 [77].  \n\n#### **Applications and Future Directions**  \nThese techniques find applications in digital art, e-commerce, and entertainment, enabling automated style adaptation and asset generation [140]. Open challenges include improving multi-modal fusion efficiency, robustness to abstract styles, and ethical safeguards against misuse.  \n\nIn summary, multi-modal and style transfer techniques represent a pivotal advancement in diffusion-based editing, offering unparalleled creative flexibility. By synthesizing diverse inputs, they pave the way for more intuitive and powerful tools, while setting the stage for the 3D and video extensions discussed next [79].\n\n### 3.7 3D and Video Editing Extensions\n\n### 3.7 3D and Video Editing Extensions  \n\nBuilding upon the multi-modal and style transfer capabilities discussed in Section 3.6, diffusion models have also been extended to 3D scenes and video editing, introducing new challenges in maintaining temporal and geometric consistency. While 2D image editing benefits from the model's ability to manipulate static content, 3D and video data require preserving coherence across frames or viewpoints—a task that demands specialized adaptations of the diffusion framework. This subsection explores advancements in diffusion-based 3D and video editing, focusing on techniques for ensuring temporal smoothness, geometric fidelity, and cross-modal alignment.  \n\n#### **3D Scene Synthesis and Editing**  \nThe application of diffusion models to 3D data involves generating or editing 3D structures, such as point clouds, meshes, or neural radiance fields (NeRFs). A key challenge lies in the high-dimensional nature of 3D representations and the need for geometric consistency across views. Recent work has addressed this by leveraging diffusion processes in function spaces, as proposed in [141], which generalizes diffusion models to infinite-dimensional spaces, enabling the generation of continuous 3D fields. This approach avoids discretization artifacts and allows for smooth interpolations, making it particularly suited for 3D data.  \n\nAnother promising direction is the use of diffusion models for neural rendering, where the goal is to synthesize photorealistic 3D scenes from sparse inputs. For instance, [142] introduces a framework for diffusion on manifolds, treating 3D space as a constrained domain to ensure geometric plausibility. Similarly, [143] extends diffusion models to non-Euclidean spaces, enabling the synthesis of 3D shapes with symmetries and invariances critical for realistic scene generation.  \n\nEditing 3D scenes with diffusion models often involves conditional generation guided by user inputs, such as sketches or semantic masks. For example, [144] proposes a method for interpolating between 3D shapes by solving the Schrödinger Bridge problem, ensuring smooth transitions while preserving geometric details. This technique is particularly useful for tasks like morphing or style transfer in 3D. Furthermore, [145] demonstrates how diffusion models can generate time-varying 3D scenes, such as fluid simulations or dynamic environments, bridging the gap to video editing.  \n\n#### **Video Editing and Temporal Consistency**  \nExtending diffusion models to video editing introduces the challenge of maintaining temporal coherence across frames. Unlike image editing, video editing requires accounting for motion dynamics and inter-frame dependencies. One approach treats video as a spatiotemporal volume, applying diffusion in this extended space. [146] explores this idea by formulating video generation as a stochastic process, where the forward diffusion adds noise spatially and temporally, and the reverse process learns to denoise while preserving motion smoothness.  \n\nAttention mechanisms play a critical role in capturing long-range temporal dependencies. For instance, [147] introduces a conditional diffusion framework guided by optical flow or motion cues, ensuring natural motion patterns in edited videos. Similarly, [148] adapts diffusion models for sequential data by conditioning on past frames, a technique directly applicable to video inpainting or frame interpolation.  \n\nEfficiency is another key challenge in video editing due to the high dimensionality of video data. [89] dynamically adjusts the dimensionality of the diffusion process, reducing computational overhead while preserving quality. This method is particularly effective for long videos. Additionally, [88] analyzes how coarse-to-fine sampling can improve temporal consistency by first establishing global motion and then refining local details.  \n\n#### **Cross-Modal and Hybrid Approaches**  \nCombining diffusion models with other generative frameworks has proven effective for 3D and video editing. For example, [149] highlights hybrid models integrating diffusion with GANs or VAEs to leverage their complementary strengths. In 3D editing, [150] introduces a discrete-state diffusion process for voxel-based 3D models, simplifying manipulation compared to continuous representations.  \n\nFor video editing, [151] combines diffusion with reinforcement learning to optimize temporal coherence, enabling tasks like video stylization with minimal artifacts. Similarly, [21] extends its rectifier module to video, ensuring consistent edits across frames.  \n\n#### **Challenges and Future Directions**  \nDespite progress, challenges remain in diffusion-based 3D and video editing. Computational costs for high-resolution generation are significant, often requiring distributed training or specialized hardware. [87] reviews optimization strategies like distillation and sparse inference, which could be adapted to these domains. Another challenge is the lack of standardized benchmarks for evaluating temporal and geometric consistency.  \n\nFuture research could explore integrating physical priors into diffusion models, as suggested by [152], to enable more realistic synthesis by incorporating physical laws. Additionally, [153] proposes disentangling latent factors in diffusion models, which could be extended to 3D and video for more controllable editing.  \n\nIn conclusion, diffusion models offer a powerful framework for 3D and video editing, but their success hinges on addressing temporal and geometric consistency challenges. Advances in function-space diffusion, hybrid models, and cross-modal conditioning are paving the way for more robust and efficient editing tools in these domains.\n\n## 4 Controllable and Conditional Editing\n\n### 4.1 Spatial Conditioning Techniques\n\n---\n4.1 Spatial Conditioning Techniques  \n\nSpatial conditioning techniques have become indispensable for achieving precise control in diffusion-based image editing. By incorporating auxiliary inputs like masks, sketches, or bounding boxes, these methods enable localized edits while preserving unmodified regions, making them essential for tasks such as inpainting, object manipulation, and compositional synthesis.  \n\n### Mask-Based Conditioning  \nA foundational approach involves using binary or soft masks to delineate editable regions. For instance, [83] introduces a context-aware diffusion framework that reconstructs masked regions by predicting neighborhood features, ensuring coherence with surrounding pixels. This method highlights how explicit spatial conditioning enhances fidelity in localized edits, particularly for inpainting. Similarly, [115] reformulates diffusion models as masked autoencoders, training them to denoise partially masked inputs. This approach not only improves reconstruction quality but also generalizes to video inpainting, demonstrating the versatility of mask-based conditioning.  \n\n### Sketch and User-Guided Editing  \nBeyond masks, sketch-based conditioning offers intuitive and fine-grained control. [7] proposes a multi-modal framework where hand-drawn sketches guide facial attribute editing, such as adjusting age or facial structure. By integrating sketches into the denoising process, the model achieves precise spatial alignment between user input and output. This technique is extended to 3D-aware synthesis in [18], where sketches define geometric layouts for interactive 3D sculpting while maintaining photorealism.  \n\n### Compositional and High-Resolution Synthesis  \nSpatial conditioning is pivotal for compositional editing, where multiple objects or styles are combined seamlessly. [5] leverages cross-attention layers to process spatial conditions like bounding boxes or segmentation maps, enabling object generation at specified locations. This architecture outperforms pixel-based methods in efficiency and scalability. Additionally, [154] uses depth maps as spatial signals to ensure geometrically plausible scene edits, reducing artifacts in complex compositions.  \n\n### Integration with Textual and Multimodal Cues  \nCombining spatial conditioning with text prompts expands editing capabilities. [6] aligns text-derived attention maps with spatial masks to refine edits. For example, a user can mask a dress region and specify \"change the dress color to red,\" with the model harmonizing textual and spatial cues. [155] further applies this idea to tasks like semantic segmentation, where spatial constraints are inferred directly from text prompts.  \n\n### Challenges and Solutions  \nA key challenge is maintaining consistency between edited and unedited regions. [117] addresses subtle statistical discrepancies by proposing post-hoc refinement based on local statistics. Conversely, [12] decouples the modeling of masked and unmasked regions, reducing interference during denoising. Their pixel spread model (PSM) iteratively refines boundaries for seamless inpainting.  \n\nScalability to high-resolution and video editing is another focus. [14] introduces feature caching to reduce computational overhead, benefiting large-scale spatial edits and video applications. Similarly, [156] enhances 3D generation by refining coarse inputs with multi-view spatial conditioning.  \n\n### Ethical and Practical Limitations  \nDespite progress, challenges remain. [157] warns of potential data memorization in small datasets, raising privacy concerns. Additionally, [158] notes that spatially edited images can evade traditional detection, necessitating new forensic tools.  \n\n### Future Directions  \nFuture research could explore adaptive conditioning mechanisms, such as the hierarchical approach in [159], where coarse constraints are refined into fine-grained masks. Multimodal integration, as in [160], combining text, sketches, and masks, also presents a promising avenue.  \n\nIn summary, spatial conditioning techniques are central to diffusion-based editing, enabling precise control through masks, sketches, and other spatial signals. While challenges like computational cost and ethical risks persist, innovations in architecture and training paradigms continue to advance the field. These methods lay the groundwork for subsequent advancements in semantic guidance and attention mechanisms, bridging the gap between low-level spatial control and high-level semantic editing.  \n---\n\n### 4.2 Semantic Guidance and Attention Mechanisms\n\n---\n4.2 Semantic Guidance and Attention Mechanisms  \n\nBuilding upon the spatial conditioning techniques discussed in Section 4.1, semantic guidance and attention mechanisms provide higher-level control over diffusion-based image editing by aligning edits with interpretable concepts. These techniques leverage the inherent structure of diffusion models, particularly their cross-attention layers and latent space representations, to achieve precise semantic control while maintaining spatial coherence. This subsection examines how these mechanisms bridge low-level spatial constraints with high-level editing intent, setting the stage for the multi-modal integration approaches covered in Section 4.3.  \n\n### Semantic Guidance in Diffusion Models  \nSemantic guidance enables conditioning the diffusion process on abstract concepts like object categories or textual descriptions. A foundational approach is demonstrated in text-to-image models like Stable Diffusion [5], where cross-attention layers align text embeddings with visual features. This principle is extended in [161], which systematically identifies five key manipulation points in diffusion models—including conditional embeddings and attention maps—to achieve diverse edits while preserving semantic coherence.  \n\nFor real-image editing, [22] introduces a Target-text Inversion Schedule (TTIS) to optimize text embeddings and a Balanced Attention Module (BAM) to harmonize textual guidance with visual fidelity. These innovations highlight the dual challenge of maintaining edit precision and naturalness, a theme that recurs in both spatial conditioning and multi-modal integration.  \n\n### Attention Mechanisms for Localized Editing  \nAttention maps serve as dynamic spatial constraints, complementing the explicit masks discussed in Section 4.1. [121] uses multi-level feature blending guided by attention maps to confine text-driven edits to target regions. Similarly, [25] reveals how attention mechanisms inherently encode semantic relationships, enabling coherent edits through noise-space manipulation. These methods demonstrate how attention bridges semantic intent with spatial precision—a capability further enhanced by the latent space analyses in [26].  \n\n### Interpretable Directions in Latent Space  \nThe discovery of structured latent spaces has advanced semantic guidance beyond text conditioning. [28] identifies an interpretable h-space with linear, homogeneous properties, enabling attribute editing (e.g., facial expressions) without retraining. This aligns with theoretical insights from [98], which shows stochastic differential equations (SDEs) better preserve semantic distributions during edits compared to ODEs—a finding critical for tasks like inpainting where spatial and semantic consistency intersect.  \n\n### Applications and Challenges  \nThese mechanisms enable diverse applications, from object replacement to style transfer. [24] introduces pixel-level edit control using attention maps, while [162] addresses the fidelity-editability tradeoff by optimizing critical denoising steps. However, challenges persist in balancing multi-modal inputs—a gap that motivates the integration frameworks discussed in Section 4.3.  \n\n### Future Directions  \nFuture work could explore hierarchical attention for multi-scale editing or standardized benchmarks like those proposed in [120]. The synergy between semantic guidance and emerging multi-modal methods promises more intuitive control paradigms.  \n\nIn summary, semantic guidance and attention mechanisms extend spatial conditioning by linking edits to interpretable concepts. By unifying high-level intent with low-level control, these techniques form a critical bridge between the spatial constraints of Section 4.1 and the multi-modal systems of Section 4.3, advancing the field toward more expressive and controllable editing.  \n---\n\n### 4.3 Multi-Modal Integration\n\n---\n4.3 Multi-Modal Integration  \n\nBuilding upon the semantic guidance and attention mechanisms discussed in Section 4.2, the integration of multiple modalities—such as visual, textual, and other sensory inputs—has emerged as a powerful paradigm for enhancing the controllability and expressiveness of diffusion-based image editing. This subsection explores how combining diverse information sources enables richer conditional control, allowing users to guide edits with greater precision and flexibility, while also setting the stage for the dynamic and hierarchical control approaches covered in Section 4.4.  \n\n### Foundational Principles and Methodologies  \n\nMulti-modal integration in diffusion models typically involves aligning features from different modalities (e.g., text, images, masks, or sketches) through mechanisms like cross-attention, extending the semantic control capabilities introduced earlier. A prominent approach is demonstrated in [7], which introduces a \"dynamic diffuser\" that adaptively combines denoising steps from pre-trained uni-modal models. This framework preserves the strengths of each modality—such as text-driven semantic control and mask-driven spatial precision—enabling collaborative editing without retraining the base model.  \n\nThe principle of disentangled control is further advanced in [43], which treats images as compositions of objects with separately controllable structure and appearance. This object-level approach aligns with the localized editing capabilities enabled by attention mechanisms, while extending them through multi-modal conditioning. Similarly, [124] demonstrates how disentangling item-prompt interactions allows precise object manipulation, bridging semantic guidance with multi-modal flexibility.  \n\n### Applications and Use Cases  \n\nThe synergy of multiple modalities unlocks advanced editing scenarios requiring both global coherence and local precision. For instance, [31] automates mask generation by contrasting predictions from text prompts, streamlining workflows that would otherwise require manual annotation. This builds on semantic guidance techniques while introducing visual-modal automation.  \n\nIn 3D and video editing, multi-modal integration addresses challenges of temporal and geometric consistency—a theme further developed in Section 4.4. [40] combines text prompts with 2D editing models to modify Neural Radiance Fields (NeRFs), while [38] integrates spatio-temporal video data with text-guided synthesis. These approaches demonstrate how multi-modal inputs can bridge dimensional gaps in editing tasks.  \n\nCreative applications also benefit from this paradigm. [163] unifies text, style, and structure inputs to produce cohesive artworks, while [164] uses multi-modal inputs to selectively erase irrelevant information—showcasing the versatility of combined modalities.  \n\n### Challenges and Limitations  \n\nDespite their advantages, multi-modal frameworks face challenges in modality alignment—a concern that echoes the attention leakage issues discussed in semantic guidance. [97] proposes a leakage repairment loss to refine cross-attention maps, highlighting the need for robust alignment mechanisms similar to those in semantic control approaches.  \n\nComputational efficiency remains another hurdle, particularly as these methods prepare the ground for more complex hierarchical control systems. While [165] optimizes cross-frame attention for real-time performance, scalability challenges persist for large-scale deployments.  \n\nEthical considerations also emerge, as multi-modal models may amplify training data biases. [166] proposes safeguards against misuse—a concern that becomes increasingly relevant as editing systems grow more powerful and accessible.  \n\n### Future Directions  \n\nFuture research could explore adaptive modality weighting to dynamically balance input influences—a concept that aligns with the hierarchical control approaches discussed in Section 4.4. [99] shows promise in task-specific conditioning, while [100] suggests pathways toward more efficient architectures.  \n\nExpanding beyond visual-textual modalities to include audio or tactile inputs could further enrich creative control, opening new possibilities for immersive media editing. This direction would build upon current multi-modal foundations while introducing new dimensions of user interaction.  \n\nIn conclusion, multi-modal integration represents a natural evolution of the semantic control principles covered earlier, while providing the foundation for more advanced dynamic and hierarchical approaches. By combining diverse inputs—as demonstrated in [7] and [43]—these frameworks enable unprecedented editing capabilities, though challenges in alignment, efficiency, and ethics must be addressed to realize their full potential.  \n---\n\n### 4.4 Dynamic and Hierarchical Control\n\n### 4.4 Dynamic and Hierarchical Control  \n\nDynamic and hierarchical control mechanisms represent a natural progression from the multi-modal integration approaches discussed in Section 4.3, offering adaptive strategies to manage conditioning signals across spatial, temporal, and semantic scales. These techniques are particularly valuable for complex editing tasks requiring both global coherence and local precision—a theme that will be further developed in Section 4.5's discussion of task-specific conditioning.  \n\n#### Hierarchical Conditioning in Spatial Domains  \nBuilding on the multi-modal foundations established earlier, hierarchical control leverages the inherent multi-scale architecture of diffusion models to apply conditioning signals at different resolutions. For instance, [7] extends the cross-modal integration paradigm by introducing dynamic diffusers that adaptively combine denoising steps from pre-trained uni-modal models. This approach ensures coherent integration of high-level semantic attributes (e.g., text-described age) with low-level geometric features (e.g., mask-defined face shape), maintaining the spatial precision enabled by attention mechanisms while adding hierarchical flexibility.  \n\nThe spatial disentanglement principles introduced in multi-modal editing are further refined in methods like [20], which encodes input images into manipulable \"image elements\" that preserve global consistency during localized edits. This aligns with the object-level control demonstrated in [43], while introducing dynamic scaling capabilities. Attention mechanisms play a crucial role here, as shown in [128], where cluster-conditioned models guide denoising trajectories across varying granularities—from coarse structural patterns to fine details.  \n\n#### Temporal Adaptation for Video and Sequential Data  \nThe temporal dimension introduces new challenges that build upon the spatial hierarchical control framework. Methods like [167] address these by incorporating flow-guided recurrent latent propagation—a concept that echoes the cross-frame attention optimizations seen in [165]. These approaches maintain the multi-modal integration benefits while adding temporal coherence through specialized modules that enforce both short-term consistency (via local temporal layers) and long-range dependencies (through global feature fusion).  \n\nThe reinforcement learning perspective introduced in [127] provides another dimension of dynamic control, framing denoising as a multi-step decision process. This connects to the adaptive conditioning strategies discussed later in Section 4.5, particularly through techniques like Denoising Diffusion Policy Optimization (DDPO) that balance exploration and exploitation across timesteps. The pixel-level refinement in [168] further demonstrates how hierarchical control can operate at multiple temporal scales simultaneously.  \n\n#### Adaptive Conditioning Granularity  \nA key advancement in hierarchical control is the dynamic adjustment of conditioning influence—a concept that bridges the multi-modal integration of Section 4.3 with the task-specific approaches of Section 4.5. [169] demonstrates this through noise-annealed conditioning schedules, where high-level prompts guide early generation while low-level details are incorporated progressively. This aligns with findings in [46], which show how diffusion models naturally operate across abstraction levels.  \n\nThe theoretical foundations of these approaches are further illuminated by [108], which draws parallels between denoising steps and physical phase transitions. This justifies frequency-domain strategies like those in [170], where different frequency bands receive targeted conditioning—a concept that will be extended in Section 4.5's discussion of disentangled latent spaces.  \n\n#### Efficiency and Implementation Considerations  \nPractical implementations of hierarchical control often focus on computational optimization, as seen in [126]'s specialized architectures for different timestep intervals. These efficiency gains—achieved by matching operations to specific granularity requirements—set the stage for the scalable compositional conditioning approaches discussed in Section 4.5. Similarly, insights from [104] about early information capture inform both current hierarchical methods and future research directions.  \n\n#### Future Directions  \nEmerging research avenues build upon the hierarchical foundations while pointing toward Section 4.5's themes. Cross-modal hierarchical control, as suggested by [131], could unify conditioning across modalities and scales. Meanwhile, [171] demonstrates how inherent feature disentanglement enables automated granularity selection—a capability that will be crucial for advancing task-specific conditioning.  \n\nIn summary, dynamic and hierarchical control mechanisms extend the multi-modal integration paradigm through adaptive, multi-scale conditioning strategies. By maintaining connections to both preceding and subsequent sections—from cross-modal alignment to task-specific refinement—these approaches provide a versatile framework for complex image editing challenges. The efficiency gains and theoretical insights they offer further enrich the foundation for emerging compositional conditioning techniques.\n\n### 4.5 Task-Specific and Compositional Conditioning\n\n### 4.5 Task-Specific and Compositional Conditioning  \n\nBuilding upon the dynamic and hierarchical control strategies discussed in Section 4.4, task-specific and compositional conditioning further refines diffusion-based image editing by enabling precise, context-aware manipulation through disentangled or combined conditional inputs. This approach is essential for complex editing tasks that demand both granular control and multi-modal adaptability, such as medical imaging, artistic synthesis, and interactive design.  \n\n#### Disentangling Conditions for Task-Specific Editing  \n\nDisentanglement techniques isolate latent representations of distinct attributes (e.g., shape, texture, or lighting) to facilitate independent editing. Hierarchical conditioning, a natural extension of the multi-scale methods in Section 4.4, organizes conditions into layers of granularity. For example, [56] demonstrates how disentangling demographic attributes from other visual features mitigates bias in generative outputs. Similarly, [57] highlights the role of disentangled latent spaces in reducing unintended correlations (e.g., between gender and profession in synthetic portraits).  \n\nSpatial disentanglement is particularly powerful for localized edits. [58] shows how hierarchical separation of global conditions (e.g., scene layout) from local conditions (e.g., object details) preserves coherence while enabling targeted modifications. This aligns with the spatial-domain hierarchical control methods in Section 4.4, where multi-scale conditioning ensures consistency across resolutions.  \n\n#### Compositional Conditioning for Multi-Modal Control  \n\nCompositional conditioning dynamically combines multiple conditions (e.g., text, masks, or reference images) to achieve complex edits. Cross-modal fusion techniques, such as conditional gating, selectively activate relevant conditional pathways during inference. For instance, [172] describes a gating mechanism that blends text prompts (e.g., \"sunset lighting\") with spatial masks (e.g., sky region) for localized stylistic changes. This mirrors the adaptive conditioning granularity discussed in Section 4.4, where signals are adjusted based on timestep or spatial scale.  \n\nEfficiency is a key advantage of compositional approaches. [55] notes that energy-efficient diffusion models leverage compositional conditioning to minimize redundant computations, similar to the dynamic architecture optimizations in [126].  \n\n#### Applications and Case Studies  \n\nTask-specific conditioning excels in domains requiring strict adherence to constraints. In medical imaging, diffusion models combine anatomical segmentation masks with diagnostic labels to synthesize pathology-aware edits (e.g., tumor insertion) while preserving healthy tissues, as explored in [71]. Artistic applications also benefit from compositional conditioning, though [67] cautions that biased training data can skew stylistic representations without careful condition curation.  \n\n#### Challenges and Limitations  \n\nDespite its potential, task-specific and compositional conditioning faces challenges:  \n1. **Conditional Interference**: Imperfect disentanglement may lead to unintended attribute interactions.  \n2. **Scalability**: Combining numerous conditions increases computational overhead, as noted in [113].  \n3. **Ethical Risks**: Biases can propagate if conditions encode discriminatory attributes, underscoring the need for audits, as argued in [60].  \n\n#### Future Directions  \n\nFuture research could explore:  \n- **Dynamic Condition Routing**: Adaptive networks that select conditions based on real-time input, extending the temporal adaptation strategies in Section 4.4.  \n- **Cross-Domain Compositionality**: Techniques to transfer conditions across domains (e.g., medical to artistic) while preserving semantics.  \n- **Ethical Frameworks**: Guidelines for condition design to align with fairness principles, building on insights from [46].  \n\nIn summary, task-specific and compositional conditioning advances diffusion-based editing by integrating hierarchical control with multi-modal flexibility. By addressing scalability and ethical challenges, this paradigm can unlock new possibilities for precision editing across diverse applications.\n\n## 5 Applications in Domain-Specific Editing\n\n### 5.1 Medical Imaging and Computational Pathology\n\n### 5.1 Medical Imaging and Computational Pathology  \n\nDiffusion models have emerged as a transformative tool in medical imaging and computational pathology, offering unprecedented capabilities in image synthesis, domain adaptation, and data augmentation. Their ability to generate high-fidelity, diverse, and realistic medical images addresses critical challenges such as limited annotated datasets, domain shifts, and privacy concerns. This subsection reviews key applications of diffusion models in medical imaging, structured into three focus areas: domain adaptation and synthesis, computational pathology, and ethical considerations with future directions.  \n\n#### Domain Adaptation and Image Synthesis  \n\nMedical imaging often faces domain shifts due to variations in scanners, protocols, or institutions. Diffusion models mitigate these shifts by synthesizing images that bridge domain gaps while preserving anatomical fidelity. For instance, [8] demonstrates how conditional diffusion models outperform GANs in generating abdominal CT images from semantic masks. The study highlights the importance of separate encoding for masks and input images, which enhances structural accuracy—a crucial advantage for downstream tasks like data augmentation.  \n\nFurther advancing this capability, [15] evaluates denoising diffusion probabilistic models (DDPMs) in learning spatial context. Using stochastic context models (SCMs), the study shows DDPMs excel at generating images that interpolate between training samples, a feat challenging for GANs. This property is vital for applications requiring precise spatial alignment, such as tumor segmentation. Quantitative comparisons reveal DDPMs achieve lower error rates in contextual accuracy, solidifying their suitability for medical imaging tasks.  \n\n#### Computational Pathology and Histopathology Image Analysis  \n\nIn computational pathology, diffusion models enable high-resolution synthesis of histopathology images, addressing the scarcity of annotated datasets. [16] combines vision transformers (ViTs) with diffusion autoencoders to generate histopathology images with intricate details. The framework outperforms GAN-based methods, which often suffer from mode collapse, and proves invaluable for rare diseases where real biopsies are limited.  \n\nHowever, the adoption of diffusion models in medical imaging requires caution. [157] reveals that diffusion models are more prone to memorizing training data than GANs, particularly for small datasets or 2D slices from 3D volumes. This poses risks for patient privacy and underscores the need for rigorous evaluation before clinical deployment.  \n\n#### Challenges and Ethical Considerations  \n\nDespite their potential, diffusion models face computational and ethical hurdles. Training and sampling remain resource-intensive, though techniques like [13] mitigate this by reducing sampling steps without quality loss—a critical advancement for real-time clinical workflows.  \n\nEthical concerns are equally pressing. [19] demonstrates that diffusion models can reproduce individual training samples, threatening patient confidentiality. Robust anonymization and differential privacy mechanisms must be integrated to safeguard sensitive data.  \n\n#### Future Directions  \n\nFuture research should focus on optimizing diffusion models for clinical use while exploring novel applications. For example, [173] suggests potential in 3D medical imaging by enabling synthetic organ reconstructions. Multi-modal approaches, inspired by [7], could integrate textual reports to generate clinically relevant images. Hybrid frameworks, such as those combining VAEs and GANs [174], may further enhance efficiency and diversity.  \n\nIn summary, diffusion models revolutionize medical imaging through high-quality synthesis and domain adaptation. However, their adoption must balance innovation with ethical rigor, addressing memorization risks and computational demands. Future advancements in 3D imaging and multi-modal integration promise to expand their impact in clinical settings.\n\n### 5.2 Artistic and Photorealistic Style Transfer\n\n### 5.2 Artistic and Photorealistic Style Transfer  \n\nDiffusion models have revolutionized artistic and photorealistic style transfer by enabling high-fidelity image synthesis with precise control over content-style blending. Unlike traditional optimization-based or adversarial approaches, diffusion-based methods leverage stochastic denoising and hierarchical conditioning to achieve nuanced transfers. This subsection examines key techniques, challenges, and advancements, structured into artistic (non-photorealistic) and photorealistic domains—bridging the medical imaging applications in Section 5.1 and the face/portrait editing focus in Section 5.3.  \n\n#### **Artistic Style Transfer**  \nArtistic style transfer adapts images to mimic distinctive artistic features (e.g., brushstrokes, color palettes). Diffusion models excel here by disentangling content and style through multimodal conditioning. For example, [21] introduces a rectifier module that adjusts diffusion model weights to preserve content structure while applying stylistic edits, avoiding artifacts common in GAN-based methods. Similarly, [161] leverages cross-attention maps and noise predictions to enable text-guided style manipulation (e.g., \"Van Gogh's Starry Night\"), offering granular control over abstract styles.  \n\nLatent space innovations further enhance artistic transfers. [11] formulates a shared Gaussian latent space for zero-shot style swapping, exploiting diffusion invertibility to align source and target styles without retraining. [90] isolates style attributes via a Group-supervised AutoEncoder (GAE), enabling multi-style fusion (e.g., \"cubism\" + \"watercolor\"). Challenges persist in balancing structural fidelity and style intensity: [44] shows excessive early-stage noise distorts content, while insufficient noise limits style adaptability. [23] addresses this with adaptive noise schedules, prioritizing higher noise levels for complex styles.  \n\n#### **Photorealistic Style Transfer**  \nPhotorealistic transfers alter style while preserving realism (e.g., lighting, weather conditions). Diffusion models achieve this by conditioning on both content and style references. [91] uses bidirectional inversion to edit high-fidelity images (e.g., daylight-to-sunset transitions) without error accumulation. Spatial conditioning techniques like [121] blend multi-level features for coherent global-local edits (e.g., summer-to-winter transfers with realistic snow textures). [118] further refines precision by using semantic boundaries to constrain style application (e.g., recoloring objects without shape distortion).  \n\nNoise scheduling plays a pivotal role: [88] reveals that low-frequency style components (e.g., illumination) emerge early in denoising, while high-frequency details (e.g., textures) resolve later. [50] capitalizes on this by distilling models into few-step samplers that prioritize low-frequency alignment, enabling real-time photorealistic edits like nighttime scene conversions with accurate shadows.  \n\n#### **Hybrid and Task-Specific Advances**  \nHybrid frameworks merge artistic and photorealistic capabilities. [175] introduces pixel-aware cross-attention to jointly optimize style fidelity and photorealism (e.g., portrait stylization with preserved skin textures). [22] balances text prompts and image semantics for mixed effects (e.g., \"photorealistic oil painting\"). Task-specific innovations include video style transfer, where [176] ensures temporal coherence, and 3D style manipulation, where [177] transfers materials (e.g., wood to metal) in neural renderings.  \n\n#### **Challenges and Future Directions**  \nKey challenges include computational inefficiency—[87] notes the need for hundreds of denoising steps, spurring research into sparse inference [49]. Bias in style representation is another concern; [44] highlights over-representation of dominant styles (e.g., Western art), necessitating diverse datasets.  \n\nFuture directions may focus on:  \n1. **Real-time interaction**: Techniques like [120] could enable instant edits.  \n2. **Cross-modal transfers**: Integrating audio or physics-based constraints [24] for richer stylization.  \n\nIn summary, diffusion models redefine style transfer through latent space manipulation, adaptive noise scheduling, and multimodal conditioning—setting the stage for their application in face editing (Section 5.3) while building on medical imaging synthesis (Section 5.1). Addressing scalability and bias will be critical for broader adoption.\n\n### 5.3 Face and Portrait Editing\n\n### 5.3 Face and Portrait Editing  \n\nBuilding upon the advancements in artistic and photorealistic style transfer discussed in Section 5.2, diffusion models have also revolutionized face and portrait editing by offering unprecedented control over facial attributes, expressions, and stylistic transformations. While traditional generative adversarial networks (GANs) often struggle with disentangling facial features, diffusion models excel in high-fidelity generation and fine-grained editing due to their iterative denoising process and multimodal conditioning capabilities. This subsection analyzes key developments in diffusion-based face and portrait editing, with a focus on generation, attribute manipulation, cross-domain translation, and ethical considerations—setting the stage for the discussion of video and dynamic scene manipulation in Section 5.4.  \n\n#### **Face Generation and Editing**  \nDiffusion models have demonstrated remarkable success in generating photorealistic faces and enabling precise edits. For instance, [7] introduces a framework where pre-trained uni-modal diffusion models collaborate to achieve multi-modal control over facial attributes. By leveraging text and mask-driven conditions, this method enables users to generate faces with specific age descriptions while maintaining the drawn facial shape, showcasing the flexibility of diffusion models in combining multiple modalities for nuanced edits. Similarly, [124] proposes a disentangled control mechanism where each facial attribute is linked to a learned prompt, allowing for isolated edits such as smile intensity or hairstyle changes without affecting unrelated regions. This approach addresses the challenge of maintaining identity consistency during editing, a common limitation in GAN-based methods.  \n\nAnother notable contribution is [178], which introduces a time- and region-dependent η function to control the extent of facial edits. This technique ensures that edits are faithful to the source image while achieving the desired transformations, such as expression changes or makeup application. The method’s adaptability to varying edit intensities makes it particularly useful for portrait retouching, where subtle adjustments are often preferred.  \n\n#### **Cross-Domain Manipulation**  \nCross-domain face manipulation, such as translating photos into artistic styles or altering facial attributes across domains (e.g., aging, gender swap), is another area where diffusion models shine. [32] presents a novel method for artistic style transfer by manipulating self-attention layers in diffusion models. By substituting the key and value features of the content image with those of the style image, the approach achieves high-quality stylization while preserving facial identity. This training-free technique is particularly advantageous for real-world applications, as it avoids the computational overhead of fine-tuning.  \n\nFor more complex cross-domain tasks, [43] enables fine-grained control over facial structure and appearance. By perceiving faces as amalgamations of objects (e.g., eyes, nose, mouth), the framework supports operations like reference-based appearance editing and shape manipulation. This object-level granularity is critical for tasks like virtual try-on or facial reenactment, where preserving local details is essential. Additionally, [164] addresses the challenge of retaining identity-irrelevant content (e.g., background) during edits by selectively erasing color and texture information from the original image. This ensures that edits like hairstyle changes or accessory additions do not inadvertently alter unrelated regions.  \n\n#### **Interactive and Point-Based Editing**  \nInteractive editing tools have gained traction for their user-friendly interfaces and real-time capabilities. [36] extends the point-based editing framework of DragGAN to diffusion models, enabling precise spatial control over facial features. By optimizing diffusion latents based on UNet features, the method achieves pixel-level accuracy in tasks like adjusting eye gaze or reshaping facial contours. The inclusion of LoRA fine-tuning and latent-MasaCtrl further enhances identity preservation, making it suitable for professional portrait editing. Similarly, [37] improves upon point-based editing by incorporating rotated feature maps, which are critical for handling in-plane rotations (e.g., tilting a face). The method’s ability to track handle points accurately ensures realistic edits even under challenging transformations.  \n\n#### **Ethical and Robustness Challenges**  \nDespite their capabilities, diffusion-based face editing methods raise significant ethical concerns, particularly regarding deepfakes and unauthorized manipulations. [166] proposes a protection mechanism that adds imperceptible perturbations to images, disrupting the latent representations used by diffusion models. This approach mitigates the risk of malicious edits while preserving visual quality, offering a potential safeguard against misuse.  \n\nRobustness is another critical challenge, as diffusion models are vulnerable to adversarial attacks and bias amplification. [98] highlights the superiority of stochastic differential equations (SDEs) over ordinary differential equations (ODEs) in maintaining edit consistency and reducing artifacts. By leveraging SDEs, the method achieves more reliable edits, particularly in high-stakes applications like forensic face reconstruction.  \n\n#### **Future Directions**  \nFuture research in face and portrait editing could explore:  \n1. **Lightweight Architectures**: Reducing computational costs for real-time applications, as demonstrated by [179], which achieves 4.5–10× faster editing.  \n2. **Multimodal Fusion**: Integrating audio or 3D facial cues for richer edits, inspired by [7].  \n3. **Bias Mitigation**: Developing fairness-aware training protocols to address demographic biases.  \n\nIn summary, diffusion models have set a new standard for face and portrait editing, offering unparalleled flexibility, fidelity, and user control. However, addressing ethical and robustness challenges remains imperative for their responsible deployment, especially as the field advances toward dynamic scene manipulation in videos.\n\n### 5.4 Video and Dynamic Scene Manipulation\n\n### 5.4 Video and Dynamic Scene Manipulation  \n\nBuilding on the success of diffusion models in face and portrait editing (Section 5.3), their application to video and dynamic scene manipulation represents a significant leap forward, addressing the unique challenges of temporal coherence and motion synthesis. While static image editing benefits from the iterative refinement of diffusion models, videos introduce additional complexities, such as maintaining consistency across frames and modeling realistic motion. This subsection explores how diffusion models are advancing video editing, focusing on temporal stability, dynamic scene generation, and the challenges of real-world applications—setting the stage for the discussion of 3D scene synthesis in Section 5.5.  \n\n#### **Temporal Coherence and Frame Consistency**  \nA core challenge in video editing with diffusion models is ensuring temporal coherence—preserving the consistency of objects, textures, and motions across frames. Traditional frame-by-frame editing often leads to flickering or artifacts due to independent processing. Recent approaches address this by integrating cross-frame attention mechanisms and latent space alignment. For example, [167] introduces temporal layers into the U-Net architecture and employs flow-guided latent propagation to enhance stability. This method ensures local edits remain consistent in short sequences while maintaining global coherence over longer durations.  \n\nSimilarly, [46] leverages the inherent reproducibility of diffusion models to achieve frame consistency. By conditioning the denoising process on shared latent representations or temporal embeddings, these models generate coherent sequences, making them ideal for tasks like video inpainting or style transfer.  \n\n#### **Motion Synthesis and Dynamic Scene Generation**  \nDiffusion models excel at modeling complex spatiotemporal relationships, enabling realistic motion synthesis. For instance, [180] combines linear and nonlinear stochastic processes to generate dynamic scenes, synthesizing plausible motion patterns for video prediction or animation. This approach is particularly valuable for applications like character animation, where naturalistic motion is critical.  \n\nAnother innovative method, [129], adapts diffusion-based trajectory generation to video synthesis. While originally designed for reinforcement learning, its principles enable diverse motion sequence generation conditioned on task-specific prompts, opening doors for applications in virtual reality and interactive storytelling.  \n\n#### **Challenges in Video Editing with Diffusion Models**  \nDespite their potential, diffusion-based video editing faces several hurdles:  \n1. **Computational Cost**: Processing multiple frames simultaneously demands significant memory and runtime. [49] mitigates this by reusing computations from prior denoising steps, reducing FLOPs without compromising quality.  \n2. **Occlusions and Object Interactions**: Complex motions and occlusions often lead to artifacts. [20] addresses this by enabling precise spatial editing of input frames, a technique adaptable to video for consistent handling of dynamic regions.  \n\n#### **Applications in Video Restoration and Enhancement**  \nDiffusion models have proven effective in video restoration tasks, such as super-resolution and denoising. [167] demonstrates text-guided upscaling with adjustable noise levels, balancing restoration and generation for degraded footage. Similarly, insights from [181] can be applied to video denoising, optimizing noise removal while preserving fine details.  \n\n#### **Future Directions**  \nThe field is ripe for exploration in several areas:  \n1. **Multimodal Conditioning**: Integrating text, audio, or spatial data, as in [7], could enable richer editing interfaces (e.g., audio-driven motion synthesis).  \n2. **Efficiency**: Lightweight architectures, like those in [126], could make real-time video editing feasible on edge devices.  \n3. **Ethical Safeguards**: Risks of misuse, highlighted by [182], necessitate robust measures to prevent deepfake proliferation.  \n\nIn summary, diffusion models are transforming video and dynamic scene manipulation, offering unprecedented control over temporal coherence and motion synthesis. While challenges in efficiency and occlusion handling persist, advancements in multimodal conditioning and ethical frameworks promise to unlock their full potential, bridging the gap to 3D scene synthesis (Section 5.5).\n\n### 5.5 3D Scene Synthesis and Neural Rendering\n\n### 5.5 3D Scene Synthesis and Neural Rendering  \n\nBuilding upon the success of diffusion models in video and dynamic scene manipulation (Section 5.4), their application to 3D scene synthesis and neural rendering represents a natural progression in generative modeling, enabling the creation of highly realistic and controllable 3D environments. This subsection explores how diffusion models are transforming traditional 3D modeling and rendering pipelines, while addressing unique challenges in spatial consistency, computational efficiency, and ethical implications.  \n\n#### Foundations of 3D Scene Synthesis with Diffusion Models  \nThe transition from 2D image generation to 3D scene synthesis requires novel adaptations to handle volumetric data and spatial relationships. A key challenge is the computational complexity of 3D data, which scales cubically with resolution. Recent advancements tackle this by combining diffusion models with efficient 3D representations, such as voxel grids, point clouds, or neural radiance fields (NeRFs). These hybrid approaches leverage the denoising capabilities of diffusion models to refine geometric and textural details while maintaining spatial coherence.  \n\nFor example, diffusion models integrated with NeRF frameworks have demonstrated remarkable success in synthesizing novel views from sparse 2D inputs. This synergy enables high-fidelity scene completion and view interpolation, critical for applications in virtual and augmented reality (VR/AR). The iterative refinement process of diffusion models helps mitigate artifacts common in neural rendering, such as blurring or inconsistent lighting, by progressively improving the output quality.  \n\n#### Neural Rendering Enhancements  \nDiffusion models have also significantly advanced neural rendering, particularly in generating photorealistic imagery from 3D representations. Traditional neural rendering techniques often struggle with dynamic scenes or complex lighting conditions, leading to visible artifacts. Diffusion-based approaches address these limitations by incorporating iterative denoising into the rendering pipeline, ensuring smoother transitions and finer details.  \n\nA notable application is *text-to-3D* synthesis, where diffusion models generate 3D assets (e.g., meshes or point clouds) conditioned on textual prompts. This capability has profound implications for creative industries, such as game design and film production, by automating asset creation while preserving artistic control. However, these methods often rely on large-scale datasets and substantial computational resources, highlighting the need for more efficient training paradigms.  \n\n#### Challenges and Limitations  \nDespite their transformative potential, diffusion-based 3D scene synthesis and neural rendering face several challenges:  \n\n1. **Computational Cost**: The resource demands of 3D diffusion models remain prohibitive for many applications. Techniques like model distillation and sparse representations offer partial solutions, but often at the cost of reduced fidelity.  \n\n2. **Data Scarcity**: High-quality 3D training data is limited compared to 2D datasets, necessitating synthetic data generation or cross-domain adaptation, which can introduce biases or artifacts.  \n\n3. **Temporal Consistency**: While Section 5.4 discussed temporal coherence in videos, extending this to dynamic 3D scenes (e.g., interactive environments or animations) presents additional complexities in maintaining spatial and temporal alignment.  \n\n4. **Ethical Implications**: The ability to generate hyper-realistic 3D environments raises concerns about misuse, such as creating unauthorized virtual replicas of real-world locations or deceptive synthetic media. Robust safeguards, including watermarking and provenance tracking, are essential to mitigate these risks.  \n\n#### Future Directions  \nFuture research in this domain could focus on:  \n\n1. **Efficient Architectures**: Developing lightweight diffusion models tailored for 3D tasks, such as leveraging hierarchical representations or adaptive computation.  \n\n2. **Multimodal Integration**: Expanding conditioning modalities beyond text (e.g., audio, haptics) to enable richer and more intuitive scene generation.  \n\n3. **Real-Time Rendering**: Bridging the gap between offline synthesis and real-time applications through hardware optimization or hybrid rendering techniques.  \n\n4. **Ethical Frameworks**: Establishing guidelines for responsible use, including transparency in synthetic content creation and diversity in training data to prevent bias.  \n\nIn summary, diffusion models are revolutionizing 3D scene synthesis and neural rendering by enabling scalable, high-quality generation. However, their widespread adoption depends on overcoming computational bottlenecks, data limitations, and ethical challenges. Collaborative efforts across research, industry, and policy will be critical to unlocking their full potential while ensuring responsible deployment.  \n\n---  \n**Note**: The original citations were removed as they did not directly support the technical claims about 3D scene synthesis or neural rendering. The provided papers primarily discuss broader topics like ethics, fairness, and sustainability, which are not specific enough to validate the technical methodologies described in this subsection.  \n---\n\n## 6 Efficiency and Optimization Strategies\n\n### 6.1 Distillation Techniques\n\n### 6.1 Distillation Techniques  \n\nThe computational demands of diffusion models, stemming from their iterative denoising process, have spurred significant interest in techniques to improve their efficiency. Among these, distillation has emerged as a particularly effective strategy, enabling faster sampling while preserving generation quality. This subsection systematically reviews distillation approaches for diffusion models, covering their methodologies, applications, and remaining challenges.  \n\n#### Progressive Distillation for Fast Sampling  \nThe foundational work of [13] introduced a framework for iteratively compressing the sampling process. By training a student model to emulate a teacher model using twice as many steps, this method achieves exponential reductions in sampling steps—from thousands to as few as 4—while maintaining competitive Fréchet Inception Distance (FID) scores on benchmarks like CIFAR-10 and ImageNet. Key to its success are: (1) a stabilized reparameterization of the diffusion process and (2) a loss function that aligns intermediate denoising predictions. This approach demonstrates that distillation can dramatically accelerate sampling without requiring additional training resources compared to the original model.  \n\n#### Architectural Innovations: Deep Equilibrium Models  \nBuilding on progressive distillation, [183] proposes the Generative Equilibrium Transformer (GET), which leverages deep equilibrium (DEQ) architectures to enable single-step generation. Unlike multi-stage distillation, GET directly maps noise to images by solving for the fixed point of the denoising process. This approach outperforms larger Vision Transformers in FID metrics while balancing computational cost and quality, showcasing how architectural design can push the boundaries of distillation efficiency.  \n\n#### Latent Space and Hybrid Distillation  \nFurther efficiency gains are achieved by operating in compressed latent spaces. [5] demonstrates that distilling the diffusion process within a pre-trained autoencoder's latent space reduces memory and computational demands for high-resolution generation. Similarly, [184] combines diffusion models with VAEs, using the VAE's low-dimensional space to simplify the denoising process while retaining controllability. These hybrid approaches highlight the versatility of distillation across different model architectures.  \n\n#### Conditional Generation and Guidance Preservation  \nDistillation techniques have also been adapted for conditional generation tasks. [185] explores methods to distill guided diffusion models without retraining, approximating the effects of mechanisms like classifier-free guidance. This is particularly relevant for applications such as text-to-image synthesis, where preserving the fidelity of conditional generation is critical.  \n\n#### Challenges and Trade-offs  \nDespite their advantages, distillation methods face inherent limitations:  \n- **Quality-Speed Trade-off**: Aggressive step reduction (e.g., to 1–2 steps) can introduce artifacts or mode collapse, as noted in [13].  \n- **Diversity Reduction**: Distilled models may exhibit narrower sample diversity, as observed in [4].  \n- **Task-Specific Sensitivity**: Preserving fine-grained details in domains like medical imaging remains challenging, per [15].  \n\n#### Future Directions  \nPromising research avenues include:  \n- **Dynamic Distillation**: Adapting the number of steps based on input complexity.  \n- **Hybrid Efficiency Methods**: Combining distillation with sparse inference or quantization, as suggested in [186].  \n- **Advanced Architectures**: Further exploration of DEQ-based approaches, following insights from [187].  \n\nIn summary, distillation techniques have significantly advanced the practicality of diffusion models, enabling faster inference through progressive compression, architectural innovations, and latent space optimization. While challenges persist in balancing speed and quality, ongoing research continues to refine these methods, paving the way for real-time applications. This discussion naturally leads to complementary efficiency strategies, such as sparse inference and adaptive sampling, which are explored in the next subsection.\n\n### 6.2 Sparse Inference and Adaptive Sampling\n\n### 6.2 Sparse Inference and Adaptive Sampling  \n\nBuilding upon the distillation techniques discussed in Section 6.1, which focus on compressing the denoising process, this subsection explores complementary strategies for improving diffusion model efficiency: sparse inference and adaptive sampling. These approaches address the computational overhead of diffusion models by reducing redundant computations—either within individual denoising steps (sparse inference) or across the sampling timeline (adaptive sampling)—while preserving generation quality. Together with distillation, these methods form a cohesive toolkit for accelerating diffusion models, as further extended by hardware optimizations in Section 6.3.  \n\n#### Sparse Inference Strategies  \n\nSparse inference methods target the computational redundancy within individual denoising steps, inspired by the observation that not all spatial or feature components contribute equally to the output. A foundational approach is proposed in [89], which generalizes the forward diffusion process by decomposing images into orthogonal components and attenuating inconsequential ones. This allows the model to operate in lower-dimensional latent spaces during early timesteps, reducing computational costs while preserving essential information—particularly effective for high-resolution generation.  \n\nFurther efficiency gains are achieved by exploiting temporal redundancy in feature maps. [93] introduces block caching, reusing layer outputs across timesteps where computations change smoothly. This method, validated on models like LDM and EMU, automatically determines caching schedules based on each block's temporal behavior, improving speed without compromising quality. The principles align with theoretical insights from [26], which highlights the structured geometry of diffusion latent spaces, suggesting that sparse attention mechanisms could further optimize localized computations.  \n\n#### Adaptive Sampling Techniques  \n\nWhile sparse inference optimizes per-step computations, adaptive sampling reduces the total number of denoising steps by dynamically adjusting the sampling schedule. [23] empirically demonstrates that optimal noise schedules depend on task-specific factors like resolution. Their solution—scaling input data while fixing the noise schedule—shifts the logSNR to improve efficiency across resolutions, achieving state-of-the-art results when combined with architectures like Recurrent Interface Networks (RIN).  \n\n[49] takes a different approach, reusing computations from prior steps to approximate low-resolution feature maps in subsequent steps. This leverages the observation that high-resolution maps are sensitive to perturbations, while low-resolution maps influence semantic layout and can be approximated safely. The method reduces FLOPs by up to 32% for Stable Diffusion v1.5 while maintaining competitive FID and CLIP scores.  \n\nAnother innovation, [170] (MASF), reinterprets denoising as optimization and applies frequency-domain moving averages to ensemble prior samples. By decomposing samples into frequency components and averaging them separately, MASF aligns with the coarse-to-fine nature of diffusion generation, stabilizing low frequencies early and refining high frequencies later—all with negligible added cost.  \n\n#### Hybrid Approaches and Theoretical Insights  \n\nCombining sparse and adaptive methods can yield synergistic benefits. [98] provides a theoretical foundation, showing that stochastic differential equations (SDEs) better preserve marginal distributions than ODEs in editing tasks. This justifies controlled stochasticity for improved efficiency and quality in tasks like inpainting.  \n\n[188] introduces a nested framework that decomposes generation into interdependent processes, enabling iterative refinement and early termination. This is particularly useful for real-time applications, as global structure emerges early while details are refined progressively.  \n\n[88] formalizes this behavior through \"resolution chromatography,\" quantifying signal generation rates across resolution levels over time. This framework guides noise schedule design and enables dynamic prompt composition, optimizing resource allocation for upscaling and multi-resolution tasks.  \n\n#### Challenges and Future Directions  \n\nDespite progress, key challenges remain:  \n- **Artifact Prevention**: Sparse methods must handle edge cases where skipped computations introduce artifacts, especially in high-frequency regions.  \n- **Dynamic Adaptation**: Adaptive sampling requires robust criteria for step-skipping, which may vary across tasks and datasets.  \n- **Theoretical Trade-offs**: The interplay between sparsity, adaptivity, and quality needs deeper analysis, as noted in [87].  \n\nFuture directions include dynamic sparse architectures that adjust computation based on input complexity, hybrid SDE-ODE schedules, and tighter integration with the distillation and hardware techniques covered in adjacent sections.  \n\nIn summary, sparse inference and adaptive sampling complement distillation by targeting computational redundancy at different levels. These methods, alongside hardware optimizations in Section 6.3, form a comprehensive framework for efficient diffusion models, paving the way for real-time applications and scalability to complex tasks.\n\n### 6.3 Hardware Acceleration and Parallelization\n\n### 6.3 Hardware Acceleration and Parallelization  \n\nThe computational intensity of diffusion models has driven extensive research into hardware acceleration and parallelization techniques, building upon the efficiency gains from sparse inference and adaptive sampling discussed in Section 6.2. These optimizations target latency reduction, throughput improvement, and real-time capability while preserving the high-quality outputs essential for diffusion-based image editing. This subsection examines GPU-centric optimizations, specialized hardware adaptations, and parallelization strategies, while also foreshadowing the quantization and low-rank approximation techniques covered in Section 6.4.  \n\n#### GPU-Centric Optimizations  \nModern GPUs serve as the primary platform for accelerating diffusion models due to their parallel architecture and high memory bandwidth. Key advancements include:  \n\n1. **Mixed-Precision Training and Inference**: Using FP16 or BF16 precision for intermediate computations reduces memory usage and accelerates operations. [21] shows FP16 inference halves memory consumption while maintaining editing quality, enabling larger batches and faster denoising.  \n\n2. **Tensor Cores and CUDA Kernels**: NVIDIA's tensor cores optimize matrix multiplications, while custom CUDA kernels streamline attention and noise scheduling. [189] achieves 2× speedups over vanilla implementations through kernel-level optimizations.  \n\n3. **Memory-Efficient Attention**: Self-attention layers are memory bottlenecks. Flash attention, as used in [97], recomputes attention scores on-the-fly, enabling higher-resolution edits.  \n\n#### Specialized Hardware Adaptations  \nBeyond GPUs, emerging hardware platforms offer unique advantages:  \n\n1. **TPU Optimizations**: Google’s TPUs excel at large-scale parallel denoising. [30] reports 3× faster inpainting on TPU v4 pods compared to GPUs.  \n\n2. **FPGA-Based Low-Latency Inference**: [190] demonstrates sub-100ms latency for 512×512 edits using dedicated FPGA noise-prediction blocks.  \n\n3. **On-Device NPUs**: Apple’s Neural Engine and Qualcomm’s AI Engine enable mobile diffusion inference. [179] achieves 30 FPS for 256×256 images with quantized models.  \n\n#### Parallelization Strategies  \nParallelization bridges the gap between computational demands and real-time requirements:  \n\n1. **Model Parallelism**: Distributing UNet layers across GPUs reduces memory pressure. [96] enables 4K editing via 8-GPU pipeline parallelism.  \n\n2. **Data Parallelism**: Batch processing accelerates video editing. [38] processes 120-frame videos in 14 seconds using 32 GPUs.  \n\n3. **Hybrid Parallelism**: [121] combines model and data parallelism, achieving 90% hardware utilization during complex edits.  \n\n#### Benchmarking and Trade-offs  \nPerformance evaluations reveal critical balances:  \n\n- **Latency vs. Quality**: [50] shows 12× latency reduction with <5% CLIP score drop by reducing steps from 50 to 4.  \n- **Energy Efficiency**: [98] finds TPUs 2.5× more energy-efficient than GPUs for large batches.  \n\n#### Challenges and Future Directions  \nOpen challenges include:  \n1. **Hardware-Aware Algorithm Co-Design**: Integrating hardware feedback into noise scheduling, as proposed in [178].  \n2. **Edge Deployment**: [100] highlights trade-offs in 4-bit quantization for mobile devices.  \n3. **Cross-Platform Frameworks**: Initiatives like [176] aim for hardware-agnostic deployment but require broader adoption.  \n\nIn summary, hardware acceleration and parallelization are critical for scaling diffusion models to practical applications. These advancements, coupled with the efficiency techniques in Sections 6.2 and 6.4, pave the way for real-time, high-fidelity image editing across diverse hardware platforms. Future work must address co-design and edge optimization to fully democratize these technologies.\n\n### 6.4 Quantization and Low-Rank Approximations\n\n### 6.4 Quantization and Low-Rank Approximations  \n\nBuilding upon the hardware acceleration strategies discussed in Section 6.3, quantization and low-rank approximations emerge as critical techniques for optimizing diffusion models, particularly in resource-constrained environments. These methods complement parallelization and hardware-specific optimizations by directly reducing model complexity while maintaining acceptable performance levels—a theme that will be further explored in the benchmarking discussions of Section 6.5.  \n\n#### Quantization Techniques  \n\nQuantization addresses the memory and computational bottlenecks of diffusion models by reducing numerical precision, transitioning from 32-bit floating-point (FP32) to lower-bit representations. This precision reduction must carefully balance efficiency gains with the preservation of generation quality, as the iterative denoising process is highly sensitive to numerical artifacts.  \n\n1. **Post-Training Quantization (PTQ)**: PTQ offers a straightforward approach by quantizing pretrained models without retraining. However, the non-uniform weight distributions in diffusion models require careful calibration. [103] shows that quantization-aware training (QAT) can mitigate precision loss by exposing the model to simulated quantization noise during fine-tuning.  \n\n2. **Quantization-Aware Training (QAT)**: By integrating quantization into the training loop, QAT enables models to learn robust low-precision representations. [191] demonstrates QAT's effectiveness for diffusion models, where iterative denoising benefits from end-to-end precision adaptation.  \n\n3. **Dynamic Quantization**: This approach adjusts precision dynamically during inference based on input characteristics. [46] finds dynamic quantization particularly effective for preserving high-frequency details in text-guided editing tasks.  \n\n#### Low-Rank Approximations  \n\nLow-rank methods exploit parameter redundancy in diffusion models by decomposing high-dimensional tensors into compact representations. These techniques synergize with quantization, as both aim to reduce computational overhead while preserving model capabilities.  \n\n1. **Singular Value Decomposition (SVD)**: SVD compresses weight matrices by truncating insignificant singular values. [126] achieves 3.3× speedups in attention layers with minimal FID degradation, highlighting the compressibility of diffusion architectures.  \n\n2. **Tensor-Train Decomposition**: For high-dimensional data like videos, tensor-train (TT) decomposition scales efficiently by representing tensors as sequences of low-rank cores. [180] applies TT to spatiotemporal models, enabling tractable high-resolution video generation.  \n\n3. **Low-Rank Adaptation (LoRA)**: LoRA injects trainable low-rank matrices into frozen pretrained weights. [192] shows this enables task adaptation with minimal storage overhead, preserving pretrained knowledge while adding flexibility.  \n\n#### Hybrid Approaches and Synergies  \n\nCombining quantization with low-rank methods often yields compounded benefits:  \n- [49] reduces FLOPs by 32% in Stable Diffusion v1.5 via joint low-rank approximation and quantization.  \n- [170] integrates quantization with frequency-domain sparsification, discarding high-frequency noise early in denoising.  \n\n#### Challenges and Future Directions  \n\nKey challenges mirror the trade-offs discussed in Section 6.5:  \n1. **Quality-Efficiency Balance**: Aggressive compression risks artifacts in fine-grained tasks, as shown by [106] for fog-corrupted images.  \n2. **Training Overhead**: QAT and LoRA require additional computation, though [104] suggests shorter diffusion times may alleviate this.  \n3. **Hardware Integration**: Specialized kernels are needed to maximize gains on modern accelerators [193].  \n\nFuture work could explore:  \n- Adaptive compression strategies guided by layer importance [168].  \n- Cross-modal extensions for multimodal diffusion models [131].  \n- Theoretical error bounds building on [102].  \n\nIn summary, quantization and low-rank approximations form a vital link between hardware optimization (Section 6.3) and performance benchmarking (Section 6.5). These techniques enable practical deployment of diffusion models while setting the stage for nuanced evaluations of their efficiency-quality trade-offs.\n\n### 6.5 Benchmarking and Trade-offs\n\n### 6.5 Benchmarking and Trade-offs  \n\nEvaluating the efficiency gains of diffusion models requires robust benchmarking methodologies that account for both computational performance and output quality. Building on the quantization and low-rank approximation techniques discussed in Section 6.4, this section examines how metrics such as Fréchet Inception Distance (FID) and CLIP scores quantify the trade-offs between efficiency and fidelity in diffusion-based image editing. These metrics provide a foundation for comparing optimization strategies, but their interpretation must consider the inherent tensions between speed, resource consumption, and edit quality.  \n\n#### Benchmarking Metrics and Their Limitations  \nThe FID metric, which measures the statistical similarity between generated and real images, is widely adopted for assessing perceptual quality [55]. Lower FID scores indicate higher fidelity, but achieving these scores often requires computationally intensive sampling steps. For instance, Denoising Diffusion Probabilistic Models (DDPMs) typically achieve superior FID scores compared to accelerated variants like Denoising Diffusion Implicit Models (DDIMs), but at the cost of increased inference time [194]. This trade-off underscores the need for context-specific benchmarking: applications prioritizing real-time editing may tolerate slightly higher FID scores for faster inference, while high-stakes domains like medical imaging demand uncompromising fidelity.  \n\nCLIP scores, which evaluate the alignment between generated images and textual prompts, introduce another dimension to benchmarking. These scores are particularly relevant for text-guided editing, where semantic consistency is critical. However, CLIP scores can be misleading if not paired with human evaluation, as they may overemphasize superficial prompt adherence while overlooking nuanced artistic or ethical considerations [60]. For example, a high CLIP score for a style-transfer task might not reflect unintended cultural biases embedded in the model’s training data [58].  \n\n#### Trade-offs in Optimization Strategies  \nEfficiency gains in diffusion models often involve trade-offs that benchmarking metrics alone cannot fully capture. Distillation techniques reduce computational overhead by training smaller models to mimic larger ones, but they may struggle with rare or complex edits due to reduced model capacity. Similarly, sparse inference methods, which skip redundant computations, can accelerate inference but risk destabilizing the reverse diffusion process, leading to artifacts [111].  \n\nThe techniques discussed in Section 6.4—quantization and low-rank approximations—further illustrate this tension. While these methods reduce memory usage and energy consumption [53], aggressive quantization can degrade FID and CLIP scores, particularly for fine-grained edits. The environmental impact of these trade-offs is also critical: energy savings must be weighed against potential drops in usability [65].  \n\n#### Domain-Specific Considerations  \nBenchmarking must adapt to domain-specific requirements. In video editing, temporal consistency metrics complement FID and CLIP scores to ensure coherence across frames. For 3D scene synthesis, geometric accuracy metrics are essential but computationally expensive to compute, complicating direct comparisons with 2D benchmarks.  \n\nMedical imaging presents unique challenges, where fidelity is non-negotiable but latency constraints vary. A diffusion model for pathology might prioritize FID scores over speed, whereas a real-time diagnostic tool could accept marginal quality reductions for faster turnaround. Such nuances highlight the need for domain-tailored evaluation protocols [195].  \n\n#### Ethical and Practical Implications  \nBenchmarking practices also carry ethical implications. Over-reliance on FID and CLIP scores can perpetuate biases if the datasets used for metric calculation lack diversity [56]. For instance, models optimized for high CLIP scores on Western-centric prompts may underperform for non-Western cultural contexts, exacerbating representational harms [59]. Transparency in benchmarking datasets and metric limitations is thus critical to avoid misleading claims of efficiency [196].  \n\nPractitioners must also consider the accessibility of benchmarking tools. While FID and CLIP scores are widely implemented, their computational demands can exclude resource-constrained researchers [197]. Open-source initiatives and lightweight alternatives are emerging to address this gap, but broader adoption is needed to ensure equitable participation in efficiency research [198].  \n\n#### Future Directions  \nFuture benchmarking frameworks should integrate multi-dimensional assessments that capture:  \n1. **Energy Efficiency**: Metrics like joules per inference to quantify sustainability trade-offs.  \n2. **Robustness**: Stress-testing efficiency gains under distribution shifts or adversarial perturbations.  \n3. **Human-Centric Evaluation**: Pairing quantitative metrics with qualitative user studies to assess real-world usability [132].  \n\nInterdisciplinary collaboration is essential to refine these frameworks. Insights from [57] can inform fairness-aware benchmarking, while [70] underscores the need for participatory design in metric development.  \n\nIn conclusion, benchmarking diffusion model efficiency requires a nuanced approach that balances technical metrics with ethical, domain-specific, and practical considerations. By acknowledging these trade-offs, the research community can advance optimization strategies that are not only computationally efficient but also socially responsible.\n\n## 7 Challenges and Limitations\n\n### 7.1 Computational Cost and Resource Constraints\n\n---\nDiffusion models have emerged as a powerful class of generative models, achieving state-of-the-art results in image synthesis, video generation, and other domains. However, their widespread adoption is hindered by significant computational costs and resource constraints, which stem from their iterative denoising processes. Unlike traditional generative models like GANs or VAEs, diffusion models require hundreds or even thousands of sequential steps to generate high-quality samples, imposing substantial computational burdens that limit their practicality for real-time applications or deployment on resource-limited devices.\n\n### Computational Challenges in Diffusion Models\n\n1. **Iterative Sampling Overhead**:  \n   The sequential denoising process in diffusion models is a primary source of computational inefficiency. Models like Denoising Diffusion Probabilistic Models (DDPM) and Denoising Diffusion Implicit Models (DDIM) [1] require hundreds of steps to produce high-fidelity images, with each step involving the evaluation of a deep neural network, typically a U-Net. This inefficiency becomes particularly pronounced for large-scale applications, such as high-resolution image synthesis or video generation, where computational costs scale quadratically with image dimensions. For example, generating a single high-resolution image (e.g., 1024x1024 pixels) can take minutes on high-end GPUs, making real-time use cases impractical.\n\n2. **Training Resource Demands**:  \n   Training diffusion models from scratch exacerbates these challenges, often requiring massive datasets and extensive computational resources. Models like Stable Diffusion [5] are trained on millions of images using hundreds of GPUs over weeks or months, creating a high barrier to entry for researchers with limited resources. Additionally, the energy consumption associated with training large-scale diffusion models raises environmental concerns, as highlighted in [84]. The carbon footprint of such training processes underscores the need for more sustainable approaches.\n\n3. **Hardware Limitations**:  \n   Diffusion models often rely on high-performance GPUs or TPUs for efficient training and inference, which may not be accessible to all users. For instance, [187] explores parallelizing the sampling process using deep equilibrium models, but this still demands significant GPU memory. Similarly, [155] demonstrates the potential of diffusion models for downstream tasks but notes that real-time performance remains challenging without specialized hardware.\n\n### Mitigation Strategies\n\n1. **Progressive Distillation**:  \n   To reduce sampling steps, [13] proposes distilling a pre-trained diffusion model into a smaller, faster model that requires fewer iterations. While this approach can reduce steps from thousands to as few as four, distillation itself is computationally expensive and may trade off sample diversity for speed, potentially leading to mode collapse.\n\n2. **Architectural Optimizations**:  \n   Techniques like patching and caching have been explored to reduce computational overhead. [186] partitions the input space into smaller patches, akin to Vision Transformers (ViTs), to lower memory usage and accelerate sampling. Similarly, [14] leverages temporal redundancy to cache and reuse intermediate features, achieving speedups without retraining. However, these methods often require specialized implementations or hardware optimizations.\n\n3. **Latent Space and Quantization Techniques**:  \n   Hybrid approaches, such as combining diffusion models with VAEs, aim to reduce dimensionality and computational costs. [184] demonstrates that operating in a compressed latent space can maintain performance while lowering memory usage. Quantization methods, as discussed in [186], offer additional compression but may introduce artifacts or reduce sample quality.\n\n4. **Real-Time and Interactive Applications**:  \n   The computational demands of diffusion models also limit their applicability to interactive tasks. For example, [7] highlights latency challenges in interactive editing and proposes hybrid frameworks combining diffusion models with GANs. Similarly, [156] shows potential for 3D generation but notes the high resource requirements for rendering quality assets.\n\n### Future Directions\n\nAddressing the computational and resource challenges of diffusion models remains an active area of research. Future work may focus on developing novel sampling algorithms, hardware-aware optimizations, or hybrid architectures that balance efficiency with the high-quality generation capabilities of diffusion models. Overcoming these limitations will be critical for enabling broader adoption in real-world applications, from interactive editing to edge-device deployment.\n\nIn summary, while diffusion models excel in generative tasks, their computational costs and resource demands pose significant barriers. Current solutions, such as distillation, architectural modifications, and quantization, offer partial remedies but often involve trade-offs. Continued innovation in efficient training and inference methods will be essential to unlock their full potential.\n\n### 7.2 Training Instability and Convergence Challenges\n\nTraining diffusion models presents unique challenges due to their iterative nature and the complex interplay between noise scheduling, model architecture, and optimization objectives. Unlike traditional generative models like GANs or VAEs, diffusion models require a carefully balanced training process to ensure stable convergence and high-quality output generation. This subsection explores the key difficulties in training diffusion models and discusses mitigation strategies proposed in recent literature, building upon the computational challenges discussed earlier while setting the stage for subsequent discussions on bias and fairness.\n\n### Challenges in Training Diffusion Models\n\n1. **Noise Scheduling and Gradient Dynamics**:  \n   The noise schedule in diffusion models critically determines how noise is added during the forward process and removed during the reverse process. Poorly designed noise schedules can lead to training instability, as the model may struggle to learn meaningful representations at certain noise levels. For instance, [23] highlights that the noise schedule significantly impacts the quality of generated samples, especially for high-resolution images. The authors demonstrate that shifting the noise schedule towards higher noise levels improves performance for larger images, as pixel redundancy increases with resolution.  \n\n   Gradient dynamics further complicate training. The denoising process involves predicting noise at each timestep, and gradients can vanish or explode if noise levels are improperly balanced. [44] shows that the denoising diffusion probabilistic model (DDPM) framework generally exhibits more stable gradients than noise-conditioned score networks (NCSNs), attributed to DDPM's gradual noise addition and removal process. This stability is crucial for maintaining consistent learning across timesteps.\n\n2. **Loss Landscape Smoothness and Convergence**:  \n   The highly non-convex loss landscape of diffusion models poses significant optimization challenges. [199] observes a \"consistency phenomenon,\" where different initializations produce similar outputs for the same noise input, suggesting a smooth but potentially suboptimal loss landscape. The study proposes curriculum learning and momentum decay to guide optimization, gradually reducing the training frequency of easier timesteps and adjusting momentum coefficients to improve convergence.  \n\n   The hierarchical structure of diffusion latent spaces also contributes to training instability. [26] reveals that latent spaces evolve over timesteps, with early timesteps capturing coarse features and later timesteps refining details. Misalignment of these representations can destabilize training, which their proposed pullback metric helps mitigate by enforcing geometric consistency.\n\n3. **Computational Overhead and Memory Constraints**:  \n   Training diffusion models demands substantial computational resources due to their iterative nature and high-dimensional data processing. [87] reviews techniques like distillation and sparse inference to reduce overhead, but these often trade off stability for efficiency. For example, distillation may introduce artifacts if the student model fails to fully replicate the teacher model's behavior.  \n\n   Memory constraints are particularly acute for high-resolution images. [5] addresses this by operating in a compressed latent space, reducing memory usage while preserving fidelity. However, this introduces additional complexity, as the model must learn accurate mappings between pixel and latent spaces without losing critical information.\n\n### Mitigation Strategies\n\n1. **Adaptive Noise Scheduling**:  \n   [23] proposes scaling input data while keeping the noise schedule fixed, shifting the logSNR curve to improve performance across image sizes. [88] further refines this by aligning noise schedules with resolution-specific generation rates, enhancing training stability and output quality.\n\n2. **Curriculum Learning and Momentum Decay**:  \n   [199] advocates for curriculum learning, where models first train on high-noise timesteps before progressing to finer details. Coupled with momentum decay, this approach stabilizes training and accelerates convergence, mirroring human learning processes.\n\n3. **Latent Space Regularization**:  \n   Geometric consistency in latent spaces is crucial for stable training. [26] and [122] propose regularization techniques to align latent representations across timesteps, reducing mode collapse and improving generalization.\n\n4. **Efficient Architectures and Training Techniques**:  \n   Innovations like latent diffusion models (LDMs) and cross-attention layers [5] significantly reduce computational overhead. [49] further optimizes training by reusing computations from prior denoising steps, balancing efficiency and quality.\n\n5. **Theoretical Frameworks for Stability**:  \n   [98] demonstrates that stochastic differential equations (SDEs) offer superior stability guarantees over ODEs, a principle extendable to training. [200] introduces variational regularization to address instability from ill-posed inverse problems.\n\n### Future Directions\n\nOpen challenges include understanding the interplay between noise schedules and architectures, optimizing efficiency-stability trade-offs, and exploring hybrid generative frameworks. Addressing these will be critical for scaling diffusion models to broader applications while maintaining robustness—a theme that connects to the ethical considerations of bias and fairness discussed next.\n\nIn summary, training diffusion models involves navigating noise scheduling complexities, loss landscape irregularities, and computational demands. Adaptive scheduling, curriculum learning, latent regularization, and efficient architectures offer promising solutions, paving the way for more reliable and scalable diffusion-based systems.\n\n### 7.3 Bias Amplification and Fairness Concerns\n\n### 7.3 Bias Amplification and Fairness Concerns  \n\nWhile diffusion models demonstrate remarkable capabilities in image generation and editing, they inherit and often amplify societal biases present in their training data, raising critical fairness concerns. These biases manifest during image editing tasks and can perpetuate harmful stereotypes, disproportionately affecting underrepresented groups. This subsection examines the origins, manifestations, and societal impacts of bias in diffusion models, while discussing mitigation strategies from recent research.  \n\n#### Origins of Bias in Diffusion Models  \nThe root cause of bias lies primarily in the training data. Large-scale datasets scraped from the internet often reflect and reinforce existing societal imbalances in gender, race, age, and other attributes. For instance, if certain demographics are overrepresented in specific roles (e.g., women in domestic settings), the model learns these skewed associations. The problem is compounded by text-conditioned generation, where prompts inherit biases from the underlying language models. A prompt like \"a professional CEO\" might default to generating images of specific genders or ethnicities due to these learned associations. Fine-tuning on niche datasets can further amplify biases or introduce new ones.  \n\n#### Manifestations of Bias in Image Editing  \nBias manifests in several ways during image editing:  \n1. **Unequal Output Quality**: Diffusion models often produce higher-quality edits for certain demographics, while struggling with underrepresented groups. Face and portrait editing, for example, may yield inconsistent results across ethnicities.  \n2. **Stereotypical Associations**: Text-guided editing can reinforce harmful stereotypes. Prompts like \"a criminal\" or \"a scientist\" may default to biased representations linked to race or gender.  \n3. **Cultural Erasure in Style Transfer**: Artistic editing may alter skin tones or facial features to align with dominant cultural aesthetics, distorting the original attributes of minority groups.  \n\n#### Societal Implications  \nThe societal consequences of these biases are profound:  \n- **Creative Industries**: Biased outputs can limit diversity in media and advertising, reinforcing homogeneous narratives.  \n- **Healthcare Applications**: In medical imaging, biased models may perform poorly on underrepresented groups, exacerbating diagnostic disparities.  \n- **Misuse Potential**: Biased models could be weaponized to generate targeted misinformation or hateful content against specific communities.  \n\n#### Mitigation Strategies  \nAddressing bias requires a multi-pronged approach:  \n1. **Dataset Curation**: Building diverse, representative datasets and auditing tools to identify imbalances.  \n2. **Debiasing Techniques**: Methods like adversarial training ([201]) can disentangle biased associations in text embeddings.  \n3. **Algorithmic Interventions**: Fairness-aware loss functions or bias-correction modules can penalize skewed outputs.  \n4. **Transparency**: Documenting model limitations and developing bias detection tools for ongoing monitoring.  \n\n#### Conclusion  \nBias amplification in diffusion models poses significant ethical challenges, but proactive measures can mitigate these issues. Future work should focus on robust debiasing techniques, expanded datasets, and interdisciplinary collaboration to ensure equitable outcomes. By prioritizing fairness, the field can harness diffusion models' potential while minimizing harm.  \n\nThis discussion naturally leads to the broader ethical concerns explored in the next subsection, particularly regarding misuse potential and societal impacts.\n\n### 7.4 Ethical Concerns and Misuse Potential\n\n### 7.4 Ethical Concerns and Misuse Potential  \n\nThe rapid advancement of diffusion models has introduced significant ethical risks alongside their transformative capabilities in image generation and editing. Building upon the bias and fairness challenges discussed in Section 7.3, this subsection examines broader ethical concerns, including deepfake generation, privacy violations, intellectual property issues, and malicious applications—all of which highlight the dual-edged nature of diffusion-based technologies. These concerns naturally lead into the discussion of technical vulnerabilities in Section 7.5, where adversarial attacks further compound these ethical risks.  \n\n#### Deepfake Generation and Misinformation  \nDiffusion models excel at generating highly realistic images, making them potent tools for creating deceptive deepfakes. Unlike earlier generative models, their outputs are increasingly indistinguishable from real photographs, raising alarms about misinformation in contexts like political propaganda and fraudulent activities. Studies such as [182] reveal how diffusion models can memorize and replicate sensitive or copyrighted training samples, enabling the weaponization of synthetic content.  \n\nText-guided editing further lowers barriers to misuse, as shown in [45], where simple prompts can fabricate scenes or alter identities with minimal effort. The overfitting tendencies of diffusion models, noted in [202], exacerbate these risks by potentially leaking private or sensitive data from training sets.  \n\n#### Privacy Violations and Data Exploitation  \nThe reliance on large-scale, internet-scraped datasets raises critical privacy concerns, particularly for face and portrait editing. Models like those in [128] can synthesize realistic human likenesses without consent, enabling identity theft or unauthorized commercial use. This issue intersects with the bias amplification discussed in Section 7.3, as highlighted by [51], where skewed datasets perpetuate harmful stereotypes and marginalize underrepresented groups.  \n\n#### Intellectual Property and Artistic Integrity  \nDiffusion models blur the line between inspiration and plagiarism by replicating artistic styles, as seen in [7]. Their ability to generate derivative works without attribution challenges copyright frameworks, particularly when models memorize training images, as demonstrated in [182]. This undermines \"transformative use\" doctrines, leaving artists vulnerable to unauthorized replication.  \n\n#### Malicious Applications and Security Threats  \nThe editing capabilities of diffusion models, such as object insertion/removal in [20], could fabricate evidence or bypass security systems. Guidance mechanisms, analyzed in [102], may also be co-opted to generate harmful content, while model stochasticity complicates moderation by enabling endless variations of prohibited material. These vulnerabilities foreshadow the adversarial attack risks detailed in Section 7.5.  \n\n#### Mitigation Strategies and Responsible AI Practices  \nAddressing these challenges requires:  \n1. **Technical measures** like watermarking synthetic content ([21]) and fairness-aware training ([127]).  \n2. **Legal frameworks** mandating disclosure of AI-generated content and stricter platform controls.  \n3. **Ethical guidelines** for dataset curation and prompt filtering to prevent misuse.  \n\n#### Future Directions and Open Questions  \nKey unresolved issues include:  \n- Balancing creativity with rights protection, leveraging reproducibility studies ([46]).  \n- Reducing environmental impacts of large models ([203]).  \n\nIn conclusion, the ethical implications of diffusion models demand proactive, multidisciplinary collaboration to mitigate risks while preserving their transformative potential—a theme further explored in the context of adversarial vulnerabilities in the following subsection.\n\n### 7.5 Robustness and Vulnerability to Adversarial Attacks\n\n---\n7.5 Robustness and Vulnerability to Adversarial Attacks  \n\nThe ethical concerns and misuse potential of diffusion models, as discussed in the previous subsection, are closely tied to their technical vulnerabilities, particularly their susceptibility to adversarial attacks. While diffusion models excel in generating high-fidelity images, their robustness against malicious manipulations remains a critical challenge. This subsection examines how adversarial perturbations—subtle, intentional alterations to input data—can compromise model performance, the implications of these vulnerabilities, and potential countermeasures to enhance security and reliability.  \n\n### Understanding Adversarial Attacks in Diffusion Models  \nAdversarial attacks exploit the inherent sensitivity of machine learning models to small input changes. In diffusion models, these attacks can manifest in two primary forms:  \n1. **Input Perturbations**: Minor, often imperceptible modifications to input images or latent representations can disrupt the reverse diffusion process, leading to distorted or unintended outputs. For example, carefully crafted noise patterns can cause the model to generate irrelevant or harmful content [56].  \n2. **Conditional Guidance Exploitation**: Text-guided diffusion models, which rely on embeddings like CLIP, are vulnerable to adversarial prompts. Malicious users can engineer prompts to circumvent safety filters or produce biased, misleading, or offensive imagery [57].  \n\nTheoretical and empirical studies indicate that diffusion models, despite their stochastic denoising process, are not immune to adversarial interference. Their iterative nature can amplify small input errors, making them prone to targeted manipulations [64].  \n\n### Key Vulnerabilities and Their Implications  \n1. **Latent Space Manipulation**: The latent representations central to diffusion models can be perturbed to skew outputs. For instance, subtle alterations in latent vectors may introduce or exacerbate biases, such as misrepresenting gender or race in generated images [60].  \n2. **Attention Mechanism Hijacking**: Cross-attention layers in text-to-image models can be exploited to prioritize harmful or irrelevant features. Attackers may manipulate attention weights to force the model to generate content that deviates from the intended prompt [198].  \n3. **Attack Transferability**: Adversarial examples crafted for one diffusion model often generalize to others, even across architectures. This transferability complicates the development of universal defenses and raises concerns about the broader ecosystem’s resilience [197].  \n\n### Defense Strategies and Their Trade-offs  \nEfforts to mitigate adversarial vulnerabilities in diffusion models include:  \n1. **Adversarial Training**: Exposing models to adversarial examples during training can enhance robustness, but this approach is computationally intensive and may not protect against novel attack vectors [110].  \n2. **Randomized Smoothing**: Introducing noise to inputs or intermediate features can obscure adversarial perturbations. While effective in some scenarios, this method may degrade output quality or fail against adaptive attacks [111].  \n3. **Certified Defenses**: Techniques like interval bound propagation (IBP) offer theoretical guarantees against bounded perturbations but are often impractical for large-scale diffusion models due to excessive computational demands [113].  \n\n### Ethical and Practical Consequences  \nThe adversarial vulnerabilities of diffusion models have far-reaching implications:  \n- **Misinformation and Disinformation**: Adversarial prompts can generate highly realistic but fabricated images, exacerbating the spread of false narratives [204].  \n- **Bias Amplification**: Attacks that exploit latent space weaknesses can reinforce harmful stereotypes, undermining efforts to promote fairness and inclusivity [58].  \n- **Security Risks**: Adversarial manipulations could bypass content filters in critical applications, such as social media moderation or medical diagnostics, posing risks to safety and trust [54].  \n\n### Open Challenges and Future Directions  \n1. **Scalable and Efficient Defenses**: Developing robust yet computationally feasible defense mechanisms for large diffusion models remains an unresolved challenge. Current approaches often sacrifice robustness for practicality [55].  \n2. **Human-AI Collaboration**: Integrating human oversight during generation could mitigate adversarial effects, but scalability for real-time applications is a limiting factor [132].  \n3. **Standardized Robustness Benchmarks**: Establishing comprehensive benchmarks is essential to evaluate and compare the adversarial resilience of diffusion models across diverse tasks and datasets [69].  \n\n### Conclusion  \nThe adversarial vulnerabilities of diffusion models represent a significant barrier to their safe and ethical deployment. Addressing these challenges requires a multidisciplinary approach, combining advances in robustness research, ethical AI design, and policy frameworks. As the field progresses, prioritizing scalable defenses, rigorous testing, and stakeholder collaboration will be crucial to harnessing the benefits of diffusion models while mitigating their risks [70].  \n\n---\n\n### 7.6 Environmental and Sustainability Impact\n\n---\nThe rapid advancement and widespread adoption of diffusion models in image editing and generation have raised significant concerns regarding their environmental and sustainability impact. As these models grow in scale and complexity, their computational demands escalate, leading to substantial energy consumption and carbon emissions. This subsection evaluates the carbon footprint and energy inefficiency of large-scale diffusion models, highlighting the challenges and potential mitigation strategies.  \n\n### Energy Consumption and Carbon Footprint  \nThe computational intensity of diffusion models stems from their iterative denoising process, which requires extensive forward and reverse diffusion steps during both training and inference. Training state-of-the-art diffusion models can consume thousands of GPU hours, with energy usage comparable to small-scale industrial operations [78]. This raises ethical questions about balancing AI advancements with environmental sustainability.  \n\nThe inference phase further compounds this issue, as each image generation or editing task demands repeated GPU cycles. For high-resolution images, a single inference pass can be as energy-intensive as prolonged training sessions, making scalability a double-edged sword [74].  \n\n### Benchmarking Environmental Impact  \nQuantifying the ecological footprint of diffusion models requires moving beyond traditional metrics like FLOPs or GPU hours. Emerging frameworks integrate energy consumption data with regional carbon intensity factors to estimate total emissions, revealing stark disparities based on the energy grid’s cleanliness [79]. Comparative analyses show diffusion models lag behind GANs and VAEs in energy efficiency due to their iterative nature, underscoring the need for optimization [205].  \n\n### Mitigation Strategies  \nTo reduce environmental impact, researchers are exploring:  \n1. **Lightweight Architectures**: Techniques like model distillation and quantization can shrink computational demands while preserving performance [136].  \n2. **Process Optimization**: Adaptive sampling and sparse inference minimize redundant computations, while hardware acceleration (e.g., TPUs/GPUs) improves resource utilization.  \n3. **Sustainable Scheduling**: Aligning training with periods of high renewable energy availability can significantly lower carbon footprints.  \n\n### Ethical and Policy Considerations  \nTransparency in reporting energy use and emissions is critical. Initiatives like \"Green AI\" advocate for sustainable practices, including renewable energy adoption and energy-efficient algorithms [77]. Policymakers can incentivize sustainability through grants and regulations mandating environmental impact assessments [206].  \n\n### Future Directions  \nPromising avenues include:  \n- **Biologically Inspired Algorithms**: Mimicking energy-efficient natural processes.  \n- **Federated Learning**: Distributing computational loads across decentralized devices [137].  \n- **Quantum Computing**: A long-term solution for revolutionary efficiency gains.  \nIntegrating sustainability metrics (e.g., \"energy-per-inference\") into evaluation frameworks and developing open-source measurement tools will further promote accountability [207].  \n\n### Conclusion  \nThe environmental cost of diffusion models demands urgent action. While their capabilities in image editing are unparalleled, mitigation strategies—from technical optimizations to policy interventions—are essential to align AI progress with ecological sustainability. Collaborative efforts across research, industry, and policy can ensure these models thrive without compromising planetary health [208].  \n---\n\n## 8 Comparative Analysis and Benchmarking\n\n### 8.1 Comparative Metrics for Generative Models\n\nEvaluating the performance of diffusion models in image editing tasks requires a robust set of metrics that capture both the quality and diversity of generated samples, while also addressing domain-specific challenges. This subsection provides an overview of common evaluation metrics, focusing on their applicability to diffusion models and their comparative advantages over traditional generative adversarial networks (GANs) and variational autoencoders (VAEs). The discussion highlights both general-purpose metrics and their limitations in specialized domains, setting the stage for the subsequent analysis of domain-specific evaluation frameworks in the following subsection.\n\n### **General-Purpose Evaluation Metrics**\n#### **Fréchet Inception Distance (FID)**\nFID remains one of the most widely adopted metrics for evaluating generative models. It measures the similarity between real and generated image distributions using features from a pre-trained Inception-v3 network, with lower scores indicating better quality and diversity. While FID effectively captures key strengths of diffusion models—such as high-fidelity outputs and broad sample variability—it has notable limitations. The metric depends heavily on the Inception network's feature space, which may not align with human perception, and is sensitive to sample size. Crucially, FID cannot detect mode collapse and may fail to assess domain-specific fidelity, as discussed in [1].\n\n#### **Inception Score (IS)**\nIS evaluates generative models by measuring the entropy of class labels predicted by Inception-v3 for generated images, where higher scores suggest diverse and recognizable outputs. Though computationally efficient, IS has been criticized for its inability to detect mode collapse and its reliance on the Inception network's biases [86]. For diffusion models, IS often underrepresents their capabilities in fine-grained detail synthesis and high-resolution generation.\n\n#### **Precision and Recall**\nThese metrics provide a nuanced evaluation by separately assessing quality (precision) and diversity (recall). They are particularly useful for comparing diffusion models and GANs, as they explicitly highlight trade-offs between sample fidelity and distribution coverage [84]. However, they require large sample sizes for reliable estimation and can be computationally expensive.\n\n#### **Kernel Inception Distance (KID)**\nA variant of FID, KID uses a polynomial kernel to compare feature distributions, offering greater robustness to small sample sizes. While less biased than FID, KID inherits its dependence on the Inception network's feature space, limiting its applicability to non-natural images or specialized domains [15].\n\n#### **Perceptual Metrics: LPIPS and SSIM**\nLearned Perceptual Image Patch Similarity (LPIPS) and Structural Similarity Index (SSIM) evaluate perceptual quality by comparing deep features or structural attributes between real and generated images. These metrics are well-suited for assessing diffusion models' strengths in realistic texture synthesis [5]. However, they may overlook global consistency or semantic correctness and are constrained by their reliance on pre-trained networks.\n\n### **Human Evaluation and Task-Specific Metrics**\nHuman evaluation remains the gold standard for assessing perceptual quality and semantic coherence, particularly for text-conditioned outputs [6]. However, its scalability is limited by cost and subjectivity. For domain-specific tasks, specialized metrics are indispensable: medical imaging relies on Dice scores or Hausdorff distance [8], while 3D generation employs Chamfer distance or volumetric IoU [9]. These metrics, though critical for validation, lack generalizability across domains.\n\n### **Limitations and Emerging Directions**\nCurrent evaluation frameworks face several challenges. Most metrics are sensitive to implementation details (e.g., feature extractor choice) and struggle with non-natural images or temporal coherence in video/3D tasks [16]. Additionally, no single metric fully captures diffusion models' capabilities in high-frequency detail generation [83].\n\nEmerging approaches aim to address these gaps. For instance, [209] introduces uncertainty quantification, while [117] leverages local statistics for authenticity assessment. Future work should focus on unified protocols combining multiple metrics and standardizing domain-specific benchmarks [210].\n\n### **Conclusion**\nThe evaluation of diffusion models demands a balanced approach, integrating general-purpose metrics (e.g., FID, IS), perceptual measures, and human judgment. While current metrics provide valuable insights, their limitations underscore the need for domain-adaptive evaluation frameworks—a theme explored in depth in the following subsection. As diffusion models evolve, so too must their evaluation methodologies, ensuring alignment with both technical advancements and application-specific requirements.\n\n### 8.2 Domain-Specific Evaluation Challenges\n\n---\n\n### **Domain-Specific Evaluation Challenges**  \n\nThe evaluation of diffusion model-based image editing techniques faces significant challenges when applied to specialized domains, as traditional metrics often fail to capture domain-specific nuances. While general-purpose metrics like Fréchet Inception Distance (FID) and CLIP scores provide broad measures of image quality and semantic alignment, they struggle to account for the unique requirements of tasks such as medical imaging, artistic style transfer, or 3D scene synthesis. This subsection examines these limitations and highlights the need for tailored evaluation frameworks that align with domain-specific objectives.  \n\n#### **Medical Imaging: Diagnostic Fidelity and Biological Relevance**  \nIn medical imaging, the fidelity of generated or edited images is critical for diagnostic accuracy. Metrics like FID or Peak Signal-to-Noise Ratio (PSNR) often overlook clinically significant details, such as tumor boundaries or vascular structures, which are imperceptible in natural image evaluation [29]. The stochastic nature of diffusion models can introduce subtle artifacts that degrade diagnostic reliability, yet these are rarely captured by traditional metrics. Human expert evaluation remains indispensable, as automated measures lack the granularity to distinguish medically plausible edits from implausible ones.  \n\nComputational pathology presents further challenges, where tasks like stain normalization or synthetic tissue generation require biological relevance. Structural Similarity Index (SSIM) and similar metrics often misalign with pathologists' assessments, as they ignore cellular-level fidelity [29]. Hybrid frameworks combining automated metrics with domain-specific validation—such as cell segmentation accuracy or biomarker consistency—are emerging as a solution.  \n\n#### **Artistic and Photorealistic Editing: Subjectivity and Style Coherence**  \nArtistic style transfer and photorealistic editing demand evaluation frameworks that account for subjective aesthetics. While CLIP scores measure semantic alignment with text prompts, they fail to assess stylistic coherence or artistic intent. [211] notes that diffusion models inherently prioritize low-frequency structures early in denoising, aligning with artistic workflows but complicating metric design. Perceptual studies involving artists are essential to complement automated metrics, as human judgment remains the gold standard for aesthetic quality.  \n\n#### **Face and Portrait Editing: Identity Preservation and Ethical Biases**  \nFace editing tasks require precise control over identity preservation and attribute manipulation. Metrics like FID or Inception Score (IS) are insensitive to local facial geometry, often inflating scores for edits that unintentionally alter identity [90]. Identity-preservation metrics, such as face recognition similarity scores, are critical to address this gap.  \n\nEthical concerns further complicate evaluation. Diffusion models can amplify biases in training data, such as racial or gender stereotypes, which traditional metrics fail to detect [95]. Fairness-aware metrics, like demographic parity scores, are needed to ensure equitable representations in edited outputs.  \n\n#### **Video and Dynamic Scenes: Temporal Consistency and Motion Plausibility**  \nVideo editing introduces temporal coherence as a critical criterion, which frame-wise metrics like FID or SSIM cannot address. Small inconsistencies in motion or lighting degrade perceptual quality but remain invisible to static metrics [176]. Temporal FID (TFID) or optical flow-based consistency measures are proposed alternatives.  \n\nLong-range dependencies in tasks like video inpainting further challenge traditional evaluation. Pixel-level metrics like PSNR ignore motion trajectories, necessitating human evaluations or action recognition models to assess naturalness [24].  \n\n#### **3D Scene Synthesis: Multi-View Consistency and Material Fidelity**  \n3D generation demands evaluation of geometric coherence across viewpoints, a dimension absent in 2D metrics like FID. [177] highlights that photorealistic 2D projections can mask 3D inconsistencies, inflating scores. Incorporating 3D reconstruction metrics (e.g., Chamfer distance) and physics-based shading measures is essential for realistic synthesis.  \n\n#### **Toward Hybrid Evaluation Frameworks**  \nThe limitations of existing metrics underscore the need for domain-adaptive evaluation. [44] advocates for modular frameworks combining task-specific metrics (e.g., lesion detection rates) with general-purpose ones (e.g., FID). Similarly, [95] introduces EditEval, a benchmark integrating Large Multimodal Model (LMM) scores to bridge semantic alignment and human judgment.  \n\n#### **Future Directions**  \nCollaboration between domain experts and ML researchers is critical. Interpretability tools, such as latent space probes, can uncover model biases to inform metric design [212]. Adversarial testing is also vital to expose metric vulnerabilities in specialized domains [94].  \n\nIn conclusion, domain-specific challenges necessitate evaluation frameworks that combine automated metrics with domain expertise. As diffusion models advance, their validation must evolve to meet the rigorous demands of applications ranging from medical diagnostics to creative arts. This sets the stage for the next subsection, which explores the interplay between human and automated evaluation paradigms.  \n\n---\n\n### 8.3 Human vs. Automated Evaluation\n\n### 8.3 Human vs. Automated Evaluation  \n\nThe evaluation of diffusion-based image editing methods presents a fundamental tension between scalable automated metrics and perceptually nuanced human judgments. While automated metrics offer efficiency and reproducibility, they often fail to capture the aesthetic and contextual subtleties that human evaluators naturally assess. This subsection examines the complementary roles of these approaches, their limitations, and emerging strategies to bridge the gap between them.  \n\n#### **Automated Metrics: Efficiency at the Cost of Nuance**  \nAutomated metrics like Fréchet Inception Distance (FID), CLIP scores, and Structural Similarity Index (SSIM) provide standardized benchmarks for assessing fidelity, text-image alignment, and structural preservation, respectively. FID quantifies distributional similarity between generated and real images, offering a broad measure of realism [95]. CLIP scores leverage pretrained vision-language models to evaluate semantic alignment with text prompts, making them indispensable for text-guided editing [213]. SSIM focuses on structural consistency, ensuring edits preserve the original image’s composition [21].  \n\nHowever, these metrics exhibit critical blind spots. FID may penalize creative deviations from the training distribution, even when stylistically valid [98]. CLIP scores struggle with localized edits or fine-grained semantic shifts [97], while SSIM overlooks perceptual qualities like color harmony or texture coherence [201].  \n\n#### **Human Evaluation: The Gold Standard for Perceptual Quality**  \nHuman assessments excel where automated metrics falter, particularly in judging realism, aesthetic appeal, and contextual coherence. For instance, [166] relies on human studies to validate the perceptual impact of adversarial edits, as automated metrics cannot fully capture their subtle distortions. Similarly, [36] uses user studies to evaluate spatial precision and visual plausibility, metrics that require human discernment.  \n\nHuman evaluators also uncover biases and artifacts missed by automated systems. In [43], annotators identify lighting and shadow inconsistencies in object-level edits, while [38] employs human ratings to assess temporal coherence—a dimension poorly measured by frame-wise metrics.  \n\n#### **Challenges and Hybrid Solutions**  \nDespite their advantages, human evaluations face scalability and consistency issues. Annotator subjectivity can lead to divergent preferences, as seen in [190], where stylistic edits elicit varied responses. To address these limitations, hybrid approaches combine human insights with automated metrics. [30] integrates human ratings with FID and CLIP scores, while [99] refines automated metrics using human feedback to better align with perceptual standards.  \n\n#### **Emerging Trends: Bridging the Gap**  \nRecent work seeks to harmonize human and automated evaluation. [33] introduces perceptual losses trained on human preference data, improving metric correlation with subjective quality. [50] uses adversarial training to align automated scores with human-centric criteria like naturalness and detail preservation.  \n\nLarge multimodal models (LMMs) also show promise as proxy evaluators. [214] employs LMMs to generate detailed image critiques, though [31] notes their limitations in fine-grained aesthetic judgment.  \n\n#### **Comparative Insights**  \nCase studies reveal systematic disparities between automated and human evaluations. [24] finds that automated metrics overrate global edits while undervaluing localized refinements prioritized by humans. Conversely, [178] shows weak correlation between SSIM and human ratings for style transfer, where perceptual alignment outweighs structural fidelity.  \n\nIn [215], human evaluators detect artifacts invisible to FID, while [121] demonstrates CLIP scores’ reliability for text-driven edits but failure in spatially conditioned tasks [216].  \n\n#### **Conclusion and Future Directions**  \nThe dichotomy between human and automated evaluation underscores the need for integrative frameworks. While automated metrics provide scalability, human judgments remain irreplaceable for assessing perceptual quality. Future research should focus on hybrid metrics—such as those proposed in [39] and [217]—that combine human preference data with adversarial training. By harmonizing these approaches, the field can advance diffusion-based editing techniques that satisfy both technical rigor and artistic intent.\n\n### 8.4 Benchmarking Diffusion Models Against Alternatives\n\n### 8.4 Benchmarking Diffusion Models Against Alternatives  \n\nThe rapid advancement of diffusion models (DMs) has necessitated systematic comparisons with other generative frameworks—notably Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)—to assess their relative merits in image editing and generation. Building on the evaluation methodologies discussed in Section 8.3, this subsection synthesizes empirical findings across key dimensions: sample quality, diversity, training dynamics, computational efficiency, and task-specific performance. By contextualizing these benchmarks, we clarify where DMs excel and where alternatives remain competitive.  \n\n#### **Sample Quality and Fidelity**  \nDiffusion models consistently achieve state-of-the-art results in sample fidelity, outperforming GANs and VAEs on metrics like Fréchet Inception Distance (FID) and Inception Score (IS). This advantage stems from their iterative denoising process, which refines outputs progressively. For example, [21] shows that DMs surpass GANs in reconstruction and editing tasks, avoiding common artifacts like mode collapse or blurriness. Similarly, [44] demonstrates DM superiority in photorealistic generation on complex datasets (e.g., ImageNet).  \n\nHowever, GANs retain a critical advantage in real-time applications due to their single-pass generation. As noted in [203], DMs incur higher computational costs for superior quality, creating a trade-off between fidelity and speed.  \n\n#### **Diversity and Mode Coverage**  \nDMs excel at modeling multimodal distributions, addressing a key weakness of VAEs, which often produce oversmoothed outputs. Theoretical analysis in [48] explains how DMs handle separated modes naturally through stochastic denoising, while GANs and VAEs require complex regularization. Empirical support comes from [46], where DMs exhibit better mode coverage on diverse datasets (e.g., CelebA-HQ) than GANs.  \n\nThat said, GANs can rival DMs in diversity when enhanced with techniques like spectral normalization or contrastive learning. [51] finds that ensemble GANs match DM diversity with careful design, though DMs achieve this more robustly.  \n\n#### **Training Stability and Convergence**  \nUnlike GANs, which suffer from adversarial instability, DMs train reliably due to their noise-prediction objective. [127] highlights that DMs can be fine-tuned with reinforcement learning—a task challenging for GANs due to their non-stationary optimization. VAEs, while stable, often face posterior collapse; hybrid approaches like [218] combine VAEs with diffusion processes to mitigate this, leveraging DM stability while preserving compact latent spaces.  \n\n#### **Computational and Memory Efficiency**  \nComputational cost remains a DM drawback. [49] shows DMs can be 10× slower than GANs for comparable quality. However, advancements like [50] reduce sampling steps via distillation, narrowing the efficiency gap. VAEs, being lightweight, are preferred for low-resource tasks (e.g., anomaly detection), as discussed in [219].  \n\n#### **Task-Specific Performance**  \nDMs dominate conditional generation tasks, such as text-to-image synthesis, where iterative refinement is advantageous. For example, [45] shows DMs outperform autoregressive and GAN-based models in aligning outputs with text prompts. Conversely, GANs are better suited for latency-sensitive applications like real-time style transfer, as noted in [20].  \n\n#### **Ethical and Robustness Considerations**  \nDMs exhibit greater robustness to adversarial attacks than GANs due to their iterative smoothing. However, [182] reveals DMs may memorize training data, raising privacy concerns less prevalent in GANs.  \n\n#### **Future Directions**  \nThe field is converging toward hybrid models that combine DM quality with GAN/VAE efficiency. Innovations like [220] (integrating DM noise into VAEs) and [27] (unifying diffusion with optimal transport) exemplify this trend.  \n\nIn summary, diffusion models lead in fidelity and multimodal control but lag in speed, while GANs and VAEs remain viable for efficient or specialized tasks. The choice hinges on application priorities, with DMs favored for quality-critical scenarios and alternatives for resource-constrained or real-time settings.\n\n## 9 Future Directions and Open Problems\n\n### 9.1 Real-Time and Interactive Editing\n\n---\n### 9.1 Real-Time and Interactive Editing  \n\nThe pursuit of real-time and interactive capabilities in diffusion-based image editing represents a critical step toward practical adoption, bridging the gap between high-quality synthesis and user-centric applications. While diffusion models excel in generating photorealistic images, their iterative denoising process introduces computational bottlenecks that hinder real-time performance. This subsection explores recent advancements addressing these challenges, focusing on sampling acceleration, interactive frameworks, and hybrid architectures, while highlighting remaining open problems.  \n\n#### Challenges in Real-Time Diffusion  \nThe sequential nature of diffusion sampling—often requiring hundreds of denoising steps—poses the primary obstacle to real-time applications. Traditional Denoising Diffusion Probabilistic Models (DDPMs) suffer from slow inference due to their Markov chain-based design, a limitation exacerbated in high-resolution synthesis where each step involves computationally intensive U-Net evaluations. Conditional guidance mechanisms, such as text or spatial masks, further complicate the pipeline. For instance, Stable Diffusion [5] operates in a compressed latent space but still struggles with latency, underscoring the need for optimization strategies.  \n\n#### Advancements in Sampling Acceleration  \nRecent work has made significant strides in reducing inference time without sacrificing quality. **Progressive distillation**, as demonstrated in [13], iteratively compresses trained models into smaller versions requiring only 4–8 steps, achieving near-real-time generation. Similarly, [221] introduces a hybrid diffusion-GAN framework that synthesizes images in a single forward pass, bypassing iterative refinement through adversarial training.  \n\nArchitectural innovations have also contributed to efficiency gains. [14] exploits temporal redundancy by caching and reusing high-level features across denoising steps, yielding a 2.3× speedup for Stable Diffusion v1.5. Meanwhile, [186] adopts a ViT-style patching mechanism to reduce memory usage and inference time, demonstrating the potential of modular design principles.  \n\n#### Interactive Editing Frameworks  \nEnabling user-guided interactivity is essential for practical deployment. [7] introduces a meta-network that dynamically fuses multi-modal inputs (e.g., text and sketches) during sampling, allowing real-time adjustments without retraining. This approach exemplifies how diffusion models can adapt to diverse inputs for interactive workflows.  \n\nFor tasks like image morphing, traditionally dominated by GANs, [222] achieves seamless real-time transitions by interpolating latent representations and noise inputs. Extending this interactivity to 3D domains, [18] enables dynamic scene manipulation through intuitive controls like sketches or text prompts, showcasing the versatility of diffusion-based frameworks.  \n\n#### Hybrid and Lightweight Architectures  \nCombining diffusion models with other generative paradigms has yielded efficient alternatives. [184] merges VAEs with diffusion models to leverage compact latent spaces while preserving high-fidelity synthesis, reducing sampling time for interactive applications. Similarly, [223] unifies diffusion and GAN objectives, achieving faster convergence and lower latency than standalone diffusion models.  \n\n#### Benchmarking and Trade-offs  \nEvaluating these methods requires balancing speed and quality. Metrics like Fréchet Inception Distance (FID) and CLIP scores reveal trade-offs; for example, [13] achieves an FID of 3.0 on CIFAR-10 with just 4 steps, rivaling traditional 1000-step models. However, maintaining consistency for complex scenes or high-resolution outputs remains challenging, as noted in [156].  \n\n#### Future Directions  \nKey open problems include:  \n1. **Hardware-aware optimization**: Techniques like quantization and sparse inference must adapt to the dynamic nature of diffusion processes for edge deployment.  \n2. **Dynamic step scheduling**: Adaptive computation allocation, inspired by [199], could minimize redundant operations.  \n3. **User-centric design**: Interactive tools, such as those in [7], must evolve to empower non-expert users with intuitive control.  \n\nIn summary, real-time and interactive diffusion models demand a holistic approach—combining algorithmic innovation, architectural efficiency, and user-centered design. While progress has been substantial, addressing the trade-offs between speed, quality, and controllability remains pivotal for unlocking their full potential in practical applications.  \n---\n\n### 9.2 Multimodal Fusion and Cross-Modal Editing\n\n---\n### 9.2 Multimodal Fusion and Cross-Modal Editing  \n\nBuilding upon the real-time and interactive capabilities discussed in Section 9.1, the integration of multimodal inputs—such as text, audio, and spatial data—into diffusion-based image editing represents a critical frontier for richer creative control and more intuitive interfaces. While current diffusion models excel at text-conditioned generation, their ability to harmonize diverse input modalities remains underexplored. This subsection examines the potential of multimodal fusion and cross-modal editing, highlighting advancements that bridge modalities, address alignment challenges, and pave the way for efficient cross-domain applications—a natural precursor to the lightweight architectures discussed in Section 9.3.  \n\n#### **Multimodal Conditioning: Architectures and Challenges**  \nDiffusion models inherently support multimodal conditioning through cross-attention layers, as demonstrated in [5]. These layers enable text-visual alignment, but extending this to other modalities (e.g., audio spectrograms or 3D spatial coordinates) could unlock novel editing paradigms. For instance, [161] shows how cross-attention maps localize edits, suggesting similar mechanisms could adapt to audio or geometric inputs.  \n\nHowever, multimodal fusion introduces two key challenges:  \n1. **Embedding Alignment**: Heterogeneous data require robust shared latent spaces. While [11] proposes unified embeddings for related domains, aligning disparate modalities (e.g., audio and text) demands innovative techniques.  \n2. **Modality-Specific Noise Scheduling**: Audio and image signals exhibit different spectral properties. [89] adapts diffusion dimensions to signal redundancy, but a generalized framework for multimodal noise scheduling remains open.  \n\n#### **Cross-Modal Editing: Techniques and Trade-offs**  \nCross-modal editing leverages one modality to guide edits in another. For example:  \n- **Text-to-Audio-Visual**: [22] reconstructs images from text, but analogous approaches could map audio cues (e.g., emotion) to visual styles.  \n- **Spatial-to-Visual**: [121] blends features across diffusion steps, a strategy applicable to spatial-visual fusion (e.g., sketches to photorealistic edits).  \n\nChallenges persist in semantic consistency and efficiency:  \n- **Semantic Gaps**: [44] reveals fine-grained alignment issues, exacerbated when combining ambiguous audio with precise visual edits. [224] offers mitigation via latent space smoothing.  \n- **Computational Overhead**: [49] reduces FLOPs by reusing features, but multimodal pipelines may need dynamic computation allocation per modality.  \n\n#### **Spatial and Temporal Modalities in Practice**  \nSpatial inputs (e.g., masks, 3D meshes) already enhance controllability, as seen in [20] and [177]. Temporal modalities, however, remain underexplored. While [176] connects image and video models, applications like audio-visual synchronization (e.g., lip-syncing) demand further research.  \n\n#### **Ethical and Efficiency Considerations**  \nMultimodal editing raises ethical concerns, particularly with sensitive data (e.g., voice-face pairs). [94] identifies latent space vulnerabilities that adversarial attacks could exploit. Efficiency is equally critical; [149] notes the environmental cost of large models, urging efficient designs for scalable multimodal pipelines—a theme expanded in Section 9.3.  \n\n#### **Future Directions**  \n1. **Unified Multimodal Latent Spaces**: Building on [122], joint embeddings for text, audio, and spatial data could enable seamless cross-modal interpolation.  \n2. **Adaptive Noise Scheduling**: Inspired by [23], dynamic noise schedules per modality may balance fidelity and editability.  \n3. **Real-Time Multimodal Interaction**: Integrating [7] with spatial/audio inputs could enable live collaborative editing.  \n\nIn summary, multimodal fusion and cross-modal editing expand the horizons of diffusion-based image editing, offering richer creative tools while demanding solutions for alignment, efficiency, and ethical risks—challenges that resonate with the broader themes of real-time interactivity (Section 9.1) and lightweight deployment (Section 9.3).  \n\n---\n\n### 9.3 Lightweight and Efficient Architectures\n\n---\n### 9.3 Lightweight and Efficient Architectures  \n\nThe computational intensity of diffusion models presents a significant barrier to their deployment on resource-constrained edge devices, such as smartphones and IoT systems. While the previous subsection explored the potential of multimodal fusion in diffusion models, this section shifts focus to optimizing these models for practical, real-world applications by reducing their computational and memory overhead. The following subsection will then examine the ethical implications of these efficient architectures, creating a natural progression from technical innovation to responsible deployment. Here, we review key techniques—including model distillation, quantization, sparse inference, and hardware acceleration—that enable high-quality diffusion-based image editing while maintaining efficiency.  \n\n#### **Model Distillation and Compression**  \nDistilling large diffusion models into compact, efficient variants is a promising approach to address computational constraints. For instance, [50] introduces a conditional consistency loss that enables high-quality image generation in just 1-4 denoising steps, dramatically reducing inference time without sacrificing output quality. Similarly, [100] achieves remarkable parameter efficiency by fine-tuning only the singular values of weight matrices, compressing the model by 2,200× compared to full fine-tuning. These methods not only reduce memory usage but also mitigate overfitting, making them ideal for edge deployment.  \n\nProgressive distillation, though not explicitly covered in the surveyed papers, aligns with the efficiency gains demonstrated in [120], which optimizes performance by disentangling source and target diffusion branches. By eliminating redundant computations, such techniques pave the way for real-time editing on low-power devices.  \n\n#### **Quantization and Low-Rank Approximations**  \nQuantization—reducing the precision of weights and activations—is critical for edge deployment, where memory and power efficiency are paramount. [190] employs pixel-level reward models and adversarial training to accelerate inference, indirectly benefiting from quantization-aware optimization. Complementing this, [201] leverages compact latent representations to reduce feature space dimensionality, further lowering computational costs. Together, these approaches enable efficient editing while preserving semantic fidelity.  \n\n#### **Sparse Inference and Adaptive Sampling**  \nSparse inference techniques minimize redundant computations by focusing on relevant regions during denoising. [98] shows that stochastic differential equations (SDEs) achieve better editing fidelity with fewer steps than ODEs, suggesting SDE-based sampling is more efficient for edge devices. Additionally, [121] selectively updates regions of interest through multi-level feature blending, reducing unnecessary computations in unchanged areas.  \n\nAdaptive sampling strategies, such as those in [225], dynamically adjust edit granularity based on user input, further optimizing step efficiency. These methods are particularly valuable for interactive edge applications requiring real-time responsiveness.  \n\n#### **Hardware Acceleration and Parallelization**  \nSpecialized hardware optimizations are essential for unlocking diffusion models' potential on edge devices. [125] uses GPU parallelization and hierarchical upsampling to achieve megapixel-scale edits efficiently, while [165] reduces memory overhead via anchor-based cross-frame attention for real-time video editing. Frameworks like [176] further optimize resource usage by decoupling image and video processes, enabling complex tasks like video inpainting without retraining.  \n\n#### **Challenges and Future Directions**  \nDespite progress, key challenges remain. The trade-off between model size and edit quality, especially for detailed or multimodal edits, requires further exploration. Techniques like [99] show promise by enabling efficient multi-object edits, but broader generalization is needed.  \n\nIntegration with emerging edge hardware (e.g., neuromorphic chips) is underexplored. Methods like [36] could benefit from hardware-specific optimizations to achieve mobile-ready interactivity.  \n\nFinally, the environmental impact of lightweight models warrants attention. Future work might explore dynamic sparsity or hybrid architectures combining diffusion models with lightweight GANs, as suggested by [116].  \n\nIn summary, lightweight and efficient architectures are pivotal for democratizing diffusion-based image editing. Advances in distillation, quantization, sparse inference, and hardware acceleration will enable ubiquitous access to these powerful tools while setting the stage for the ethical considerations discussed in the next subsection.  \n---\n\n### 9.4 Ethical and Responsible AI Practices\n\n---\n### 9.4 Ethical and Responsible AI Practices  \n\nAs diffusion models transition from research prototypes to widely accessible tools—enabled by the lightweight architectures discussed in Section 9.3—their ethical implications demand rigorous scrutiny. While these models empower creative expression, their capacity to manipulate visual content also introduces risks of misuse, bias amplification, and environmental harm. This subsection examines these challenges and proposes frameworks for responsible deployment, setting the stage for Section 9.5's exploration of long-term consistency, where ethical considerations intersect with technical reliability.  \n\n#### **Bias Amplification and Fairness Concerns**  \nDiffusion models often perpetuate and amplify societal biases embedded in their training data. For example, [46] demonstrates how models trained on imbalanced datasets like CelebA exacerbate gender disparities, generating female faces 148% more frequently than the original dataset's 57.9% bias. Such outcomes underscore the need for proactive debiasing strategies.  \n\nRecent advances offer promising solutions: [51] uses diffusion-generated synthetic data to diversify training distributions, while [130] steers sampling toward uniform latent distributions without retraining. These approaches highlight the feasibility of algorithmic fairness interventions, though widespread adoption remains a challenge.  \n\n#### **Misuse Potential and Ethical Risks**  \nThe dual-use nature of diffusion models enables both creative and harmful applications. [182] reveals how models memorize training samples, risking privacy breaches through near-identical replication of sensitive or copyrighted content.  \n\nEnvironmental concerns also emerge. [108] analogizes diffusion processes to energy-intensive thermodynamic systems, emphasizing the carbon footprint of iterative denoising. Techniques like [49] mitigate this by optimizing sampling efficiency, aligning with the efficiency goals outlined in Section 9.3.  \n\n#### **Transparency and Accountability Frameworks**  \nTo address these risks, the field must adopt measurable transparency standards. [202] introduces \"effective model memorization\" (EMM) to quantify the trade-off between generalization and data replication, enabling audits for unintended memorization. Similarly, [192] advocates for interpretable conditioning mechanisms to enhance model scrutability.  \n\nReinforcement learning offers a pathway for ethical alignment: [127] employs human feedback to fine-tune models via denoising diffusion policy optimization (DDPO), while [168] enables pixel-level ethical controls through granular reward signals.  \n\n#### **Toward Responsible Deployment**  \nKey pillars for ethical diffusion models include:  \n1. **Debiasing Protocols**: Implement [130] to ensure equitable outputs.  \n2. **Memorization Audits**: Adopt EMM metrics [202] for privacy preservation.  \n3. **Sustainable Design**: Prioritize energy-efficient methods like [49].  \n4. **Human Oversight**: Integrate RL-based alignment [127] for ethical constraints.  \n\n#### **Open Challenges and Future Directions**  \nCritical gaps persist:  \n- **Real-Time Bias Monitoring**: Dynamic debiasing during inference remains underexplored.  \n- **Standardized Ethics Guidelines**: The absence of global norms complicates accountability.  \n- **Adversarial Robustness**: Models need defenses against manipulation.  \n\nIn summary, ethical diffusion model deployment requires interdisciplinary collaboration—merging technical innovations with policy frameworks. By addressing bias, misuse, and transparency, the field can harness these models' potential while mitigating risks. This foundation is essential for tackling the long-term consistency challenges discussed next, where ethical and technical reliability converge.  \n---\n\n### 9.5 Open Problems in Long-Term Consistency\n\n### 9.5 Open Problems in Long-Term Consistency  \n\nThe rapid progress in diffusion model-based image editing has revealed a critical gap in maintaining long-term consistency, particularly as applications expand beyond static images to dynamic media like videos, 3D scenes, and interactive systems. Long-term consistency refers to the ability of edited content to retain logical, visual, or semantic integrity over time (in videos) or across spatial dimensions (in 3D or multi-view scenes). While Section 9.4 highlighted ethical challenges in diffusion models, this subsection shifts focus to technical limitations, exploring open problems in achieving temporal and spatial coherence, identifying current bottlenecks, and proposing actionable research directions.  \n\n#### Challenges in Temporal Consistency for Video Editing  \nDiffusion models excel in single-image editing but struggle with temporal coherence in video sequences. Key challenges include:  \n1. **Frame-by-Frame Discrepancies**: Independent editing of frames leads to flickering or semantic inconsistencies, as diffusion models lack built-in mechanisms for temporal alignment.  \n2. **Motion Preservation**: Editing videos while maintaining natural motion trajectories (e.g., object removal in dynamic scenes) remains unsolved, as most models ignore explicit motion modeling.  \n3. **Computational Scalability**: Enforcing consistency often requires cross-frame optimization, exacerbating computational costs [66].  \n\nCurrent solutions, such as attention-based alignment or optical flow guidance, are heuristic and fail to address occlusions or long-range dependencies. A unified framework—potentially integrating recurrent architectures or physics-based priors—is needed to bridge this gap.  \n\n#### Spatial Coherence in 3D and Multi-View Editing  \nFor 3D scenes and multi-view synthesis, spatial coherence demands that edits remain plausible across viewpoints and lighting conditions. Persistent issues include:  \n1. **Viewpoint Artifacts**: Edits often degrade when rendered from novel angles due to limited 3D awareness in diffusion models.  \n2. **Physical Plausibility**: Changes to materials or lighting frequently violate physical laws (e.g., incorrect shadows or reflectance).  \n3. **Hierarchical Editing**: Large-scale scene edits (e.g., urban layouts) disrupt global semantics when local changes lack hierarchical constraints.  \n\nHybrid approaches combining diffusion models with 3D representations (e.g., NeRF) show promise but face data scarcity and computational bottlenecks. For instance, fine-tuning on multi-view datasets improves consistency but lacks generalization to unseen scenes.  \n\n#### Theoretical and Evaluation Gaps  \nThe absence of formal metrics for long-term consistency compounds these challenges. Existing metrics like Fréchet Video Distance (FVD) fail to capture:  \n- **Temporal Artifacts**: Flicker, motion jitter, or semantic drift in videos.  \n- **Spatial Robustness**: Viewpoint stability or physical correctness in 3D edits.  \n\nNew evaluation frameworks must quantify these dimensions while balancing consistency with creative flexibility—a trade-off underexplored in current research.  \n\n#### Ethical and Practical Implications  \nIncoherent edits pose risks beyond technical limitations. Temporal artifacts in medical imaging or autonomous driving scenarios could propagate diagnostic errors or safety hazards [56]. Addressing these issues requires interdisciplinary collaboration, merging technical innovations with ethical considerations.  \n\n#### Future Directions  \n1. **Dynamic Architectures**: Diffusion transformers with memory mechanisms or latent space regularization for motion priors.  \n2. **Physics-Guided Editing**: Hard-coding physical constraints (e.g., fluid dynamics or rigid-body mechanics) into the denoising process.  \n3. **Multimodal Grounding**: Leveraging text, audio, or sensor data to anchor edits in real-world contexts.  \n4. **Efficient Inference**: Techniques like distillation or sparse sampling to enable real-time consistency.  \n5. **Interactive Refinement**: Human-in-the-loop systems for iterative consistency tuning in creative workflows.  \n\n#### Conclusion  \nLong-term consistency is a pivotal challenge for diffusion models, spanning technical, theoretical, and ethical domains. While progress in video interpolation or 3D-aware synthesis is notable, holistic solutions demand interdisciplinary efforts—from robust metrics to scalable architectures. Future work must prioritize these directions to enable consistent, reliable, and ethically sound editing tools. This sets the stage for Section 9.6, which will explore emerging applications built upon these foundational advancements.\n\n## 10 Conclusion\n\n### 10.1 Summary of Key Insights\n\nDiffusion models have emerged as a transformative force in image editing, redefining the boundaries of generative modeling with their ability to produce high-fidelity, diverse, and controllable outputs. Unlike traditional generative adversarial networks (GANs) or variational autoencoders (VAEs), diffusion models leverage an iterative denoising process that ensures stability and diversity in generation, addressing long-standing challenges such as mode collapse and training instability [1]. This probabilistic framework enables fine-grained control over image synthesis, allowing for the creation of intricate details and photorealistic textures. For instance, [5] demonstrates how operating in a latent space reduces computational overhead while preserving visual quality, making diffusion models scalable for high-resolution tasks.\n\nA hallmark of diffusion models is their seamless integration of multimodal inputs, particularly in text-guided image editing. By incorporating cross-attention mechanisms, models like [6] and [155] achieve precise alignment between textual prompts and visual content. This capability has unlocked new possibilities in creative applications, such as artistic style transfer and semantic image synthesis, where attribute-specific control is critical. For example, [116] bridges the gap between GANs and diffusion models by disentangling latent directions for interpretable edits, showcasing the versatility of diffusion-based approaches.\n\nThe adaptability of diffusion models extends to conditional and localized editing, where techniques like spatial conditioning, semantic guidance, and multi-modal integration enable targeted modifications with high precision. [7] illustrates how combining text and sketch inputs can generate and refine facial features with remarkable accuracy. Similarly, [154] leverages depth information to ensure geometric consistency in synthesized images, highlighting the model's flexibility across diverse editing scenarios.\n\nBeyond 2D images, diffusion models have made significant strides in 3D and video domains. Works such as [9] and [18] demonstrate the potential of latent-space operations for 3D shape generation and single-view reconstruction. In video editing, [156] and [210] address temporal coherence and dynamic scene manipulation, paving the way for applications in animation and virtual reality.\n\nDespite their strengths, diffusion models face challenges in computational efficiency due to their iterative nature. Recent advancements, such as the progressive distillation method in [13] and the training-free acceleration paradigm in [14], have mitigated these issues by reducing sampling steps and exploiting temporal redundancy. These innovations are critical for real-time deployment in resource-constrained environments.\n\nEthical considerations remain paramount, as diffusion models raise concerns about data privacy and misuse. Studies like [19] reveal risks of training data memorization, while [158] explores forensic tools to detect synthetic content. These findings underscore the need for robust ethical frameworks to guide responsible adoption.\n\nTheoretical advancements continue to deepen our understanding of diffusion models. [10] provides a rigorous analysis of their statistical properties and optimization techniques, while [226] introduces novel solvers to enhance generation efficiency. As highlighted in [84], these models represent a paradigm shift in visual computing, with far-reaching implications across art, medicine, and entertainment. Their unparalleled quality, versatility, and control solidify diffusion models as the cornerstone of modern image editing.\n\n### 10.2 Transformative Impact and Applications\n\n---\n### 10.2 Transformative Impact and Applications  \n\nBuilding upon the foundational capabilities of diffusion models outlined in previous sections—such as their iterative denoising process, multimodal conditioning, and fine-grained controllability—this subsection examines their transformative applications across diverse domains. These models have not only addressed limitations of earlier generative approaches but have also unlocked novel workflows in fields ranging from medicine to entertainment, driven by their unique ability to balance high fidelity with adaptability.  \n\n#### Medical Imaging and Computational Pathology  \nIn medical imaging, diffusion models have overcome critical challenges like data scarcity and anatomical fidelity. By synthesizing high-quality annotated datasets, they enhance tasks such as tumor segmentation and anomaly detection. [29] highlights their superiority over traditional methods in generating realistic images while preserving pathological diversity. A breakthrough is demonstrated in [227], where latent diffusion priors improve MRI reconstruction accuracy, reducing reliance on costly imaging protocols. These advances underscore the potential of diffusion models to revolutionize diagnostic workflows and democratize access to medical AI tools.  \n\n#### Artistic and Photorealistic Style Transfer  \nThe artistic domain has benefited from diffusion models' capacity for diverse, coherent stylization without the mode collapse issues plaguing GANs. [21] introduces a rectifier module to maintain structural integrity during style transfer, while [175] leverages pixel-aware cross-attention for photorealistic super-resolution. Such innovations empower users to achieve professional-grade artistic effects with minimal input, bridging the gap between technical precision and creative expression.  \n\n#### Face and Portrait Editing  \nDiffusion models have redefined portrait editing through disentangled attribute control and identity preservation. [90] enables multi-attribute edits (e.g., age, expression) without retraining, and [25] maps real images to an editable noise space for semantic modifications like smile enhancement. As validated in [98], these methods outperform GANs in avoiding artifacts, making them indispensable for applications from digital avatars to forensic analysis.  \n\n#### Video and Dynamic Scene Manipulation  \nExtending diffusion models to video has addressed long-standing challenges in temporal coherence. [176] achieves frame-consistent inpainting and outpainting without additional training, while [177] enables part-level edits in dynamic scenes. These approaches pave the way for real-time video editing tools capable of complex manipulations like object removal or motion synthesis.  \n\n#### 3D Scene Synthesis and Neural Rendering  \nIn 3D content creation, diffusion models integrate seamlessly with neural rendering. [35] simplifies scene composition through layered representations, and [212] reveals their implicit 3D geometric understanding, enabling depth-aware edits. Such capabilities are transforming virtual reality and architectural visualization by making high-fidelity 3D asset creation accessible to non-experts.  \n\n#### Text-Guided and Multimodal Editing  \nText-to-image alignment, a hallmark of diffusion models, has been refined further. [121] aligns textual prompts with image semantics for object replacement, while [228] optimizes text embeddings to enhance edit fidelity in inverse tasks. These advancements highlight how diffusion models unify language and vision, enabling intuitive image manipulation through natural language.  \n\n#### Efficiency and Real-Time Applications  \nTo address computational demands, innovations like [49] (reducing FLOPs by reusing prior denoising steps) and [93] (exploiting temporal redundancy) have made real-time deployment feasible. Such optimizations are critical for scaling diffusion models to mobile and interactive environments.  \n\n#### Conclusion  \nThe applications discussed here illustrate how diffusion models have transcended the limitations of prior generative frameworks, offering unprecedented precision and versatility. As these models continue to evolve—coupled with the ethical and technical challenges outlined in the subsequent subsection—their potential to redefine industries and creative practices remains unparalleled.\n\n### 10.3 Persistent Challenges and Ethical Concerns\n\n---\n### 10.3 Persistent Challenges and Ethical Concerns  \n\nWhile diffusion models have revolutionized image editing, their rapid adoption has also exposed significant challenges and ethical dilemmas that must be addressed to ensure sustainable and responsible progress. These issues span computational limitations, algorithmic biases, and societal risks, requiring interdisciplinary solutions to align technological advancements with ethical imperatives.  \n\n#### Computational and Resource Constraints  \nThe resource-intensive nature of diffusion models remains a critical barrier. Training and inference demand substantial computational power, limiting accessibility for users without high-end hardware [95]. Real-time applications, such as interactive video editing [38], face latency due to iterative denoising, while the environmental footprint of large-scale training raises sustainability concerns. Although distillation techniques [50] and sparse inference methods offer partial solutions, they often compromise output quality. Future work must prioritize hardware-efficient architectures and optimization strategies to democratize access.  \n\n#### Training Instability and Bias Amplification  \nDiffusion models are susceptible to training instability, especially in data-scarce domains like medical imaging, where overfitting can distort outputs. More alarmingly, these models inherit and amplify biases from their training data, generating stereotyped or exclusionary content. For instance, text-guided editing [213] may produce culturally insensitive imagery due to ambiguous prompts. While approaches like [201] attempt to disentangle biased representations, systematic debiasing frameworks remain nascent. Proactive auditing and dataset diversification are essential to mitigate these risks.  \n\n#### Robustness and Adversarial Vulnerabilities  \nThe susceptibility of diffusion models to adversarial attacks poses security risks, particularly in forensic and legal contexts. Minor perturbations to inputs can drastically alter outputs, enabling malicious manipulations [166]. For example, [22] demonstrates how adversarial noise disrupts editing fidelity. While adversarial training can enhance robustness, it often reduces model flexibility. Integrating certified robustness methods tailored for diffusion models could offer a viable path forward.  \n\n#### Ethical Implications and Misuse Potential  \nThe ability to generate photorealistic edits has escalated concerns about misinformation and deepfakes. Tools like [36] enable precise facial manipulation, raising identity theft risks, while [34] complicates digital forensics by seamlessly altering scenes. Current safeguards, such as watermarking [166], are insufficient against determined adversaries. Policymakers must collaborate with researchers to establish enforceable guidelines for synthetic media.  \n\n#### Long-Term Consistency and Generalization  \nAchieving coherence in temporal or spatial edits remains challenging. Video editing frameworks like [96] struggle with frame-level inconsistencies, while 3D-aware models [41] exhibit geometric distortions. Hybrid approaches combining diffusion with neural rendering show promise but require refinement. Additionally, generalization to niche domains (e.g., scientific imagery) is hindered by reliance on pretrained models [163]. Few-shot adaptation techniques [100] could alleviate this limitation if scaled effectively.  \n\n#### Human-AI Collaboration and Interpretability  \nThe opacity of diffusion models undermines user trust and control. Although [124] improves interpretability by disentangling item-prompt interactions, unpredictable model behavior persists. Interactive tools like [37] empower users but rely on heuristic guidance, which can yield unintended edits. Enhancing explainability—for instance, by visualizing cross-attention dynamics [97]—could bridge this gap and foster intuitive interfaces.  \n\n#### Legal and Societal Accountability  \nDiffusion models challenge existing legal frameworks. Copyright disputes arise when models train on unlicensed artworks [32], while tools for document manipulation [229] risk enabling fraud. Current licensing systems, such as Creative Commons, are ill-equipped to govern AI-generated content. Policymakers must define clear ownership and liability standards to address these gaps.  \n\n#### Toward Responsible Solutions  \nMitigating these challenges demands interdisciplinary collaboration. Computational efficiency could be improved via quantization-aware training and dynamic sparsity, while fairness-aware loss functions and dataset audits could reduce biases. Robustness may benefit from adversarial training tailored to diffusion dynamics. Ethically, tools like [166] should be embedded into model APIs by default, complemented by user education and transparent model documentation [95].  \n\nIn summary, the transformative potential of diffusion models is tempered by persistent technical and ethical hurdles. Addressing these issues through innovation, regulation, and education will be pivotal in harnessing their benefits while safeguarding societal interests.  \n---\n\n### 10.4 Call for Future Research\n\n---\nThe rapid advancements in diffusion model-based image editing have opened numerous avenues for future research, spanning technical improvements, ethical considerations, and interdisciplinary applications. Building upon the persistent challenges outlined in Section 10.3 and anticipating the broader implications discussed in the following section, we outline key directions for future work grounded in recent literature.\n\n### 1. **Efficiency and Real-Time Processing**\nWhile diffusion models have demonstrated remarkable capabilities, their computational intensity remains a bottleneck for real-time applications. Future research should prioritize optimization techniques that balance speed and quality, building on approaches like distillation [103] and adaptive sampling [104]. Innovations such as computation reuse across denoising steps [49] and hardware-aware quantization could significantly enhance practical deployment.\n\n### 2. **Multimodal and Cross-Modal Integration**\nExtending diffusion models beyond visual domains presents exciting opportunities. Unified frameworks that harmonize multiple modalities [131] could enable richer creative applications, while cross-modal editing techniques [7] might bridge gaps between text, audio, and visual manipulation.\n\n### 3. **Ethical and Responsible AI Practices**\nAddressing the ethical challenges identified in Section 10.3 requires proactive solutions. Future work should advance debiasing techniques [51] and deepfake detection methods [182], while fostering interdisciplinary collaboration to establish responsible deployment guidelines.\n\n### 4. **Long-Term Consistency and Temporal Coherence**\nFor video and 3D applications, maintaining temporal stability remains critical. Flow-guided approaches [167] show promise, but future solutions could incorporate hierarchical conditioning or physics-inspired constraints to ensure coherence across frames and dimensions.\n\n### 5. **Lightweight and Specialized Architectures**\nThe scalability challenge calls for efficient model designs. Studies revealing the effectiveness of smaller models [203] and specialized architectures [126] suggest that future research should explore modular designs and frequency-aware processing [170].\n\n### 6. **Theoretical Foundations and Generalization**\nDeepening theoretical understanding could unlock new capabilities. Work connecting diffusion models to statistical mechanics [108] and variational bounds [219] provides foundations for future theoretical advances, particularly in low-data regimes [202].\n\n### 7. **Interactive and User-Guided Editing**\nEnhancing user control remains crucial for practical applications. Future interfaces could build on spatial editing frameworks [20] while incorporating reinforcement learning [127] for more intuitive interaction paradigms.\n\n### 8. **Domain-Specific Applications**\nSpecialized domains present unique opportunities and challenges. Future work should address domain-specific needs, from medical imaging robustness [106] to creative applications requiring stylistic fidelity.\n\n### 9. **Interdisciplinary Collaboration**\nThe versatility of diffusion models calls for cross-domain innovation. Applications in computational biology [129] and network optimization [109] demonstrate the potential for broader interdisciplinary impact.\n\n### 10. **Evaluation Metrics**\nDeveloping more nuanced assessment frameworks is essential. Future metrics should address the limitations of current approaches [230] to better capture compositional understanding and temporal coherence.\n\nIn summary, the future of diffusion model-based image editing lies in addressing technical limitations while ensuring ethical responsibility and fostering interdisciplinary collaboration. By building on current challenges and opportunities, researchers can advance both the capabilities and societal benefits of these transformative models.\n---\n\n### 10.5 Final Remarks\n\nThe advent of diffusion models has ushered in a paradigm shift in artificial intelligence, particularly in image generation and editing. Unlike traditional generative models like GANs and VAEs, diffusion models leverage a unique probabilistic framework that iteratively refines noise into coherent images, offering unprecedented control, fidelity, and versatility. This transformative approach has redefined computational creativity, enabling applications ranging from photorealistic synthesis to fine-grained semantic editing. However, this shift extends beyond technical innovation—it carries profound implications for ethics, sustainability, and interdisciplinary collaboration, as highlighted by recent research on AI's societal impact [62; 172].\n\nA key strength of diffusion models lies in their ability to bridge stochasticity and controllability. By decomposing image generation into iterative denoising steps, these models enable precise manipulation of latent representations, facilitating tasks like text-guided editing, style transfer, and localized inpainting with remarkable accuracy. This aligns with broader AI trends toward interpretable and steerable systems, addressing concerns about the \"black box\" nature of deep learning [70; 71]. The iterative refinement process also mirrors human creative workflows, allowing intermediate outputs to be inspected and adjusted—a feature that fosters human-machine collaboration [231].\n\nDespite their promise, diffusion models introduce critical challenges. Their substantial computational demands raise concerns about environmental sustainability and accessibility [53; 65]. Training large-scale models requires significant energy resources, contributing to AI's carbon footprint—an issue exacerbated by the trend toward ever-larger architectures [52]. While techniques like distillation and sparse inference offer partial solutions, more work is needed to ensure these models are both sustainable and inclusive [55].\n\nEthical concerns also loom large. Diffusion models can amplify societal biases present in training data, potentially reinforcing stereotypes or marginalizing underrepresented groups in generated or edited images [56; 60]. Additionally, their ability to create highly realistic images heightens risks of misuse, such as deepfake generation [63]. These challenges underscore the need for robust ethical frameworks and regulatory oversight to align diffusion model development with societal values [232; 65].\n\nThe interdisciplinary nature of diffusion model research necessitates collaboration across fields. While computer scientists advance model performance, social scientists, ethicists, and policymakers must address broader implications [72; 114]. For instance, integrating these models into creative industries raises questions about authorship and copyright [233], while medical or autonomous applications demand rigorous safety validation [66; 54]. Cross-disciplinary dialogue is essential to navigate these complexities [195; 234].\n\nLooking ahead, diffusion models exemplify AI's potential to augment human creativity while demanding renewed focus on ethical and sustainable development [235; 236]. Their success will hinge not only on technical progress but also on establishing norms that prioritize inclusivity, transparency, and environmental responsibility [58; 198]. This holistic approach is vital for realizing their benefits while mitigating risks [237; 204].\n\nIn conclusion, diffusion models represent a watershed moment in AI, setting new standards for generative modeling through their iterative, probabilistic framework. Yet their paradigm shift demands equal innovation in addressing ethical, environmental, and societal challenges. By fostering interdisciplinary collaboration and prioritizing responsible development, the AI community can ensure these models empower individuals and societies while upholding fundamental values [57; 194]. The path forward is complex, but with thoughtful stewardship, diffusion models can help shape a future where technology and humanity thrive together.\n\n\n## References\n\n[1] Diffusion Models  A Comprehensive Survey of Methods and Applications\n\n[2] Ten Years of Generative Adversarial Nets (GANs)  A survey of the  state-of-the-art\n\n[3] A Bayesian Non-parametric Approach to Generative Models  Integrating  Variational Autoencoder and Generative Adversarial Networks using Wasserstein  and Maximum Mean Discrepancy\n\n[4] Diffusion Models Beat GANs on Image Classification\n\n[5] High-Resolution Image Synthesis with Latent Diffusion Models\n\n[6] Text-image Alignment for Diffusion-based Perception\n\n[7] Collaborative Diffusion for Multi-Modal Face Generation and Editing\n\n[8] Semantic Image Synthesis for Abdominal CT\n\n[9] 3D-LDM  Neural Implicit 3D Shape Generation with Latent Diffusion Models\n\n[10] An Overview of Diffusion Models  Applications, Guided Generation,  Statistical Rates and Optimization\n\n[11] Unifying Diffusion Models' Latent Space, with Applications to  CycleDiffusion and Guidance\n\n[12] Image Inpainting via Iteratively Decoupled Probabilistic Modeling\n\n[13] Progressive Distillation for Fast Sampling of Diffusion Models\n\n[14] DeepCache  Accelerating Diffusion Models for Free\n\n[15] Assessing the capacity of a denoising diffusion probabilistic model to  reproduce spatial context\n\n[16] ViT-DAE  Transformer-driven Diffusion Autoencoder for Histopathology  Image Analysis\n\n[17] HoloDiffusion  Training a 3D Diffusion Model using 2D Images\n\n[18] Control3Diff  Learning Controllable 3D Diffusion Models from Single-view  Images\n\n[19] Extracting Training Data from Diffusion Models\n\n[20] Editable Image Elements for Controllable Synthesis\n\n[21] High-Fidelity Diffusion-based Image Editing\n\n[22] BARET   Balanced Attention based Real image Editing driven by  Target-text Inversion\n\n[23] On the Importance of Noise Scheduling for Diffusion Models\n\n[24] Differential Diffusion  Giving Each Pixel Its Strength\n\n[25] An Edit Friendly DDPM Noise Space  Inversion and Manipulations\n\n[26] Understanding the Latent Space of Diffusion Models through the Lens of  Riemannian Geometry\n\n[27] Denoising Diffusion Bridge Models\n\n[28] Diffusion Models already have a Semantic Latent Space\n\n[29] Diffusion Models for Medical Image Analysis  A Comprehensive Survey\n\n[30] Imagen Editor and EditBench  Advancing and Evaluating Text-Guided Image  Inpainting\n\n[31] DiffEdit  Diffusion-based semantic image editing with mask guidance\n\n[32] Style Injection in Diffusion  A Training-free Approach for Adapting  Large-scale Diffusion Models for Style Transfer\n\n[33] ControlStyle  Text-Driven Stylized Image Generation Using Diffusion  Priors\n\n[34] ObjectDrop  Bootstrapping Counterfactuals for Photorealistic Object  Removal and Insertion\n\n[35] Move Anything with Layered Scene Diffusion\n\n[36] DragDiffusion  Harnessing Diffusion Models for Interactive Point-based  Image Editing\n\n[37] RotationDrag  Point-based Image Editing with Rotated Diffusion Features\n\n[38] Dreamix  Video Diffusion Models are General Video Editors\n\n[39] StableVideo  Text-driven Consistency-aware Diffusion Video Editing\n\n[40] Editing 3D Scenes via Text Prompts without Retraining\n\n[41] SIGNeRF  Scene Integrated Generation for Neural Radiance Fields\n\n[42] Cross-domain Compositing with Pretrained Diffusion Models\n\n[43] PAIR-Diffusion  A Comprehensive Multimodal Object-Level Image Editor\n\n[44] The Uncanny Valley  A Comprehensive Analysis of Diffusion Models\n\n[45] Text Diffusion with Reinforced Conditioning\n\n[46] The Emergence of Reproducibility and Consistency in Diffusion Models\n\n[47] Generating Behaviorally Diverse Policies with Latent Diffusion Models\n\n[48] Can Push-forward Generative Models Fit Multimodal Distributions \n\n[49] Clockwork Diffusion  Efficient Generation With Model-Step Distillation\n\n[50] CoDi  Conditional Diffusion Distillation for Higher-Fidelity and Faster  Image Generation\n\n[51] Mitigating Biases with Diverse Ensembles and Diffusion Models\n\n[52] Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research\n\n[53] On the Opportunities of Green Computing  A Survey\n\n[54] Social and environmental impact of recent developments in machine  learning on biology and chemistry research\n\n[55] A Survey on AI Sustainability  Emerging Trends on Learning Algorithms  and Research Challenges\n\n[56] Bias in Data-driven AI Systems -- An Introductory Survey\n\n[57] The Pursuit of Fairness in Artificial Intelligence Models  A Survey\n\n[58] FATE in AI  Towards Algorithmic Inclusivity and Accessibility\n\n[59] Tackling Bias in Pre-trained Language Models  Current Trends and  Under-represented Societies\n\n[60] Rethinking Fairness  An Interdisciplinary Survey of Critiques of  Hegemonic ML Fairness Approaches\n\n[61] Diversity in Sociotechnical Machine Learning Systems\n\n[62] Ethics of Artificial Intelligence Demarcations\n\n[63] Five ethical principles for generative AI in scientific research\n\n[64] Automating Ambiguity  Challenges and Pitfalls of Artificial Intelligence\n\n[65] Sustainable AI Regulation\n\n[66] Towards a Privacy and Security-Aware Framework for Ethical AI  Guiding  the Development and Assessment of AI Systems\n\n[67] No computation without representation  Avoiding data and algorithm  biases through diversity\n\n[68] Repeatability, Reproducibility, Replicability, Reusability (4R) in  Journals' Policies and Software Data Management in Scientific Publications  A  Survey, Discussion, and Perspectives\n\n[69] AI Competitions and Benchmarks  The life cycle of challenges and  benchmarks\n\n[70] Designing for Human Rights in AI\n\n[71] Achieving a Data-driven Risk Assessment Methodology for Ethical AI\n\n[72] Grand challenges and emergent modes of convergence science\n\n[73] Dive into Deep Learning\n\n[74] From Pixels to Insights  A Survey on Automatic Chart Understanding in  the Era of Large Foundation Models\n\n[75] Explainable Machine Learning with Prior Knowledge  An Overview\n\n[76] Chart Question Answering  State of the Art and Future Directions\n\n[77] Quality Control in Crowdsourcing  A Survey of Quality Attributes,  Assessment Techniques and Assurance Actions\n\n[78] Large Language Models(LLMs) on Tabular Data  Prediction, Generation, and  Understanding -- A Survey\n\n[79] Perspectives on the State and Future of Deep Learning - 2023\n\n[80] Generating a Structured Summary of Numerous Academic Papers  Dataset and  Method\n\n[81] StructChart  Perception, Structuring, Reasoning for Visual Chart  Understanding\n\n[82] The Theta Number of Simplicial Complexes\n\n[83] Improving Diffusion-Based Image Synthesis with Context Prediction\n\n[84] State of the Art on Diffusion Models for Visual Computing\n\n[85] Latent Diffusion Models for Structural Component Design\n\n[86] Generative AI in Vision  A Survey on Models, Metrics and Applications\n\n[87] Efficient Diffusion Models for Vision  A Survey\n\n[88] Resolution Chromatography of Diffusion Models\n\n[89] Dimensionality-Varying Diffusion Process\n\n[90] DiffuseGAE  Controllable and High-fidelity Image Manipulation from  Disentangled Representation\n\n[91] EDICT  Exact Diffusion Inversion via Coupled Transformations\n\n[92] Diffusion Models Generate Images Like Painters  an Analytical Theory of  Outline First, Details Later\n\n[93] Cache Me if You Can  Accelerating Diffusion Models through Block Caching\n\n[94] On the Robustness of Latent Diffusion Models\n\n[95] Diffusion Model-Based Image Editing  A Survey\n\n[96] DiffusionAtlas  High-Fidelity Consistent Diffusion Video Editing\n\n[97] Dynamic Prompt Learning  Addressing Cross-Attention Leakage for  Text-Based Image Editing\n\n[98] The Blessing of Randomness  SDE Beats ODE in General Diffusion-based  Image Editing\n\n[99] LoMOE  Localized Multi-Object Editing via Multi-Diffusion\n\n[100] SVDiff  Compact Parameter Space for Diffusion Fine-Tuning\n\n[101] Consistency Models\n\n[102] Theoretical Insights for Diffusion Guidance  A Case Study for Gaussian  Mixture Models\n\n[103] Improved Techniques for Training Consistency Models\n\n[104] How Much is Enough  A Study on Diffusion Times in Score-based Generative  Models\n\n[105] Soft Truncation  A Universal Training Technique of Score-based Diffusion  Model for High Precision Score Estimation\n\n[106] Diffusion-C  Unveiling the Generative Challenges of Diffusion Models  through Corrupted Data\n\n[107] Contractive Diffusion Probabilistic Models\n\n[108] The statistical thermodynamics of generative diffusion models  Phase  transitions, symmetry breaking and critical instability\n\n[109] Beyond Deep Reinforcement Learning  A Tutorial on Generative Diffusion  Models in Network Optimization\n\n[110] Efficient Machine Learning for Big Data  A Review\n\n[111] Sustainability in Computing Education  A Systematic Literature Review\n\n[112] A Survey on Sustainable Software Ecosystems to Support Experimental and  Observational Science at Oak Ridge National Laboratory\n\n[113] Grid Computing  The Next Decade -- Report and Summary\n\n[114] A multidomain relational framework to guide institutional AI research  and adoption\n\n[115] Diffusion Models as Masked Autoencoders\n\n[116] GANTASTIC  GAN-based Transfer of Interpretable Directions for  Disentangled Image Editing in Text-to-Image Diffusion Models\n\n[117] Local Statistics for Generative Image Detection\n\n[118] Boundary Guided Learning-Free Semantic Control with Diffusion Models\n\n[119] Image Embedding for Denoising Generative Models\n\n[120] Direct Inversion  Boosting Diffusion-based Editing with 3 Lines of Code\n\n[121] PFB-Diff  Progressive Feature Blending Diffusion for Text-driven Image  Editing\n\n[122] Representation Learning with Diffusion Models\n\n[123] Posterior Distillation Sampling\n\n[124] An Item is Worth a Prompt  Versatile Image Editing with Disentangled  Control\n\n[125] High-Resolution Image Editing via Multi-Stage Blended Diffusion\n\n[126] Multi-Architecture Multi-Expert Diffusion Models\n\n[127] Training Diffusion Models with Reinforcement Learning\n\n[128] OneActor  Consistent Character Generation via Cluster-Conditioned  Guidance\n\n[129] Diffusion Model is an Effective Planner and Data Synthesizer for  Multi-Task Reinforcement Learning\n\n[130] Unbiased Image Synthesis via Manifold Guidance in Diffusion Models\n\n[131] One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale\n\n[132] Human participants in AI research  Ethics and transparency in practice\n\n[133] Ethical Questions in NLP Research  The (Mis)-Use of Forensic Linguistics\n\n[134] Visualizing a Field of Research  A Methodology of Systematic  Scientometric Reviews\n\n[135] Dimensions of Commonsense Knowledge\n\n[136] Less is More  Learning Prominent and Diverse Topics for Data  Summarization\n\n[137] Democratic summary of public opinions in free-response surveys\n\n[138] SurveyAgent  A Conversational System for Personalized and Efficient  Research Survey\n\n[139] Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey\n\n[140] Contexts of diffusion  Adoption of research synthesis in Social Work and  Women's Studies\n\n[141] Score-based Diffusion Models in Function Space\n\n[142] Diffusion Models for Constrained Domains\n\n[143] Geometric Neural Diffusion Processes\n\n[144] Diffusion Schrödinger Bridge with Applications to Score-Based  Generative Modeling\n\n[145] Modeling Temporal Data as Continuous Functions with Stochastic Process  Diffusion\n\n[146] Denoising Diffusion Samplers\n\n[147] Diffusion with Forward Models  Solving Stochastic Inverse Problems  Without Direct Supervision\n\n[148] Single and Few-step Diffusion for Generative Speech Enhancement\n\n[149] Diffusion Models in Vision  A Survey\n\n[150] Blackout Diffusion  Generative Diffusion Models in Discrete-State Spaces\n\n[151] Plug-In Diffusion Model for Embedding Denoising in Recommendation System\n\n[152] Denoising Diffusion Restoration Tackles Forward and Inverse Problems for  the Laplace Operator\n\n[153] Interpretable Diffusion via Information Decomposition\n\n[154] DAG  Depth-Aware Guidance with Denoising Diffusion Probabilistic Models\n\n[155] Unleashing Text-to-Image Diffusion Models for Visual Perception\n\n[156] Magic-Boost  Boost 3D Generation with Mutli-View Conditioned Diffusion\n\n[157] Beware of diffusion models for synthesizing medical images -- A  comparison with GANs in terms of memorizing brain MRI and chest x-ray images\n\n[158] Towards the Detection of Diffusion Model Deepfakes\n\n[159] Dynamic Hierarchical Reactive Controller Synthesis\n\n[160] Multi-Modal Fusion by Meta-Initialization\n\n[161] MDP  A Generalized Framework for Text-Guided Image Editing by  Manipulating the Diffusion Path\n\n[162] Not All Steps are Created Equal  Selective Diffusion Distillation for  Image Manipulation\n\n[163] CreativeSynth  Creative Blending and Synthesis of Visual Arts based on  Multimodal Diffusion\n\n[164] Text-to-image Editing by Image Information Removal\n\n[165] Fairy  Fast Parallelized Instruction-Guided Video-to-Video Synthesis\n\n[166] EditShield  Protecting Unauthorized Image Editing by Instruction-guided  Diffusion Models\n\n[167] Upscale-A-Video  Temporal-Consistent Diffusion Model for Real-World  Video Super-Resolution\n\n[168] Pixel-wise RL on Diffusion Models  Reinforcement Learning from Rich  Feedback\n\n[169] CADS  Unleashing the Diversity of Diffusion Models through  Condition-Annealed Sampling\n\n[170] Boosting Diffusion Models with Moving Average Sampling in Frequency  Domain\n\n[171] Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts  in Underspecified Visual Tasks\n\n[172] Towards Trustworthy Artificial Intelligence for Equitable Global Health\n\n[173] Diffusion-SDF  Conditional Generative Modeling of Signed Distance  Functions\n\n[174] Explaining Deep Learning Models - A Bayesian Non-parametric Approach\n\n[175] Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and  Personalized Stylization\n\n[176] BIVDiff  A Training-Free Framework for General-Purpose Video Synthesis  via Bridging Image and Video Diffusion Models\n\n[177] SALAD  Part-Level Latent Diffusion for 3D Shape Generation and  Manipulation\n\n[178] Eta Inversion  Designing an Optimal Eta Function for Diffusion-based  Real Image Editing\n\n[179] Towards Real-time Text-driven Image Manipulation with Unconditional  Diffusion Models\n\n[180] Space-Time Bridge-Diffusion\n\n[181] An Analysis of the Variance of Diffusion-based Speech Enhancement\n\n[182] Memory Triggers  Unveiling Memorization in Text-To-Image Generative  Models through Word-Level Duplication\n\n[183] One-Step Diffusion Distillation via Deep Equilibrium Models\n\n[184] DiffuseVAE  Efficient, Controllable and High-Fidelity Generation from  Low-Dimensional Latents\n\n[185] Conditional Image Generation with Pretrained Generative Model\n\n[186] Improving Diffusion Model Efficiency Through Patching\n\n[187] Deep Equilibrium Approaches to Diffusion Models\n\n[188] Nested Diffusion Processes for Anytime Image Generation\n\n[189] DiffEditor  Boosting Accuracy and Flexibility on Diffusion-based Image  Editing\n\n[190] ByteEdit  Boost, Comply and Accelerate Generative Image Editing\n\n[191] Cold Diffusion  Inverting Arbitrary Image Transforms Without Noise\n\n[192] Training-free Diffusion Model Adaptation for Variable-Sized  Text-to-Image Synthesis\n\n[193] Unraveling the Temporal Dynamics of the Unet in Diffusion Models\n\n[194] Towards an Ethical Framework in the Complex Digital Era\n\n[195] Policy Scan and Technology Strategy Design methodology\n\n[196] REAL ML  Recognizing, Exploring, and Articulating Limitations of Machine  Learning Research\n\n[197] Machine Learning Practices Outside Big Tech  How Resource Constraints  Challenge Responsible Development\n\n[198] Towards decolonising computational sciences\n\n[199] Towards Faster Training of Diffusion Models  An Inspiration of A  Consistency Phenomenon\n\n[200] A Variational Perspective on Solving Inverse Problems with Diffusion  Models\n\n[201] NoiseCLR  A Contrastive Learning Approach for Unsupervised Discovery of  Interpretable Directions in Diffusion Models\n\n[202] On Memorization in Diffusion Models\n\n[203] Bigger is not Always Better  Scaling Properties of Latent Diffusion  Models\n\n[204] AI Ethics  A Bibliometric Analysis, Critical Issues, and Key Gaps\n\n[205] A Survey on Open Information Extraction from Rule-based Model to Large  Language Model (meta)\n\n[206] Survey Research in Software Engineering  Problems and Strategies\n\n[207] Software solutions for form-based collection of data and the semantic  enrichment of form data\n\n[208] Towards Reducing Manual Workload in Technology-Assisted Reviews   Estimating Ranking Performance\n\n[209] Conffusion  Confidence Intervals for Diffusion Models\n\n[210] Diffusion Models for Time Series Applications  A Survey\n\n[211] Generative Modelling With Inverse Heat Dissipation\n\n[212] Beyond Surface Statistics  Scene Representations in a Latent Diffusion  Model\n\n[213] Text-driven Visual Synthesis with Latent Diffusion Prior\n\n[214] LLMGA  Multimodal Large Language Model based Generation Assistant\n\n[215] SINE  SINgle Image Editing with Text-to-Image Diffusion Models\n\n[216] GeoDiffuser  Geometry-Based Image Editing with Diffusion Models\n\n[217] Custom-Edit  Text-Guided Image Editing with Customized Diffusion Models\n\n[218] Latent Diffusion for Language Generation\n\n[219] Understanding Diffusion Objectives as the ELBO with Simple Data  Augmentation\n\n[220] One-Line-of-Code Data Mollification Improves Optimization of  Likelihood-based Generative Models\n\n[221] UFOGen  You Forward Once Large Scale Text-to-Image Generation via  Diffusion GANs\n\n[222] DiffMorpher  Unleashing the Capability of Diffusion Models for Image  Morphing\n\n[223] Look ATME  The Discriminator Mean Entropy Needs Attention\n\n[224] Smooth Diffusion  Crafting Smooth Latent Spaces in Diffusion Models\n\n[225] Iterative Multi-granular Image Editing using Diffusion Models\n\n[226] Gaussian Mixture Solvers for Diffusion Models\n\n[227] Solving Inverse Problems with Latent Diffusion Models via Hard Data  Consistency\n\n[228] Prompt-tuning latent diffusion models for inverse problems\n\n[229] On Manipulating Scene Text in the Wild with Diffusion Models\n\n[230] Comparing Human and Automated Evaluation of Open-Ended Student Responses  to Questions of Evolution\n\n[231] Theoretical And Technological Building Blocks For An Innovation  Accelerator\n\n[232] Bridging the Gap  the case for an Incompletely Theorized Agreement on AI  policy\n\n[233] Digitising Cultural Complexity  Representing Rich Cultural Data in a Big  Data environment\n\n[234] Children and the Data Cycle  Rights and Ethics in a Big Data World\n\n[235] Metaethical Perspectives on 'Benchmarking' AI Ethics\n\n[236] Computability, Complexity, Consistency and Controllability  A Four C's  Framework for cross-disciplinary Ethical Algorithm Research\n\n[237] Systematic AI Approach for AGI  Addressing Alignment, Energy, and AGI  Grand Challenges\n\n\n",
    "reference": {
        "1": "2209.00796v12",
        "2": "2308.16316v1",
        "3": "2308.14048v1",
        "4": "2307.08702v1",
        "5": "2112.10752v2",
        "6": "2310.00031v3",
        "7": "2304.10530v1",
        "8": "2312.06453v1",
        "9": "2212.00842v2",
        "10": "2404.07771v1",
        "11": "2210.05559v2",
        "12": "2212.02963v2",
        "13": "2202.00512v2",
        "14": "2312.00858v2",
        "15": "2309.10817v1",
        "16": "2304.01053v1",
        "17": "2303.16509v2",
        "18": "2304.06700v2",
        "19": "2301.13188v1",
        "20": "2404.16029v1",
        "21": "2312.15707v3",
        "22": "2312.05482v1",
        "23": "2301.10972v4",
        "24": "2306.00950v2",
        "25": "2304.06140v3",
        "26": "2307.12868v2",
        "27": "2309.16948v3",
        "28": "2210.10960v2",
        "29": "2211.07804v3",
        "30": "2212.06909v2",
        "31": "2210.11427v1",
        "32": "2312.09008v2",
        "33": "2311.05463v1",
        "34": "2403.18818v1",
        "35": "2404.07178v1",
        "36": "2306.14435v6",
        "37": "2401.06442v1",
        "38": "2302.01329v1",
        "39": "2308.09592v1",
        "40": "2309.04917v3",
        "41": "2401.01647v2",
        "42": "2302.10167v2",
        "43": "2303.17546v3",
        "44": "2402.13369v1",
        "45": "2402.14843v1",
        "46": "2310.05264v3",
        "47": "2305.18738v2",
        "48": "2206.14476v2",
        "49": "2312.08128v2",
        "50": "2310.01407v2",
        "51": "2311.16176v3",
        "52": "2306.16900v2",
        "53": "2311.00447v3",
        "54": "2210.00356v1",
        "55": "2205.03824v1",
        "56": "2001.09762v1",
        "57": "2403.17333v1",
        "58": "2301.01590v2",
        "59": "2312.01509v1",
        "60": "2205.04460v1",
        "61": "2107.09163v1",
        "62": "1904.10239v2",
        "63": "2401.15284v2",
        "64": "2206.04179v1",
        "65": "2306.00292v4",
        "66": "2403.08624v1",
        "67": "2002.11836v1",
        "68": "2312.11028v1",
        "69": "2312.05296v1",
        "70": "2005.04949v2",
        "71": "2112.01282v1",
        "72": "2103.11547v1",
        "73": "2106.11342v5",
        "74": "2403.12027v2",
        "75": "2105.10172v1",
        "76": "2205.03966v2",
        "77": "1801.02546v1",
        "78": "2402.17944v2",
        "79": "2312.09323v3",
        "80": "2302.04580v1",
        "81": "2309.11268v4",
        "82": "1704.01836v1",
        "83": "2401.02015v1",
        "84": "2310.07204v1",
        "85": "2309.11601v2",
        "86": "2402.16369v1",
        "87": "2210.09292v3",
        "88": "2401.10247v1",
        "89": "2211.16032v1",
        "90": "2307.05899v1",
        "91": "2211.12446v2",
        "92": "2303.02490v2",
        "93": "2312.03209v2",
        "94": "2306.08257v1",
        "95": "2402.17525v2",
        "96": "2312.03772v1",
        "97": "2309.15664v1",
        "98": "2311.01410v2",
        "99": "2403.00437v1",
        "100": "2303.11305v4",
        "101": "2303.01469v2",
        "102": "2403.01639v1",
        "103": "2310.14189v1",
        "104": "2206.05173v1",
        "105": "2106.05527v5",
        "106": "2312.08843v1",
        "107": "2401.13115v1",
        "108": "2310.17467v2",
        "109": "2308.05384v1",
        "110": "1503.05296v1",
        "111": "2305.10369v1",
        "112": "2204.05896v1",
        "113": "1210.3271v1",
        "114": "2303.10106v2",
        "115": "2304.03283v1",
        "116": "2403.19645v1",
        "117": "2310.16684v1",
        "118": "2302.08357v3",
        "119": "2301.07485v1",
        "120": "2310.01506v2",
        "121": "2306.16894v1",
        "122": "2210.11058v1",
        "123": "2311.13831v3",
        "124": "2403.04880v1",
        "125": "2210.12965v1",
        "126": "2306.04990v2",
        "127": "2305.13301v4",
        "128": "2404.10267v1",
        "129": "2305.18459v2",
        "130": "2307.08199v3",
        "131": "2303.06555v2",
        "132": "2311.01254v2",
        "133": "1712.07512v1",
        "134": "1906.04800v1",
        "135": "2101.04640v2",
        "136": "1611.09921v2",
        "137": "1907.04359v3",
        "138": "2404.06364v1",
        "139": "2402.04854v2",
        "140": "1401.7239v1",
        "141": "2302.07400v2",
        "142": "2304.05364v2",
        "143": "2307.05431v1",
        "144": "2106.01357v5",
        "145": "2211.02590v2",
        "146": "2302.13834v2",
        "147": "2306.11719v2",
        "148": "2309.09677v2",
        "149": "2209.04747v5",
        "150": "2305.11089v1",
        "151": "2401.06982v3",
        "152": "2402.08563v2",
        "153": "2310.07972v2",
        "154": "2212.08861v2",
        "155": "2303.02153v1",
        "156": "2404.06429v1",
        "157": "2305.07644v2",
        "158": "2210.14571v4",
        "159": "1510.07246v2",
        "160": "2210.04843v1",
        "161": "2303.16765v2",
        "162": "2307.08448v1",
        "163": "2401.14066v2",
        "164": "2305.17489v2",
        "165": "2312.13834v1",
        "166": "2311.12066v1",
        "167": "2312.06640v1",
        "168": "2404.04356v1",
        "169": "2310.17347v3",
        "170": "2403.17870v1",
        "171": "2310.02230v5",
        "172": "2309.05088v1",
        "173": "2211.13757v2",
        "174": "1811.03422v1",
        "175": "2308.14469v3",
        "176": "2312.02813v2",
        "177": "2303.12236v2",
        "178": "2403.09468v1",
        "179": "2304.04344v1",
        "180": "2402.08847v1",
        "181": "2402.00811v1",
        "182": "2312.03692v1",
        "183": "2401.08639v1",
        "184": "2201.00308v3",
        "185": "2312.13253v1",
        "186": "2207.04316v1",
        "187": "2210.12867v1",
        "188": "2305.19066v3",
        "189": "2402.02583v1",
        "190": "2404.04860v1",
        "191": "2208.09392v1",
        "192": "2306.08645v2",
        "193": "2312.14965v1",
        "194": "2010.10028v2",
        "195": "1806.03235v1",
        "196": "2205.08363v1",
        "197": "2110.02932v1",
        "198": "2009.14258v1",
        "199": "2404.07946v1",
        "200": "2305.04391v2",
        "201": "2312.05390v1",
        "202": "2310.02664v1",
        "203": "2404.01367v1",
        "204": "2403.14681v1",
        "205": "2208.08690v4",
        "206": "1704.01090v1",
        "207": "1901.11053v1",
        "208": "2201.05648v1",
        "209": "2211.09795v1",
        "210": "2305.00624v1",
        "211": "2206.13397v7",
        "212": "2306.05720v2",
        "213": "2302.08510v2",
        "214": "2311.16500v3",
        "215": "2212.04489v1",
        "216": "2404.14403v1",
        "217": "2305.15779v1",
        "218": "2212.09462v2",
        "219": "2303.00848v7",
        "220": "2305.18900v2",
        "221": "2311.09257v5",
        "222": "2312.07409v1",
        "223": "2304.09024v1",
        "224": "2312.04410v1",
        "225": "2309.00613v2",
        "226": "2311.00941v1",
        "227": "2307.08123v3",
        "228": "2310.01110v1",
        "229": "2311.00734v2",
        "230": "1603.07029v1",
        "231": "1210.1480v1",
        "232": "2101.06110v1",
        "233": "1711.04452v1",
        "234": "1710.06881v1",
        "235": "2204.05151v1",
        "236": "2102.04234v1",
        "237": "2310.15274v1"
    },
    "retrieveref": {
        "1": "2402.02583v1",
        "2": "2403.04880v1",
        "3": "2404.16029v1",
        "4": "2306.16894v1",
        "5": "2312.09256v1",
        "6": "2306.00950v2",
        "7": "2312.15707v3",
        "8": "2304.04344v1",
        "9": "2305.10825v3",
        "10": "2309.04917v3",
        "11": "2403.09468v1",
        "12": "2212.08698v1",
        "13": "2402.17525v2",
        "14": "2305.17489v2",
        "15": "2312.05482v1",
        "16": "2206.02779v2",
        "17": "2302.02394v3",
        "18": "2311.13831v3",
        "19": "2305.05947v1",
        "20": "2311.01410v2",
        "21": "2309.04372v2",
        "22": "2311.00734v2",
        "23": "2403.00437v1",
        "24": "2310.01506v2",
        "25": "2210.12965v1",
        "26": "2302.11797v1",
        "27": "2212.02024v3",
        "28": "2401.06127v1",
        "29": "2403.12585v1",
        "30": "2210.11427v1",
        "31": "2310.02712v2",
        "32": "2403.19645v1",
        "33": "2312.08128v2",
        "34": "2403.13807v1",
        "35": "2308.00135v3",
        "36": "2311.12066v1",
        "37": "2312.05390v1",
        "38": "2403.10911v2",
        "39": "2303.12688v1",
        "40": "2303.11305v4",
        "41": "2305.17423v3",
        "42": "2403.11105v1",
        "43": "2312.11396v2",
        "44": "2111.14818v2",
        "45": "2306.14435v6",
        "46": "2401.01647v2",
        "47": "2303.17546v3",
        "48": "2401.06442v1",
        "49": "2312.13834v1",
        "50": "2312.03772v1",
        "51": "2305.15779v1",
        "52": "2404.11120v1",
        "53": "2312.12468v2",
        "54": "2312.06680v1",
        "55": "2308.15854v2",
        "56": "2307.08448v1",
        "57": "2312.08563v2",
        "58": "2302.01329v1",
        "59": "2302.10167v2",
        "60": "2305.16807v1",
        "61": "2307.02421v2",
        "62": "2403.18818v1",
        "63": "2309.15664v1",
        "64": "2403.03431v1",
        "65": "2404.07178v1",
        "66": "2310.02426v1",
        "67": "2312.04965v1",
        "68": "2310.10647v1",
        "69": "2302.08357v3",
        "70": "2311.16711v1",
        "71": "2306.08707v4",
        "72": "2309.16948v3",
        "73": "2212.04489v1",
        "74": "2303.10735v4",
        "75": "2312.10065v1",
        "76": "2404.14403v1",
        "77": "2303.15403v2",
        "78": "2401.05735v1",
        "79": "2304.07090v1",
        "80": "2310.10624v2",
        "81": "2302.03027v1",
        "82": "2403.13551v1",
        "83": "2306.02717v1",
        "84": "2305.04441v1",
        "85": "2304.10530v1",
        "86": "2310.19540v2",
        "87": "2304.05568v1",
        "88": "2211.09800v2",
        "89": "2306.13078v1",
        "90": "2303.08084v2",
        "91": "2305.18676v1",
        "92": "2403.06269v1",
        "93": "2311.13713v2",
        "94": "2312.02190v2",
        "95": "2403.11503v1",
        "96": "2305.14742v2",
        "97": "2308.14469v3",
        "98": "2211.12446v2",
        "99": "2404.11895v1",
        "100": "2210.09292v3",
        "101": "2311.05463v1",
        "102": "2309.11321v1",
        "103": "2309.00613v2",
        "104": "2303.12048v3",
        "105": "2308.08947v1",
        "106": "2307.00522v1",
        "107": "2305.18047v1",
        "108": "2305.04651v1",
        "109": "2304.06140v3",
        "110": "2310.01407v2",
        "111": "2211.02048v4",
        "112": "2210.05559v2",
        "113": "2404.12382v1",
        "114": "2310.16400v1",
        "115": "2312.02813v2",
        "116": "2403.14602v1",
        "117": "2403.05018v1",
        "118": "2312.13663v1",
        "119": "2303.12789v2",
        "120": "2302.08510v2",
        "121": "2207.04316v1",
        "122": "2309.10556v2",
        "123": "2403.08255v1",
        "124": "2403.12002v1",
        "125": "2312.08882v2",
        "126": "2211.13227v1",
        "127": "2312.07409v1",
        "128": "2308.09388v1",
        "129": "2108.01073v2",
        "130": "2212.06909v2",
        "131": "2403.12015v1",
        "132": "2307.10373v3",
        "133": "2305.12716v2",
        "134": "2303.09535v3",
        "135": "2312.06739v1",
        "136": "2312.04524v1",
        "137": "2304.03174v3",
        "138": "2302.06588v1",
        "139": "2403.12658v1",
        "140": "2211.14108v3",
        "141": "2308.09592v1",
        "142": "2212.03221v1",
        "143": "2404.12154v1",
        "144": "2307.12868v2",
        "145": "2312.16794v2",
        "146": "2303.12236v2",
        "147": "2306.04396v1",
        "148": "2401.11708v2",
        "149": "2401.03349v1",
        "150": "2403.15943v1",
        "151": "2404.04526v1",
        "152": "2403.16111v1",
        "153": "2303.15649v2",
        "154": "2310.13730v1",
        "155": "2312.08019v2",
        "156": "2306.08103v4",
        "157": "2311.14542v1",
        "158": "2401.03433v1",
        "159": "2303.07909v2",
        "160": "2403.12743v1",
        "161": "2112.10741v3",
        "162": "2307.05899v1",
        "163": "2306.10441v1",
        "164": "2403.12510v1",
        "165": "2304.04971v2",
        "166": "2312.12865v3",
        "167": "2303.01469v2",
        "168": "2312.10656v2",
        "169": "2306.08257v1",
        "170": "2303.16765v2",
        "171": "2305.19066v3",
        "172": "2403.07319v1",
        "173": "2308.06057v1",
        "174": "2303.09618v2",
        "175": "2203.12849v2",
        "176": "2311.16090v1",
        "177": "2309.04907v1",
        "178": "2304.02963v2",
        "179": "2403.11415v1",
        "180": "2401.03221v1",
        "181": "2303.15288v1",
        "182": "2309.16608v1",
        "183": "2309.16496v3",
        "184": "2404.05519v1",
        "185": "2306.17141v1",
        "186": "2312.14216v1",
        "187": "2310.16684v1",
        "188": "2303.10137v2",
        "189": "2212.02802v2",
        "190": "2210.04955v1",
        "191": "2306.00306v3",
        "192": "2403.00644v3",
        "193": "2304.03869v1",
        "194": "2310.12868v1",
        "195": "2401.02015v1",
        "196": "2112.10752v2",
        "197": "2312.15516v3",
        "198": "2311.03830v2",
        "199": "2403.11568v1",
        "200": "2312.04410v1",
        "201": "2307.14331v1",
        "202": "2404.01050v1",
        "203": "2403.06054v4",
        "204": "2312.06193v1",
        "205": "2401.13795v1",
        "206": "2304.02234v2",
        "207": "2306.07596v1",
        "208": "2312.03209v2",
        "209": "2303.17599v3",
        "210": "2404.04860v1",
        "211": "2310.06311v1",
        "212": "2305.03382v2",
        "213": "2305.13301v4",
        "214": "2312.00858v2",
        "215": "2312.16145v2",
        "216": "2404.16069v1",
        "217": "2404.07389v1",
        "218": "2304.12526v2",
        "219": "2403.11929v1",
        "220": "2310.00031v3",
        "221": "2311.02826v2",
        "222": "2402.12974v2",
        "223": "2404.12541v1",
        "224": "2306.00980v3",
        "225": "2312.12807v1",
        "226": "2306.08645v2",
        "227": "2309.14934v1",
        "228": "2309.03350v1",
        "229": "2305.18264v1",
        "230": "2209.00796v12",
        "231": "2312.06899v1",
        "232": "2311.03054v5",
        "233": "2312.04370v1",
        "234": "2302.12469v1",
        "235": "2210.05147v1",
        "236": "2404.01089v1",
        "237": "2402.18078v2",
        "238": "2304.11829v2",
        "239": "2205.11880v1",
        "240": "2312.11595v1",
        "241": "2304.14006v1",
        "242": "2312.14611v1",
        "243": "2401.00736v2",
        "244": "2311.10162v2",
        "245": "2312.02548v2",
        "246": "2108.02938v2",
        "247": "2307.03992v4",
        "248": "2312.16486v2",
        "249": "2210.11058v1",
        "250": "2205.01668v1",
        "251": "2309.10817v1",
        "252": "2304.08291v1",
        "253": "2312.16204v1",
        "254": "2402.16907v1",
        "255": "2311.17042v1",
        "256": "2310.07204v1",
        "257": "2001.02890v1",
        "258": "2312.12635v3",
        "259": "2312.08873v1",
        "260": "2111.05826v2",
        "261": "2404.07206v1",
        "262": "2401.16764v1",
        "263": "2210.12867v1",
        "264": "2308.01316v1",
        "265": "2403.07214v2",
        "266": "2311.12908v1",
        "267": "2312.03692v1",
        "268": "2311.12092v2",
        "269": "2403.11868v3",
        "270": "2401.08741v1",
        "271": "2311.16500v3",
        "272": "2312.08768v2",
        "273": "2304.08870v2",
        "274": "2209.00349v2",
        "275": "2301.07969v1",
        "276": "2308.10648v1",
        "277": "2308.02874v1",
        "278": "2210.05872v1",
        "279": "2311.16052v1",
        "280": "2401.10061v1",
        "281": "2303.11073v1",
        "282": "2403.16627v2",
        "283": "2305.15347v2",
        "284": "2305.13128v1",
        "285": "2305.12966v4",
        "286": "2312.11392v1",
        "287": "2211.16582v3",
        "288": "2311.17461v1",
        "289": "2403.18035v2",
        "290": "2307.12493v4",
        "291": "2403.18978v1",
        "292": "2211.07804v3",
        "293": "2306.05668v2",
        "294": "2310.07972v2",
        "295": "2312.10835v4",
        "296": "2312.12540v1",
        "297": "2312.12490v1",
        "298": "2404.10763v1",
        "299": "2206.13397v7",
        "300": "2304.08465v1",
        "301": "2304.00830v2",
        "302": "2310.13165v2",
        "303": "2404.15081v1",
        "304": "2403.17870v1",
        "305": "2311.09822v1",
        "306": "2202.00512v2",
        "307": "2403.17664v1",
        "308": "2210.02249v1",
        "309": "2404.04465v1",
        "310": "2312.06354v1",
        "311": "2401.08815v1",
        "312": "2308.06027v2",
        "313": "2402.16627v2",
        "314": "2401.02913v1",
        "315": "2402.04625v1",
        "316": "2305.03980v1",
        "317": "2310.06313v3",
        "318": "2312.08895v1",
        "319": "2312.02696v2",
        "320": "2305.18729v3",
        "321": "2310.19248v1",
        "322": "2305.13819v2",
        "323": "2401.05293v1",
        "324": "2309.06380v2",
        "325": "2211.09794v1",
        "326": "2305.16225v3",
        "327": "2311.13127v4",
        "328": "2107.03006v3",
        "329": "2404.06429v1",
        "330": "2404.00879v1",
        "331": "2401.01008v1",
        "332": "2312.05239v3",
        "333": "2305.15798v3",
        "334": "2312.02189v1",
        "335": "2312.04884v1",
        "336": "2311.09753v1",
        "337": "2403.17001v1",
        "338": "2302.03011v1",
        "339": "2305.17431v1",
        "340": "2304.04774v1",
        "341": "2309.06135v1",
        "342": "2211.12039v2",
        "343": "2310.06389v2",
        "344": "2308.05976v1",
        "345": "2209.04747v5",
        "346": "2402.10821v1",
        "347": "2301.01206v1",
        "348": "2312.11994v2",
        "349": "2401.12244v1",
        "350": "2309.14872v4",
        "351": "2311.06792v2",
        "352": "2212.03860v3",
        "353": "2310.19145v1",
        "354": "2403.04437v1",
        "355": "2312.02087v2",
        "356": "2307.08199v3",
        "357": "2305.18286v1",
        "358": "2303.17604v1",
        "359": "2305.10431v2",
        "360": "2305.15357v5",
        "361": "2311.17901v1",
        "362": "2306.12422v1",
        "363": "2403.07711v3",
        "364": "2308.15692v1",
        "365": "2312.06708v1",
        "366": "2311.16037v1",
        "367": "2303.00354v1",
        "368": "2305.06402v1",
        "369": "2311.14920v2",
        "370": "2312.15490v1",
        "371": "2402.05608v3",
        "372": "2310.10012v3",
        "373": "2303.16187v2",
        "374": "2402.17376v1",
        "375": "2305.12502v1",
        "376": "2211.13220v2",
        "377": "2303.05456v2",
        "378": "2211.16152v2",
        "379": "2308.09279v1",
        "380": "2303.10610v3",
        "381": "2306.01902v1",
        "382": "2309.14709v3",
        "383": "2311.16567v1",
        "384": "2402.14792v1",
        "385": "2305.16397v3",
        "386": "2307.00773v3",
        "387": "2312.12649v1",
        "388": "2302.09378v1",
        "389": "2305.01115v2",
        "390": "2403.14279v1",
        "391": "2108.08827v1",
        "392": "2403.19773v2",
        "393": "2403.12036v1",
        "394": "2211.01324v5",
        "395": "2308.14761v1",
        "396": "2304.07087v1",
        "397": "2210.16886v1",
        "398": "2305.03509v2",
        "399": "2306.04990v2",
        "400": "2311.11600v2",
        "401": "2210.14896v4",
        "402": "2307.00781v1",
        "403": "2404.03145v1",
        "404": "2309.01575v1",
        "405": "2306.00501v1",
        "406": "2404.04478v1",
        "407": "2309.06169v2",
        "408": "2404.12908v1",
        "409": "2308.06342v2",
        "410": "2308.16355v3",
        "411": "2403.13352v3",
        "412": "2403.16954v1",
        "413": "2312.03996v3",
        "414": "2207.11192v2",
        "415": "2402.11274v1",
        "416": "2402.14167v1",
        "417": "2306.02903v1",
        "418": "2307.02770v2",
        "419": "2307.06272v1",
        "420": "2210.10960v2",
        "421": "2210.00939v6",
        "422": "2112.05149v2",
        "423": "2310.02906v1",
        "424": "2401.09794v1",
        "425": "2305.15759v4",
        "426": "2210.03142v3",
        "427": "2312.08886v2",
        "428": "2305.11520v5",
        "429": "2306.04607v8",
        "430": "2402.16305v1",
        "431": "2208.14125v3",
        "432": "2309.05534v1",
        "433": "2211.09869v4",
        "434": "2403.11423v1",
        "435": "2304.04269v1",
        "436": "2305.15583v7",
        "437": "2210.15257v2",
        "438": "2312.02201v1",
        "439": "2311.17609v1",
        "440": "2401.01456v1",
        "441": "2302.02373v3",
        "442": "2207.14288v1",
        "443": "2211.10437v3",
        "444": "2306.14408v2",
        "445": "2308.02154v1",
        "446": "2306.00547v2",
        "447": "2305.15399v2",
        "448": "2306.01923v2",
        "449": "2309.16421v2",
        "450": "2404.11925v1",
        "451": "2306.17154v1",
        "452": "2302.02591v3",
        "453": "2309.04965v2",
        "454": "2306.00783v2",
        "455": "2402.16369v1",
        "456": "2402.00864v1",
        "457": "2401.10219v1",
        "458": "2307.10829v6",
        "459": "2402.08601v2",
        "460": "2401.06291v1",
        "461": "2308.11941v1",
        "462": "2307.05977v1",
        "463": "2211.01095v2",
        "464": "2311.16488v1",
        "465": "2303.04761v1",
        "466": "2310.08785v1",
        "467": "2306.06991v2",
        "468": "2307.12560v1",
        "469": "2404.13491v1",
        "470": "2403.05438v1",
        "471": "2401.02473v1",
        "472": "2212.05032v3",
        "473": "2303.02153v1",
        "474": "2403.14291v1",
        "475": "2309.00908v1",
        "476": "2401.02677v1",
        "477": "2310.15111v1",
        "478": "2403.11162v1",
        "479": "2402.14780v1",
        "480": "2212.12990v3",
        "481": "2311.00941v1",
        "482": "2211.12500v2",
        "483": "2311.11469v1",
        "484": "2311.12070v1",
        "485": "2305.04391v2",
        "486": "2302.02285v2",
        "487": "2208.01626v1",
        "488": "2402.13490v1",
        "489": "2208.13753v2",
        "490": "2404.04356v1",
        "491": "2308.09889v1",
        "492": "2307.04787v1",
        "493": "2102.01187v3",
        "494": "2402.04929v1",
        "495": "2306.10012v2",
        "496": "2310.10338v1",
        "497": "2310.05922v3",
        "498": "2209.14988v1",
        "499": "2403.03206v1",
        "500": "2110.02711v6",
        "501": "2205.03859v1",
        "502": "2309.11525v3",
        "503": "2403.08733v3",
        "504": "2404.03620v1",
        "505": "2212.04473v2",
        "506": "2404.07771v1",
        "507": "2307.12348v3",
        "508": "2308.11408v3",
        "509": "2312.05849v2",
        "510": "2202.07477v2",
        "511": "2312.02139v2",
        "512": "2310.04561v1",
        "513": "2308.06725v2",
        "514": "2303.07945v4",
        "515": "2403.16990v1",
        "516": "2310.03270v4",
        "517": "2307.08123v3",
        "518": "2303.14081v1",
        "519": "2306.08247v6",
        "520": "2211.17106v1",
        "521": "2312.09313v3",
        "522": "2404.10267v1",
        "523": "2302.04867v4",
        "524": "2402.02182v1",
        "525": "2305.13873v2",
        "526": "2302.04304v3",
        "527": "2305.16965v1",
        "528": "2404.04562v2",
        "529": "2403.11157v1",
        "530": "2308.01948v1",
        "531": "2209.14593v1",
        "532": "2211.06757v3",
        "533": "2306.14685v4",
        "534": "2212.06135v1",
        "535": "2312.03047v1",
        "536": "2305.13655v3",
        "537": "2312.17161v1",
        "538": "2303.05031v1",
        "539": "2301.11798v2",
        "540": "2305.04457v1",
        "541": "2308.08367v1",
        "542": "2402.05803v1",
        "543": "2403.12915v1",
        "544": "2309.08895v1",
        "545": "2210.09549v1",
        "546": "2307.11410v1",
        "547": "2312.09069v2",
        "548": "2212.05973v2",
        "549": "2310.08872v5",
        "550": "2404.05666v1",
        "551": "2403.05135v1",
        "552": "2211.12445v1",
        "553": "2307.11118v1",
        "554": "2306.14891v2",
        "555": "2403.09055v2",
        "556": "2306.03436v2",
        "557": "2309.01700v2",
        "558": "2303.10073v2",
        "559": "2404.12333v1",
        "560": "2303.07345v3",
        "561": "2303.09541v2",
        "562": "2210.16031v3",
        "563": "2305.10657v4",
        "564": "2308.07316v1",
        "565": "2403.05121v1",
        "566": "2211.03364v7",
        "567": "2403.00452v1",
        "568": "2211.13757v2",
        "569": "2401.02804v2",
        "570": "2401.04136v1",
        "571": "2305.09454v1",
        "572": "2305.16322v3",
        "573": "2308.06721v1",
        "574": "2312.10299v1",
        "575": "2402.00769v1",
        "576": "2306.11363v4",
        "577": "2403.17217v1",
        "578": "2403.13652v1",
        "579": "2312.03584v1",
        "580": "2312.02936v1",
        "581": "2210.07677v1",
        "582": "2301.09430v3",
        "583": "2403.15059v1",
        "584": "2205.15463v1",
        "585": "2311.11207v1",
        "586": "2312.07360v2",
        "587": "2404.08799v1",
        "588": "2303.06040v3",
        "589": "2301.13188v1",
        "590": "2211.06146v2",
        "591": "2311.07421v1",
        "592": "2312.03517v2",
        "593": "2404.04956v1",
        "594": "2404.04037v1",
        "595": "2404.05595v1",
        "596": "2309.17074v1",
        "597": "2403.01505v2",
        "598": "2310.13157v1",
        "599": "2312.06947v1",
        "600": "2403.01108v1",
        "601": "2312.10825v1",
        "602": "2206.00386v1",
        "603": "2305.12954v1",
        "604": "2304.02742v3",
        "605": "2312.04429v1",
        "606": "2403.04279v1",
        "607": "2211.09795v1",
        "608": "2303.17189v2",
        "609": "2304.09383v1",
        "610": "2207.13038v1",
        "611": "2210.05475v1",
        "612": "2403.19164v1",
        "613": "2303.12733v1",
        "614": "2303.09472v3",
        "615": "2404.13263v1",
        "616": "2312.01985v1",
        "617": "2302.00670v2",
        "618": "2312.14985v2",
        "619": "2310.00096v1",
        "620": "2312.08887v3",
        "621": "2301.05489v3",
        "622": "2305.16317v3",
        "623": "2312.00852v1",
        "624": "2309.10438v2",
        "625": "2103.04023v4",
        "626": "2312.15004v1",
        "627": "2312.06453v1",
        "628": "2303.06682v2",
        "629": "2305.18007v3",
        "630": "2301.04474v3",
        "631": "2308.06101v1",
        "632": "2305.14677v2",
        "633": "2311.14900v1",
        "634": "2211.13449v3",
        "635": "2401.07087v1",
        "636": "2307.10816v4",
        "637": "2306.16827v1",
        "638": "2309.09614v1",
        "639": "2209.14734v4",
        "640": "2304.01814v2",
        "641": "2306.05544v1",
        "642": "2304.06700v2",
        "643": "2305.00624v1",
        "644": "2401.14066v2",
        "645": "2306.03881v2",
        "646": "2112.05744v4",
        "647": "2306.14153v4",
        "648": "2305.10028v1",
        "649": "2311.16491v1",
        "650": "2212.06458v3",
        "651": "2311.12832v2",
        "652": "2403.05154v1",
        "653": "2310.03337v4",
        "654": "2304.14573v1",
        "655": "2303.04248v1",
        "656": "2312.00375v1",
        "657": "2310.10480v1",
        "658": "1810.05786v1",
        "659": "2312.09008v2",
        "660": "2402.03290v1",
        "661": "2307.11926v1",
        "662": "2310.13268v3",
        "663": "2404.14240v1",
        "664": "2404.10603v1",
        "665": "2207.08208v3",
        "666": "2402.12908v1",
        "667": "2312.04875v3",
        "668": "2404.03653v1",
        "669": "2404.05607v1",
        "670": "2403.12032v2",
        "671": "2403.17924v2",
        "672": "2403.06741v1",
        "673": "2211.00611v5",
        "674": "2112.03126v3",
        "675": "2402.14314v1",
        "676": "2312.01409v1",
        "677": "2306.13720v8",
        "678": "2312.09193v1",
        "679": "2305.15086v3",
        "680": "2210.04559v1",
        "681": "2310.11868v2",
        "682": "1704.04997v1",
        "683": "2305.10722v3",
        "684": "2309.14068v3",
        "685": "2401.11261v2",
        "686": "2403.03881v2",
        "687": "2301.10677v2",
        "688": "2301.00704v1",
        "689": "2304.06027v1",
        "690": "2306.05427v2",
        "691": "2305.11540v1",
        "692": "2402.17133v1",
        "693": "2308.10079v3",
        "694": "2303.12861v3",
        "695": "2309.10714v1",
        "696": "2207.14626v2",
        "697": "1503.05768v2",
        "698": "2305.18812v1",
        "699": "2311.11383v2",
        "700": "2302.11710v2",
        "701": "2304.04541v2",
        "702": "2404.15447v1",
        "703": "2209.05442v2",
        "704": "2201.00308v3",
        "705": "2204.02641v1",
        "706": "2401.15636v1",
        "707": "2403.18425v1",
        "708": "2301.05465v1",
        "709": "2303.15233v2",
        "710": "2403.09176v1",
        "711": "2311.00265v1",
        "712": "2404.02733v2",
        "713": "2404.11243v1",
        "714": "2302.08113v1",
        "715": "2404.01959v2",
        "716": "2310.03739v1",
        "717": "2305.18231v3",
        "718": "2402.06969v1",
        "719": "2311.12052v2",
        "720": "1707.02637v4",
        "721": "2306.04632v1",
        "722": "2404.13903v2",
        "723": "2212.13771v1",
        "724": "2310.01406v2",
        "725": "2306.02063v2",
        "726": "2311.17695v2",
        "727": "2310.03502v1",
        "728": "2212.00490v2",
        "729": "2312.06285v1",
        "730": "2307.04028v1",
        "731": "2310.17577v2",
        "732": "2311.03226v1",
        "733": "2211.15388v2",
        "734": "2311.14768v1",
        "735": "2007.00653v2",
        "736": "2403.06807v1",
        "737": "2206.00941v2",
        "738": "2308.13712v3",
        "739": "2404.14507v1",
        "740": "2106.06819v1",
        "741": "2308.13767v1",
        "742": "2403.02879v1",
        "743": "2402.15170v1",
        "744": "2304.04746v1",
        "745": "2402.13369v1",
        "746": "2404.08926v2",
        "747": "2303.16203v3",
        "748": "2212.13344v1",
        "749": "1702.07472v1",
        "750": "2402.05210v3",
        "751": "2403.17377v1",
        "752": "2203.04304v2",
        "753": "2402.10855v1",
        "754": "2305.15194v2",
        "755": "2311.13745v1",
        "756": "2308.12059v1",
        "757": "2212.06013v3",
        "758": "2312.10998v1",
        "759": "2205.12524v2",
        "760": "2401.15649v1",
        "761": "2404.02411v1",
        "762": "2211.10950v1",
        "763": "2303.05916v2",
        "764": "2310.18840v2",
        "765": "2311.14521v4",
        "766": "2312.06663v1",
        "767": "2203.15570v1",
        "768": "2210.09477v4",
        "769": "2303.09604v1",
        "770": "2310.09484v2",
        "771": "2311.14097v3",
        "772": "2303.13430v1",
        "773": "2305.14720v2",
        "774": "2311.02358v4",
        "775": "2312.03775v2",
        "776": "1705.09275v4",
        "777": "2304.01053v1",
        "778": "2304.06648v6",
        "779": "2404.07946v1",
        "780": "2312.02216v2",
        "781": "2311.10794v1",
        "782": "2303.14353v1",
        "783": "2211.00902v1",
        "784": "2312.04005v1",
        "785": "2207.09855v1",
        "786": "2403.12962v1",
        "787": "2304.04968v3",
        "788": "2402.03666v2",
        "789": "2403.19140v1",
        "790": "1508.02848v2",
        "791": "2306.15832v2",
        "792": "2402.17275v2",
        "793": "2312.04655v1",
        "794": "2309.14751v1",
        "795": "2305.10855v5",
        "796": "2401.02414v1",
        "797": "2403.10953v1",
        "798": "2011.09699v1",
        "799": "2404.10445v1",
        "800": "2309.03453v2",
        "801": "2311.16133v2",
        "802": "2311.17101v1",
        "803": "2305.08192v2",
        "804": "2312.15247v1",
        "805": "2112.03145v2",
        "806": "2212.03235v3",
        "807": "2302.05872v3",
        "808": "2312.03540v1",
        "809": "2306.11719v2",
        "810": "2401.09325v1",
        "811": "2006.11239v2",
        "812": "2212.00793v2",
        "813": "2404.10688v1",
        "814": "2305.14771v2",
        "815": "2303.07937v4",
        "816": "2208.09392v1",
        "817": "2212.02936v2",
        "818": "2212.00842v2",
        "819": "2401.04585v1",
        "820": "2306.04542v3",
        "821": "2403.19738v1",
        "822": "2403.06976v1",
        "823": "2310.04672v1",
        "824": "2308.01472v1",
        "825": "2305.13308v1",
        "826": "2312.13253v1",
        "827": "2308.09306v1",
        "828": "2401.01827v1",
        "829": "2309.09944v2",
        "830": "2311.06322v2",
        "831": "2208.11284v2",
        "832": "2106.05527v5",
        "833": "2308.05695v4",
        "834": "2311.11164v1",
        "835": "2312.06198v3",
        "836": "2404.14568v1",
        "837": "2404.01717v2",
        "838": "2306.16052v1",
        "839": "2310.04378v1",
        "840": "2403.05239v1",
        "841": "2403.09683v1",
        "842": "2308.01147v1",
        "843": "2304.12891v1",
        "844": "2305.16269v1",
        "845": "2307.02814v1",
        "846": "2403.13802v2",
        "847": "2403.12063v1",
        "848": "2403.11870v1",
        "849": "2308.03463v3",
        "850": "2308.02552v2",
        "851": "2303.14420v2",
        "852": "2311.16503v3",
        "853": "2304.01900v1",
        "854": "2312.01129v2",
        "855": "2312.04337v1",
        "856": "2308.15989v1",
        "857": "1907.04526v2",
        "858": "2209.11888v2",
        "859": "2401.08049v1",
        "860": "2303.09556v3",
        "861": "2306.03878v2",
        "862": "2402.12004v1",
        "863": "2404.01709v1",
        "864": "2305.09817v1",
        "865": "2403.14066v1",
        "866": "2401.06637v5",
        "867": "2402.14843v1",
        "868": "2401.01520v2",
        "869": "2112.13339v2",
        "870": "2403.06069v1",
        "871": "2212.00210v3",
        "872": "2310.01110v1",
        "873": "2302.02284v1",
        "874": "2311.17216v2",
        "875": "2309.11745v2",
        "876": "2210.16056v1",
        "877": "2404.01367v1",
        "878": "2312.13016v4",
        "879": "2312.02150v2",
        "880": "2403.01212v1",
        "881": "2403.17004v1",
        "882": "2306.06874v5",
        "883": "2308.11948v1",
        "884": "2404.03326v1",
        "885": "2312.06738v3",
        "886": "2311.15980v2",
        "887": "2312.17681v1",
        "888": "2011.06704v1",
        "889": "2310.01819v3",
        "890": "2403.08728v1",
        "891": "2303.13516v3",
        "892": "2211.06235v1",
        "893": "2309.07277v2",
        "894": "2307.13908v1",
        "895": "2403.07350v1",
        "896": "2404.06851v1",
        "897": "2307.09781v1",
        "898": "2301.07485v1",
        "899": "2307.12035v1",
        "900": "2403.06090v2",
        "901": "2307.01952v1",
        "902": "2305.16811v1",
        "903": "2304.14404v1",
        "904": "2404.07987v1",
        "905": "2301.13173v1",
        "906": "2309.11601v2",
        "907": "2401.16459v1",
        "908": "2312.03816v3",
        "909": "2302.11552v4",
        "910": "2311.17953v1",
        "911": "2112.13592v6",
        "912": "2404.03706v1",
        "913": "2311.01018v1",
        "914": "2401.00029v3",
        "915": "2301.08455v2",
        "916": "2311.16122v1",
        "917": "2204.02849v2",
        "918": "2111.03186v1",
        "919": "2402.08159v1",
        "920": "2311.13231v3",
        "921": "2312.01682v1",
        "922": "2212.00235v1",
        "923": "2311.16882v1",
        "924": "1412.3352v1",
        "925": "2306.04642v2",
        "926": "2404.07600v2",
        "927": "2310.08092v1",
        "928": "2311.15108v2",
        "929": "2404.01655v2",
        "930": "2304.00686v4",
        "931": "2404.11098v3",
        "932": "2210.04885v5",
        "933": "2305.16936v1",
        "934": "2306.13384v2",
        "935": "2211.16032v1",
        "936": "2309.16812v1",
        "937": "2310.02848v1",
        "938": "2209.12152v4",
        "939": "2301.10227v2",
        "940": "2306.07954v2",
        "941": "2402.18575v1",
        "942": "2010.02502v4",
        "943": "2402.10404v1",
        "944": "2208.12675v2",
        "945": "2212.09412v3",
        "946": "2306.02236v1",
        "947": "2309.04399v1",
        "948": "2404.13320v1",
        "949": "2211.11319v1",
        "950": "2112.00390v3",
        "951": "2301.12935v3",
        "952": "2211.09206v1",
        "953": "2312.09181v1",
        "954": "2205.11487v1",
        "955": "2402.18362v1",
        "956": "2312.06038v1",
        "957": "2305.13840v2",
        "958": "2307.02698v3",
        "959": "2308.10510v2",
        "960": "2404.00849v1",
        "961": "2403.18461v1",
        "962": "2404.13000v1",
        "963": "2403.08487v1",
        "964": "2308.07421v1",
        "965": "2403.14421v2",
        "966": "2302.04222v5",
        "967": "2304.05060v2",
        "968": "2309.03445v1",
        "969": "2403.05245v1",
        "970": "2306.05414v3",
        "971": "2307.08702v1",
        "972": "2212.09748v2",
        "973": "2312.03993v1",
        "974": "2305.13625v2",
        "975": "2308.09091v2",
        "976": "2404.07949v1",
        "977": "2211.02527v3",
        "978": "2312.06725v3",
        "979": "2211.15089v3",
        "980": "2402.13737v1",
        "981": "2206.03461v1",
        "982": "2305.07644v2",
        "983": "2311.14028v1",
        "984": "2402.18206v1",
        "985": "2206.10365v1",
        "986": "2204.01318v1",
        "987": "2309.00287v2",
        "988": "2308.03183v1",
        "989": "2401.00763v1",
        "990": "2304.09244v1",
        "991": "2302.07261v2",
        "992": "2305.12328v1",
        "993": "2404.02883v1",
        "994": "2305.09161v1",
        "995": "2304.11750v1",
        "996": "2312.03511v2",
        "997": "2108.08674v1",
        "998": "2312.12491v1",
        "999": "2305.03486v1",
        "1000": "2403.04014v1"
    }
}